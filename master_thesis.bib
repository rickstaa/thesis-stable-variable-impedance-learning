@online{150602640You,
  title = {[1506.02640] {{You Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  url = {https://arxiv.org/abs/1506.02640},
  urldate = {2019-11-27},
  keywords = {#nosource,object detection,robotic grasping,tno internship}
}

@online{180606920Maximum,
  title = {[1806.06920] {{Maximum}} a {{Posteriori Policy Optimisation}}},
  url = {https://arxiv.org/abs/1806.06920},
  urldate = {2021-04-07},
  keywords = {#nosource,constrained}
}

@article{abateFormalSynthesisLyapunov2021,
  title = {Formal {{Synthesis}} of {{Lyapunov Neural Networks}}},
  author = {Abate, Alessandro and Ahmed, Daniele and Giacobbe, Mirco and Peruffo, Andrea},
  date = {2021-07},
  journaltitle = {IEEE Control Systems Letters},
  volume = {5},
  number = {3},
  pages = {773--778},
  issn = {2475-1456},
  doi = {10.1109/LCSYS.2020.3005328},
  abstract = {We propose an automatic and formally sound method for synthesising Lyapunov functions for the asymptotic stability of autonomous non-linear systems. Traditional methods are either analytical and require manual effort or are numerical but lack of formal soundness. Symbolic computational methods for Lyapunov functions, which are in between, give formal guarantees but are typically semi-automatic because they rely on the user to provide appropriate function templates. We propose a method that finds Lyapunov functions fully automatically-using machine learning-while also providing formal guarantees-using satisfiability modulo theories (SMT). We employ a counterexample-guided approach where a numerical learner and a symbolic verifier interact to construct provably correct Lyapunov neural networks (LNNs). The learner trains a neural network that satisfies the Lyapunov criteria for asymptotic stability over a samples set; the verifier proves via SMT solving that the criteria are satisfied over the whole domain or augments the samples set with counterexamples. Our method supports neural networks with polynomial activation functions and multiple depth and width, which display wide learning capabilities. We demonstrate our method over several non-trivial benchmarks and compare it favourably against a numerical optimisation-based approach, a symbolic template-based approach, and a cognate LNN-based approach. Our method synthesises Lyapunov functions faster and over wider spatial domains than the alternatives, yet providing stronger or equal guarantees.},
  eventtitle = {{{IEEE Control Systems Letters}}},
  keywords = {Asymptotic stability,Biological neural networks,Computer-aided control design,Lyapunov methods,neural networks,Neurons,Numerical stability,READ,Stability analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/abate_et_al_2021_formal_synthesis_of_lyapunov_neural_networks.pdf}
}

@article{abdulkhaderDataDrivenMethodsContactRich2021,
  title = {Data-{{Driven Methods}} for {{Contact-Rich Manipulation}}: {{Control Stability}} and {{Data-Efficiency}}},
  shorttitle = {Data-{{Driven Methods}} for {{Contact-Rich Manipulation}}},
  author = {Abdul Khader, Shahbaz},
  date = {2021},
  publisher = {{KTH Royal Institute of Technology}},
  url = {http://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-299799},
  urldate = {2023-04-18},
  abstract = {DiVA portal is a finding tool for research publications and student theses written at the following 50 universities and research institutions.},
  langid = {english},
  keywords = {READ,REVIEW},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/abdul_khader_2021_data-driven_methods_for_contact-rich_manipulation.pdf}
}

@article{abu-dakkaForcebasedVariableImpedance2018,
  title = {Force-Based Variable Impedance Learning for Robotic Manipulation},
  author = {Abu-Dakka, Fares J. and Rozo, Leonel and Caldwell, Darwin G.},
  date = {2018-11-01},
  journaltitle = {Robotics and Autonomous Systems},
  shortjournal = {Robotics and Autonomous Systems},
  volume = {109},
  pages = {156--167},
  issn = {0921-8890},
  doi = {10.1016/j.robot.2018.07.008},
  url = {https://www.sciencedirect.com/science/article/pii/S0921889018300125},
  urldate = {2022-08-24},
  abstract = {In order for robots to successfully carry out manipulation tasks, they require to exploit contact forces and variable impedance control. The conditions of such type of robotic tasks may significantly vary in dynamic environments, which demand robots to be endowed with adaptation capabilities. This can be achieved through learning methods that allow the robot not only to model a manipulation task but also to adapt to unseen situations. In this context, this paper proposes a learning-from-demonstration framework that integrates force sensing and variable impedance control to learn force-based variable stiffness skills. The proposed approach estimates full stiffness matrices from human demonstrations, which are then used along with the sensed forces to encode a probabilistic model of the task. This model is used to retrieve a time-varying stiffness profile that allows the robot to satisfactorily react to new task conditions. The proposed framework evaluates two different stiffness representations: Cholesky decomposition and a Riemannian manifold approach. We validate the proposed framework in simulation using 2D and 7D systems and a couple of real scenarios.},
  langid = {english},
  keywords = {Learning from demonstration,READ,Robot learning,Robotic manipulation,Variable impedance},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/abu-dakka_et_al_2018_force-based_variable_impedance_learning_for_robotic_manipulation.pdf}
}

@inproceedings{abu-dakkaPeriodicDMPFormulation2021,
  title = {Periodic {{DMP}} Formulation for {{Quaternion Trajectories}}},
  booktitle = {2021 20th {{International Conference}} on {{Advanced Robotics}} ({{ICAR}})},
  author = {Abu-Dakka, Fares J. and Saveriano, Matteo and Peternel, Luka},
  date = {2021-12},
  pages = {658--663},
  doi = {10.1109/ICAR53236.2021.9659319},
  abstract = {Imitation learning techniques have been used as a way to transfer skills to robots. Among them, dynamic movement primitives (DMPs) have been widely exploited as an effective and an efficient technique to learn and reproduce complex discrete and periodic skills. While DMPs have been properly formulated for learning point-to-point movements for both translation and orientation, periodic ones are missing a formulation to learn the orientation. To address this gap, we propose a novel DMP formulation that enables encoding of periodic orientation trajectories. Within this formulation we develop two approaches: Riemannian metric-based projection approach and unit quaternion based periodic DMP. Both formulations exploit unit quaternions to represent the orientation. However, the first exploits the properties of Riemannian manifolds to work in the tangent space of the unit sphere. The second encodes directly the unit quaternion trajectory while guaranteeing the unitary norm of the generated quaternions. We validated the technical aspects of the proposed methods in simulation. Then we performed experiments on a real robot to execute daily tasks that involve periodic orientation changes (i.e., surface polishing/wiping and liquid mixing by shaking).},
  eventtitle = {2021 20th {{International Conference}} on {{Advanced Robotics}} ({{ICAR}})},
  keywords = {Encoding,Liquids,Manifolds,Mathematical models,Measurement,Quaternions,READ,Training data},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/abu-dakka_et_al_2021_periodic_dmp_formulation_for_quaternion_trajectories.pdf}
}

@online{abu-dakkaUnifiedFormulationGeometryaware2022,
  title = {A {{Unified Formulation}} of {{Geometry-aware Dynamic Movement Primitives}}},
  author = {Abu-Dakka, Fares J. and Saveriano, Matteo and Kyrki, Ville},
  date = {2022-03-07},
  eprint = {2203.03374},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2203.03374},
  urldate = {2022-10-30},
  abstract = {Learning from demonstration (LfD) is considered as an efficient way to transfer skills from humans to robots. Traditionally, LfD has been used to transfer Cartesian and joint positions and forces from human demonstrations. The traditional approach works well for some robotic tasks, but for many tasks of interest it is necessary to learn skills such as orientation, impedance, and/or manipulability that have specific geometric characteristics. An effective encoding of such skills can be only achieved if the underlying geometric structure of the skill manifold is considered and the constrains arising from this structure are fulfilled during both learning and execution. However, typical learned skill models such as dynamic movement primitives (DMPs) are limited to Euclidean data and fail in correctly embedding quantities with geometric constraints. In this paper, we propose a novel and mathematically principled framework that uses concepts from Riemannian geometry to allow DMPs to properly embed geometric constrains. The resulting DMP formulation can deal with data sampled from any Riemannian manifold including, but not limited to, unit quaternions and symmetric and positive definite matrices. The proposed approach has been extensively evaluated both on simulated data and real robot experiments. The performed evaluation demonstrates that beneficial properties of DMPs, such as convergence to a given goal and the possibility to change the goal during operation, apply also to the proposed formulation.},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control,READ,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/abu-dakka_et_al_2022_a_unified_formulation_of_geometry-aware_dynamic_movement_primitives.pdf}
}

@article{abu-dakkaVariableImpedanceControl2020,
  title = {Variable {{Impedance Control}} and {{Learning}}—{{A Review}}},
  author = {Abu-Dakka, Fares J. and Saveriano, Matteo},
  date = {2020},
  journaltitle = {Frontiers in Robotics and AI},
  volume = {7},
  issn = {2296-9144},
  url = {https://www.frontiersin.org/article/10.3389/frobt.2020.590681},
  urldate = {2022-04-22},
  abstract = {Robots that physically interact with their surroundings, in order to accomplish some tasks or assist humans in their activities, require to exploit contact forces in a safe and proficient manner. Impedance control is considered as a prominent approach in robotics to avoid large impact forces while operating in unstructured environments. In such environments, the conditions under which the interaction occurs may significantly vary during the task execution. This demands robots to be endowed with online adaptation capabilities to cope with sudden and unexpected changes in the environment. In this context, variable impedance control arises as a powerful tool to modulate the robot's behavior in response to variations in its surroundings. In this survey, we present the state-of-the-art of approaches devoted to variable impedance control from control and learning perspectives (separately and jointly). Moreover, we propose a new taxonomy for mechanical impedance based on variability, learning, and control. The objective of this survey is to put together the concepts and efforts that have been done so far in this field, and to describe advantages and disadvantages of each approach. The survey concludes with open issues in the field and an envisioned framework that may potentially solve them.},
  keywords = {REVIEW,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/abu-dakka_saveriano_2020_variable_impedance_control_and_learning—a_review.pdf}
}

@unpublished{achiamConstrainedPolicyOptimization2017,
  title = {Constrained {{Policy Optimization}}},
  author = {Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
  date = {2017-05-30},
  eprint = {1705.10528},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1705.10528},
  urldate = {2020-07-02},
  abstract = {For many applications of reinforcement learning it can be more convenient to specify both a reward function and constraints, rather than trying to design behavior through the reward function. For example, systems that physically interact with or around humans should satisfy safety constraints. Recent advances in policy search algorithms (Mnih et al., 2016, Schulman et al., 2015, Lillicrap et al., 2016, Levine et al., 2016) have enabled new capabilities in high-dimensional control, but do not consider the constrained setting. We propose Constrained Policy Optimization (CPO), the first general-purpose policy search algorithm for constrained reinforcement learning with guarantees for near-constraint satisfaction at each iteration. Our method allows us to train neural network policies for high-dimensional control while making guarantees about policy behavior all throughout training. Our guarantees are based on a new theoretical result, which is of independent interest: we prove a bound relating the expected returns of two policies to an average divergence between them. We demonstrate the effectiveness of our approach on simulated robot locomotion tasks where the agent must satisfy constraints motivated by safety.},
  keywords = {#nosource,Computer Science - Machine Learning,constrained policy optimization,reinforcement learning,safety}
}

@unpublished{achiamVariationalOptionDiscovery2018,
  title = {Variational {{Option Discovery Algorithms}}},
  author = {Achiam, Joshua and Edwards, Harrison and Amodei, Dario and Abbeel, Pieter},
  date = {2018-07-26},
  eprint = {1807.10299},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1807.10299},
  urldate = {2020-07-02},
  abstract = {We explore methods for option discovery based on variational inference and make two algorithmic contributions. First: we highlight a tight connection between variational option discovery methods and variational autoencoders, and introduce Variational Autoencoding Learning of Options by Reinforcement (VALOR), a new method derived from the connection. In VALOR, the policy encodes contexts from a noise distribution into trajectories, and the decoder recovers the contexts from the complete trajectories. Second: we propose a curriculum learning approach where the number of contexts seen by the agent increases whenever the agent's performance is strong enough (as measured by the decoder) on the current set of contexts. We show that this simple trick stabilizes training for VALOR and prior variational option discovery methods, allowing a single agent to learn many more modes of behavior than it could with a fixed context distribution. Finally, we investigate other topics related to variational option discovery, including fundamental limitations of the general approach and the applicability of learned options to downstream tasks.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,reinforcement learning,unsupervised RL}
}

@article{al-shukaActiveImpedanceControl2018,
  title = {Active {{Impedance Control}} of {{Bioinspired Motion Robotic Manipulators}}: {{An Overview}}},
  shorttitle = {Active {{Impedance Control}} of {{Bioinspired Motion Robotic Manipulators}}},
  author = {Al-Shuka, Hayder F. N. and Leonhardt, Steffen and Zhu, Wen-Hong and Song, Rui and Ding, Chao and Li, Yibin},
  date = {2018-10-18},
  journaltitle = {Applied Bionics and Biomechanics},
  volume = {2018},
  pages = {e8203054},
  publisher = {{Hindawi}},
  issn = {1176-2322},
  doi = {10.1155/2018/8203054},
  url = {https://www.hindawi.com/journals/abb/2018/8203054/},
  urldate = {2022-05-02},
  abstract = {There are two main categories of force control schemes: hybrid position-force control and impedance control. However, the former does not take into account the dynamic interaction between the robot’s end effector and the environment. In contrast, impedance control includes regulation and stabilization of robot motion by creating a mathematical relationship between the interaction forces and the reference trajectories. It involves an energetic pair of a flow and an effort, instead of controlling a single position or a force. A mass-spring-damper impedance filter is generally used for safe interaction purposes. Tuning the parameters of the impedance filter is important and, if an unsuitable strategy is used, this can lead to unstable contact. Humans, however, have exceptionally effective control systems with advanced biological actuators. An individual can manipulate muscle stiffness to comply with the interaction forces. Accordingly, the parameters of the impedance filter should be time varying rather than value constant in order to match human behavior during interaction tasks. Therefore, this paper presents an overview of impedance control strategies including standard and extended control schemes. Standard controllers cover impedance and admittance architectures. Extended control schemes include admittance control with force tracking, variable impedance control, and impedance control of flexible joints. The categories of impedance control and their features and limitations are well introduced. Attention is paid to variable impedance control while considering the possible control schemes, the performance, stability, and the integration of constant compliant elements with the host robot.},
  langid = {english},
  keywords = {REVIEW,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/al-shuka_et_al_2018_active_impedance_control_of_bioinspired_motion_robotic_manipulators.pdf}
}

@inproceedings{albu-schafferCartesianImpedanceControl2003,
  title = {Cartesian Impedance Control of Redundant Robots: Recent Results with the {{DLR-light-weight-arms}}},
  shorttitle = {Cartesian Impedance Control of Redundant Robots},
  booktitle = {2003 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{Cat}}. {{No}}.{{03CH37422}})},
  author = {Albu-Schaffer, A. and Ott, C. and Frese, U. and Hirzinger, G.},
  date = {2003-09},
  volume = {3},
  pages = {3704-3709 vol.3},
  issn = {1050-4729},
  doi = {10.1109/ROBOT.2003.1242165},
  abstract = {This paper addresses the problem of impedance control for flexible joint robots based on a singular perturbation approach. Some aspects of the impedance controller, which turned out to be of high practical relevance during applications are then addressed, such as the implementation of nullspace stiffness for redundant manipulators, the avoiding of mass matrix decoupling and the related design of the desired damping matrix. Finally, the proposed methods are validated through measurements on the DLR robot.},
  eventtitle = {2003 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{Cat}}. {{No}}.{{03CH37422}})},
  keywords = {Arm,Damping,Impedance,Lighting control,Medical robotics,Robot control,Robot sensing systems,Sampling methods,Torque control,Weight control},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/albu-schaffer_et_al_2003_cartesian_impedance_control_of_redundant_robots.pdf}
}

@article{albu-schafferUnifiedPassivitybasedControl2007,
  title = {A {{Unified Passivity-based Control Framework}} for {{Position}}, {{Torque}} and {{Impedance Control}} of {{Flexible Joint Robots}}},
  author = {Albu-Schäffer, Alin and Ott, Christian and Hirzinger, Gerd},
  date = {2007-01-01},
  journaltitle = {The International Journal of Robotics Research},
  shortjournal = {The International Journal of Robotics Research},
  volume = {26},
  number = {1},
  pages = {23--39},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {0278-3649},
  doi = {10.1177/0278364907073776},
  url = {https://doi.org/10.1177/0278364907073776},
  urldate = {2022-08-28},
  abstract = {This paper describes a general passivity-based framework for the control of flexible joint robots. Recent results on torque, position, as well as impedance control of flexible joint robots are summarized, and the relations between the individual contributions are highlighted. It is shown that an inner torque feedback loop can be incorporated into a passivity-based analysis by interpreting torque feedback in terms of shaping of the motor inertia. This result, which implicitly was already included in earlier work on torque and position control, can also be used for the design of impedance controllers. For impedance control, furthermore, potential energy shaping is of special interest. It is shown how, based only on the motor angles, a potential function can be designed which simultaneously incorporates gravity compensation and a desired Cartesian stiffness relation for the link angles. All the presented controllers were experimentally evaluated on DLR lightweight robots and their performance and robustness shown with respect to uncertain model parameters. Experimental results with position controllers as well as an impact experiment are presented briefly, and an overview of several applications is given in which the controllers have been applied.},
  langid = {english},
  keywords = {active vibration damping,flexible joint robots,impedance control,passivity-based control,torque feedback},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/albu-schäffer_et_al_2007_a_unified_passivity-based_control_framework_for_position,_torque_and_impedance.pdf}
}

@unpublished{alexanderStrategicAttentiveWriter2016,
  title = {Strategic {{Attentive Writer}} for {{Learning Macro-Actions}}},
  author = {Alexander and Vezhnevets and Mnih, Volodymyr and Agapiou, John and Osindero, Simon and Graves, Alex and Vinyals, Oriol and Kavukcuoglu, Koray},
  date = {2016-06-15},
  eprint = {1606.04695},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1606.04695},
  urldate = {2020-07-02},
  abstract = {We present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner by purely interacting with an environment in reinforcement learning setting. The network builds an internal plan, which is continuously updated upon observation of the next input from the environment. It can also partition this internal representation into contiguous sub- sequences by learning for how long the plan can be committed to - i.e. followed without re-planing. Combining these properties, the proposed model, dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally abstracted macro- actions of varying lengths that are solely learnt from data without any prior information. These macro-actions enable both structured exploration and economic computation. We experimentally demonstrate that STRAW delivers strong improvements on several ATARI games by employing temporally extended planning strategies (e.g. Ms. Pacman and Frostbite). It is at the same time a general algorithm that can be applied on any sequence data. To that end, we also show that when trained on text prediction task, STRAW naturally predicts frequent n-grams (instead of macro-actions), demonstrating the generality of the approach.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,hierarchical RL,reinforcement learning},
  annotation = {00111}
}

@incollection{alonsoCurrentResearchTrends2019,
  title = {Current {{Research Trends}} in {{Robot Grasping}} and {{Bin Picking}}},
  booktitle = {International {{Joint Conference SOCO}}’18-{{CISIS}}’18-{{ICEUTE}}’18},
  author = {Alonso, Marcos and Izaguirre, Alberto and Graña, Manuel},
  editor = {Graña, Manuel and López-Guede, José Manuel and Etxaniz, Oier and Herrero, Álvaro and Sáez, José Antonio and Quintián, Héctor and Corchado, Emilio},
  date = {2019},
  volume = {771},
  pages = {367--376},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-94120-2_35},
  url = {http://link.springer.com/10.1007/978-3-319-94120-2_35},
  urldate = {2019-04-14},
  abstract = {We provide a view of current research issues in Robotic Grasping and Bin Picking focused on the perception aspects of the problem, mainly related to computer vision algorithms. After recalling the evolution of the topics in the last decades, we focus on the modern use of Deep Learning Algorithms. Two main trends are followed in the approaches to innovative grasping techniques. First, Convolutional Neural Networks are used for grasping perceptual aspects. We discuss the different degrees of success of several published approaches. Second, Deep Reinforcement Learning is being extensively tested in order to develop integrated eye-hand coordination systems not requiring delicate calibration. We provide also a discussion of possible future lines of research.},
  isbn = {978-3-319-94119-6 978-3-319-94120-2},
  langid = {english},
  keywords = {#nosource,robotic grasping,tno internship},
  annotation = {00001}
}

@thesis{amanhoudControlContactTasks2021,
  title = {Control of {{Contact Tasks}} in {{Autonomous}} and {{Human-Robot Collaborative Scenarios}}: {{A Dynamical System Approach}}},
  shorttitle = {Control of {{Contact Tasks}} in {{Autonomous}} and {{Human-Robot Collaborative Scenarios}}},
  author = {Amanhoud, Walid},
  date = {2021},
  institution = {{EPFL}},
  location = {{Lausanne}},
  doi = {10.5075/epfl-thesis-8847},
  abstract = {Robotic systems are more and more present in our daily life. Robots became indeed more economical, easy to program, safe and collaborative in their interaction with humans, versatile in performing different tasks and adaptive towards changes in the environment. Despite the recent advances in robotics, developing fully autonomous robots to achieve complex industrial or daily-life tasks is still very challenging. The human, due to his superior perception, understanding and reasoning skills, especially in uncertain environment, needs to be in the loop. Therefore, the ability of robots to collaborate with humans is of great interest in the robotics community. Human-Robot Collaboration usually implies robots that share the control of the tasks with humans, providing adjustable autonomy level and mutual adaptation. A field that can particularly benefits from such collaboration is assistive robotics surgery. The introduction of shared autonomy in surgical tasks has several advantages. In particular, it can provide higher accuracy, flexibility, control and efficiency during the operation, over longer time periods, relieve the surgeon during repetitive tasks and reduce his mental workload. However, it is not very clear how to best use shared control to assist a surgeon during the operation. As part of a research project (the "Surgical project"), in this thesis we would like to propose solutions to that problem by building a teleoperated shared control architecture for a laparoscopic surgery scenario. While laparoscopic surgery usually involves one surgeon and two human assistants to perform the surgery, the project aims to replace the assistants by two partially teleoperated robotic systems. The target shared control architecture would explore various strategies to ensure safety for both the patient and the surgeon, provide accuracy and stability in motion and force control of the tools attached to the robots, reduce the mental workload of the surgeon and adapt the level of assistance based on well-identified criteria. The first year was mainly devoted to the control of motion and force for contact tasks which is of great importance in the context of the project. Robot force control is traditionally achieved either explicitly through hybrid position/force control or implicitly using impedance control. We propose an approach leveraging the properties of dynamical systems (DS) for immediate re-planning and robustness to real-time perturbations by exploiting local modulation of the DS. Dynamical systems has been widely used as a tool to represent and generate motion in robotics application. Here the idea is to propose a DS formulation to generate contact force in addition to motion. First implementation and experiments to evaluate the effectiveness of the strategy were conducted in practice with a 7-DOF robotic arm (KUKA LWR IV+). So far we focused on contact tasks that were performed autonomously by one or two robots. Future work will improve and use the proposed DS approach to consider contact tasks in shared control scenarios where one human and one or two robots need to collaborate to successfully achieve the tasks. Because of the nature of the surgical project we will use teleoperation systems to introduce the human in the loop. The developed teleoperated shared control architecture will be evaluated on laparoscopic surgery related tasks},
  langid = {english},
  pagetotal = {167},
  keywords = {Dynamical Systems,Force control,Human-Robot Collaboration,Physical Human-Robot Interaction,REVIEW,Robotic Surgery,Shared Control,Supernumerary Robotic},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/amanhoud_2021_control_of_contact_tasks_in_autonomous_and_human-robot_collaborative_scenarios.pdf}
}

@inproceedings{amanhoudDynamicalSystemApproach2019,
  title = {A {{Dynamical System Approach}} to {{Motion}} and {{Force Generation}} in {{Contact Tasks}}},
  booktitle = {Robotics: {{Science}} and {{Systems Xv}}},
  author = {Amanhoud, Walid and Khoramshahi, Mahdi and Billard, Aude},
  editor = {Bicchi, A. and KressGazit, H. and Hutchinson, S.},
  date = {2019},
  publisher = {{Mit Press}},
  location = {{Cambridge}},
  url = {https://infoscience.epfl.ch/record/265747},
  urldate = {2022-05-02},
  abstract = {Many tasks require the robot to enter in contact with surfaces, be it to take support, to polish or to grasp an object. It is crucial that the robot controls forces both upon making contact and while in contact. While many solutions exist to control for contact, none offer the required robustness to adapt to real-world uncertainties, such as sudden displacement of the object prior and once in contact. To adapt to such disturbances require to re-plan on the fly both the trajectory and the force. Dynamical systems (DS) offer a framework for instant re-planning of trajectories. They are however limited to control of motions. We extend this framework here to enable generating contact forces and trajectories through DS. The framework allows also to modulate the impedance so as to show rigidity to maintain contact, and compliance to ensure safe interaction with humans. We validate the approach in single and dual arm setting using KUKA LWR 4+ robotic arms. We show that the approach allows 1) to make smooth contact while applying large forces, 2) to maintain desired contact force when scanning non-linear surfaces, even when the surface is moved, and 3) to grasp and lift smoothly an object in the air, and to re-balance forces on the fly to maintain the grasp even when subjected to strong external disturbances.},
  isbn = {978-0-9923747-5-4},
  langid = {english},
  keywords = {dynamic system,manipulators,READ,robot,STABILITY,variable impedance control},
  annotation = {WOS:000570976800021},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/amanhoud_et_al_2019_a_dynamical_system_approach_to_motion_and_force_generation_in_contact_tasks.pdf}
}

@inproceedings{amanhoudForceAdaptationContact2020,
  title = {Force {{Adaptation}} in {{Contact Tasks}} with {{Dynamical Systems}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Amanhoud, Walid and Khoramshahi, Mahdi and Bonnesoeur, Maxime and Billard, Aude},
  date = {2020-05},
  pages = {6841--6847},
  issn = {2577-087X},
  doi = {10.1109/ICRA40945.2020.9197509},
  abstract = {In many tasks such as finishing operations, achieving accurate force tracking is essential. However, uncertainties in the robot dynamics and the environment limit the force tracking accuracy. Learning a compensation model for these uncertainties to reduce the force error is an effective approach to overcome this limitation. However, this approach requires an adaptive and robust framework for motion and force generation. In this paper, we use the time-invariant Dynamical System (DS) framework for force adaptation in contact tasks. We propose to improve force tracking accuracy through online adaptation of a state-dependent force correction model encoded with Radial Basis Functions (RBFs). We evaluate our method with a KUKA LWR IV+ robotic arm. We show its efficiency to reduce the force error to a negligible amount with different target forces and robot velocities. Furthermore, we study the effect of the hyper-parameters and provide a guideline for their selection. We showcase a collaborative cleaning task with a human by integrating our method to previous works to achieve force, motion, and task adaptation at the same time. Thereby, we highlight the benefits of using adaptive force control in real-world environments where we need reactive and adaptive behaviours in response to interactions with the environment.},
  eventtitle = {2020 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  keywords = {Compliance and Impedance Control,DS,dynamic system,Dynamics,energy tanks,Force,Force Control,Impedance,Physical Human-Robot Interaction,READ,Robots,STABILITY,Surface impedance,Task analysis,Tracking},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/amanhoud_et_al_2020_force_adaptation_in_contact_tasks_with_dynamical_systems.pdf}
}

@inproceedings{amesControlBarrierFunctions2019,
  title = {Control {{Barrier Functions}}: {{Theory}} and {{Applications}}},
  shorttitle = {Control {{Barrier Functions}}},
  booktitle = {2019 18th {{European Control Conference}} ({{ECC}})},
  author = {Ames, Aaron D. and Coogan, Samuel and Egerstedt, Magnus and Notomista, Gennaro and Sreenath, Koushil and Tabuada, Paulo},
  date = {2019-06},
  pages = {3420--3431},
  doi = {10.23919/ECC.2019.8796030},
  abstract = {This paper provides an introduction and overview of recent work on control barrier functions and their use to verify and enforce safety properties in the context of (optimization based) safety-critical controllers. We survey the main technical results and discuss applications to several domains including robotic systems.},
  eventtitle = {2019 18th {{European Control Conference}} ({{ECC}})},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ames_et_al_2019_control_barrier_functions.pdf}
}

@article{amesRapidlyExponentiallyStabilizing2014,
  title = {Rapidly {{Exponentially Stabilizing Control Lyapunov Functions}} and {{Hybrid Zero Dynamics}}},
  author = {Ames, Aaron D. and Galloway, Kevin and Sreenath, Koushil and Grizzle, Jessy W.},
  date = {2014-01-01},
  publisher = {{Carnegie Mellon University}},
  doi = {10.1184/R1/6490121.v1},
  url = {/articles/journal_contribution/Rapidly_Exponentially_Stabilizing_Control_Lyapunov_Functions_and_Hybrid_Zero_Dynamics/6490121/1},
  urldate = {2021-04-18},
  abstract = {This paper addresses the problem of exponentially stabilizing periodic orbits in a special class of hybrid models-systems with impulse effects-through control Lyapunov functions. The periodic orbit is assumed to lie in a C1 submanifold Z that is contained in the zero set of an output function and is invariant under both the continuous and discrete dynamics; the associated restriction dynamics are termed the hybrid zero dynamics. The orbit is furthermore assumed to be exponentially stable within the hybrid zero dynamics. Prior results on the stabilization of such periodic orbits with respect to the full-order dynamics of the system with impulse effects have relied on input-output linearization of the dynamics transverse to the zero dynamics manifold. The principal result of this paper demonstrates that a variant of control Lyapunov functions that enforce rapid exponential convergence to the zero dynamics surface, Z, can be used to achieve exponential stability of the periodic orbit in the full-order dynamics, thereby significantly extending the class of stabilizing controllers. The main result is illustrated on a hybrid model of a bipedal walking robot through simulations and is utilized to experimentally achieve bipedal locomotion via control Lyapunov functions.},
  langid = {english},
  keywords = {#nosource}
}

@inproceedings{anandRealtimeTemporalAdaptation2021,
  title = {Real-Time Temporal Adaptation of Dynamic Movement Primitives for Moving Targets},
  booktitle = {2021 20th {{International Conference}} on {{Advanced Robotics}} ({{ICAR}})},
  author = {Anand, Akhil S and Østvik, Andreas and Grøtli, Esten Ingar and Vagia, Marialena and Gravdahl, Jan Tommy},
  date = {2021-12},
  pages = {261--268},
  doi = {10.1109/ICAR53236.2021.9659384},
  abstract = {This work is aimed at extending the standard dynamic movement primitives (DMP) framework to adapt to real-time changes in the task execution time while preserving its style characteristics. We propose an alternative polynomial canonical system and an adaptive law allowing a higher degree of control over the execution time. The extended framework has a potential application in robotic manipulation tasks that involve moving objects demanding real-time control over the task execution time. The existing methods require a computationally expensive forward simulation of DMP at every time step which makes it undesirable for integration in realtime control systems. To address this deficiency, the behaviour of the canonical system has been adapted according to the changes in the desired execution time of the task performed. An alternative polynomial canonical system is proposed to provide increased real-time control on the temporal scaling of DMP system compared to the standard exponential canonical system. The developed method was evaluated on scenarios of tracking a moving target where the desired tracking time is varied in real-time. The results presented show that the extended version of DMP provide better control over the temporal scaling during the execution of the task. We have evaluated our approach on a UR5 robotic manipulator for tracking a moving object.},
  eventtitle = {2021 20th {{International Conference}} on {{Advanced Robotics}} ({{ICAR}})},
  keywords = {Adaptive systems,Control systems,Estimation,READ,Real-time systems,Service robots,Shape,Target tracking},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/anand_et_al_2021_real-time_temporal_adaptation_of_dynamic_movement_primitives_for_moving_targets.pdf}
}

@book{andersonNetworkAnalysisSynthesis2013,
  title = {Network Analysis and Synthesis: A Modern Systems Theory Approach},
  shorttitle = {Network Analysis and Synthesis},
  author = {Anderson, Brian DO and Vongpanitlerd, Sumeth},
  date = {2013},
  publisher = {{Courier Corporation}}
}

@unpublished{andrychowiczHindsightExperienceReplay2018,
  title = {Hindsight {{Experience Replay}}},
  author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
  date = {2018-02-23},
  eprint = {1707.01495},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1707.01495},
  urldate = {2020-07-02},
  abstract = {Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics,model-free,multitask RL,q-learning,reinforcement learning,transfer learning}
}

@unpublished{anthonyThinkingFastSlow2017,
  title = {Thinking {{Fast}} and {{Slow}} with {{Deep Learning}} and {{Tree Search}}},
  author = {Anthony, Thomas and Tian, Zheng and Barber, David},
  date = {2017-12-03},
  eprint = {1705.08439},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1705.08439},
  urldate = {2020-07-02},
  abstract = {Sequential decision making problems, such as structured prediction, robotic control, and game playing, require a combination of planning policies and generalisation of those plans. In this paper, we present Expert Iteration (ExIt), a novel reinforcement learning algorithm which decomposes the problem into separate planning and generalisation tasks. Planning new policies is performed by tree search, while a deep neural network generalises those plans. Subsequently, tree search is improved by using the neural network policy to guide search, increasing the strength of new plans. In contrast, standard deep Reinforcement Learning algorithms rely on a neural network not only to generalise plans, but to discover them too. We show that ExIt outperforms REINFORCE for training a neural network to play the board game Hex, and our final tree search agent, trained tabula rasa, defeats MoHex 1.0, the most recent Olympiad Champion player to be publicly released.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,designed model,model-based,reinforcement learning}
}

@book{antsaklisLinearSystemsPrimer2007,
  title = {A Linear Systems Primer},
  author = {Antsaklis, Panos J. and Michel, Anthony N.},
  date = {2007},
  publisher = {{Springer Science \& Business Media}},
  keywords = {IMPORTANT,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/antsaklis_michel_2007_a_linear_systems_primer.pdf}
}

@book{arakelianDynamicDecouplingRobot2018,
  title = {Dynamic {{Decoupling}} of {{Robot Manipulators}}},
  editor = {Arakelian, Vigen},
  date = {2018},
  series = {Mechanisms and {{Machine Science}}},
  volume = {56},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-74363-9},
  url = {http://link.springer.com/10.1007/978-3-319-74363-9},
  urldate = {2022-08-04},
  isbn = {978-3-319-74362-2 978-3-319-74363-9},
  langid = {english},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/arakelian_2018_dynamic_decoupling_of_robot_manipulators.pdf;/home/ricks/Zotero/storage/5BW9QY4F/978-3-319-74363-9.pdf}
}

@unpublished{arduengoGaussianProcessbasedRobotLearning2020,
  title = {Gaussian-{{Process-based Robot Learning}} from {{Demonstration}}},
  author = {Arduengo, Miguel and Colomé, Adrià and Lobo-Prat, Joan and Sentis, Luis and Torras, Carme},
  date = {2020-05-28},
  eprint = {2002.09979},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2002.09979},
  urldate = {2022-04-28},
  abstract = {Endowed with higher levels of autonomy, robots are required to perform increasingly complex manipulation tasks. Learning from demonstration is arising as a promising paradigm for transferring skills to robots. It allows to implicitly learn task constraints from observing the motion executed by a human teacher, which can enable adaptive behavior. We present a novel Gaussian-Process-based learning from demonstration approach. This probabilistic representation allows to generalize over multiple demonstrations, and encode variability along the different phases of the task. In this paper, we address how Gaussian Processes can be used to effectively learn a policy from trajectories in task space. We also present a method to efficiently adapt the policy to fulfill new requirements, and to modulate the robot behavior as a function of task variability. This approach is illustrated through a real-world application using the TIAGo robot.},
  keywords = {Computer Science - Robotics,READ,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/arduengo_et_al_2020_gaussian-process-based_robot_learning_from_demonstration.pdf}
}

@article{artigasTimeDomainPassivity2010,
  title = {Time {{Domain Passivity Control}} for {{Position-Position Teleoperation Architectures}}},
  author = {Artigas, Jordi and Ryu, Jee-Hwan and Preusche, Carsten},
  date = {2010-10},
  journaltitle = {Presence},
  volume = {19},
  number = {5},
  pages = {482--497},
  issn = {1054-7460},
  doi = {10.1162/pres_a_00013},
  abstract = {This article presents a method for passivating the communication channel of a symmetric position-position teleoperation architecture on the time domain. The time domain passivity control approach has recently gained appeal in the context of timedelayed teleoperation because passivity is not established as a design constraint, which often forces conservative rules, but rather as a property which the system must preserve during operation. Since passivity is a network property, the first design rule within this framework is to represent consistent and comprehensible circuit (i.e., network) representations of the mechanical teleoperation system. In particular, the energetic behavior of these networks is interesting because it allows straightforward conclusions about system stability. By means of so-called passivity observers (PO) and passivity controllers (PC) (Hannaford \& Ryu, 2001), the energetic response of a delayed communication channel is captured and modulated over time so that the network in question never becomes nonpassive. The case analyzed in this paper tackles a communication channel that conveys position data back and forth. This type of channel does not offer intuitive network representation since only flows are actually being transmitted. Although energy clearly travels from one side to the other, port power identification, as defined by the correlated pair flow and effort, is not evident. This work first investigates how this kind of channel can be represented by means of circuit networks even with the lack of physical effort being transmitted through the channel, and identifies which networks are susceptible to become nonpassive due to the channel characteristics (i.e., time delay, discretization or package loss). Once achieved, a distributed control structure is presented based on a PC series that keeps the system at the verge of passivity (and therefore stability) independent from the channel properties. The results obtained by the simulation and by experiment sustain the presented approach.},
  eventtitle = {Presence},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/artigas_et_al_2010_time_domain_passivity_control_for_position-position_teleoperation_architectures2.pdf}
}

@article{asifRGBDObjectRecognition2017,
  title = {{{RGB-D}} Object Recognition and Grasp Detection Using Hierarchical Cascaded Forests},
  author = {Asif, Umar and Bennamoun, Mohammed and Sohel, Ferdous A.},
  date = {2017},
  journaltitle = {IEEE Transactions on Robotics},
  volume = {33},
  number = {3},
  pages = {547--564},
  keywords = {#nosource,grasp pose detection,robotic grasping,tno internship},
  annotation = {00032}
}

@online{AutonomousRoboticManipulation,
  title = {Autonomous {{Robotic Manipulation}} | {{Autonomous Motion}} - {{Max Planck Institute}} for {{Intelligent Systems}}},
  url = {https://am.is.mpg.de/research_projects/autonomous-robotic-manipulation},
  urldate = {2022-09-14}
}

@unpublished{babarahmatiFractalImpedancePassive2019,
  title = {Fractal Impedance for Passive Controllers},
  author = {Babarahmati, Keyhan Kouhkiloui and Tiseo, Carlo and Smith, Joshua and Lin, Hsiu Chin and Erden, Mustafa Suphi and Mistry, Michael},
  date = {2019},
  eprint = {1911.04788},
  eprinttype = {arxiv},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/babarahmati_et_al_2019_fractal_impedance_for_passive_controllers.pdf}
}

@unpublished{babarahmatiFractalImpedancePassive2021,
  title = {Fractal {{Impedance}} for {{Passive Controllers}}},
  author = {Babarahmati, Keyhan Kouhkiloui and Tiseo, Carlo and Smith, Joshua and Lin, Hsiu Chin and Erden, Mustafa Suphi and Mistry, Michael},
  date = {2021-05-28},
  eprint = {1911.04788},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1911.04788},
  urldate = {2022-04-28},
  abstract = {There is increasing interest in control frameworks capable of moving robots from industrial cages to unstructured environments and coexisting with humans. Despite significant improvement in some specific applications (e.g., medical robotics), there is still the need for a general control framework that improves interaction robustness and motion dynamics. Passive controllers show promising results in this direction; however, they often rely on virtual energy tanks that can guarantee passivity as long as they do not run out of energy. In this paper, a fractal attractor is proposed to implement a variable impedance controller that can retain passivity without relying on energy tanks. The controller generates a fractal attractor around the desired state using an asymptotic stable potential field, making the controller robust to discretization and numerical integration errors. The results prove that it can accurately track both trajectories and end-effector forces during interaction. Therefore, these properties make the controller ideal for applications requiring robust dynamic interaction at the end-effector.},
  keywords = {Computer Science - Robotics,convergence analysis,IMPORTANT,READ,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/babarahmati_et_al_2021_fractal_impedance_for_passive_controllers.pdf}
}

@article{babarahmatiFractalImpedancePassive2022,
  title = {Fractal {{Impedance}} for {{Passive Controllers}}: {{A Framework}} for {{Interaction Robotics}}},
  shorttitle = {Fractal {{Impedance}} for {{Passive Controllers}}},
  author = {Babarahmati, Keyhan Kouhkiloui and Tiseo, Carlo and Smith, Joshua and Lin, Hsiu Chin and Erden, Mustafa Suphi and Mistry, Michael},
  date = {2022-11},
  journaltitle = {Nonlinear Dynamics},
  shortjournal = {Nonlinear Dyn},
  volume = {110},
  number = {3},
  eprint = {1911.04788},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {2517--2533},
  issn = {0924-090X, 1573-269X},
  doi = {10.1007/s11071-022-07754-3},
  url = {http://arxiv.org/abs/1911.04788},
  urldate = {2023-04-25},
  abstract = {There is increasing interest in control frameworks capable of moving robots from industrial cages to unstructured environments and coexisting with humans. Despite significant improvement in some specific applications (e.g., medical robotics), there is still the need for a general control framework that improves interaction robustness and motion dynamics. Passive controllers show promising results in this direction; however, they often rely on virtual energy tanks that can guarantee passivity as long as they do not run out of energy. In this paper, a Fractal Attractor is proposed to implement a variable impedance controller that can retain passivity without relying on energy tanks. The controller generates a Fractal Attractor around the desired state using an asymptotic stable potential field, making the controller robust to discretization and numerical integration errors. The results prove that it can accurately track both trajectories and end-effector forces during interaction. Therefore, these properties make the controller ideal for applications requiring robust dynamic interaction at the end-effector.},
  keywords = {Computer Science - Robotics},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/babarahmati_et_al_2022_fractal_impedance_for_passive_controllers.pdf;/home/ricks/Insync/OneDrive/Documents/Zotero/babarahmati_et_al_2022_fractal_impedance_for_passive_controllers2.pdf}
}

@inproceedings{babarahmatiRobustHighTransparencyHaptic2021,
  title = {Robust {{High-Transparency Haptic Exploration}} for {{Dexterous Telemanipulation}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Babarahmati, Keyhan Kouhkiloui and Tiseo, Carlo and Rouxel, Quentin and Li, Zhibin and Mistry, Michael},
  date = {2021-05},
  pages = {10146--10152},
  issn = {2577-087X},
  doi = {10.1109/ICRA48506.2021.9561968},
  abstract = {Robotic teleoperation provides human-in-the-loop capabilities of complex manipulation tasks in dangerous or remote environments, such as for planetary exploration or nuclear decommissioning. This work proposes a novel telemanipulation architecture using a passive Fractal Impedance Controller (FIC), which does not depend upon an active viscous component for guaranteeing stability. Compared to a traditional impedance controller in ideal conditions (no delays and maximum communication bandwidth), our proposed method yields higher transparency in interaction and demonstrates superior dexterity and capability in our telemanipulation test scenarios. We also validate its performance with extreme delays up to 1 s and communication bandwidths as low as 10 Hz. All results validate a consistent stability when using the proposed controller in challenging conditions, regardless of operator expertise.},
  eventtitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  keywords = {Automation,Bandwidth,Conferences,convergence analysis,Delays,Fractals,Haptic interfaces,Impedance,READ,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/babarahmati_et_al_2021_robust_high-transparency_haptic_exploration_for_dexterous_telemanipulation.pdf}
}

@book{bacciottiStabilityControlLinear2019,
  title = {Stability and Control of Linear Systems},
  author = {Bacciotti, Andrea},
  date = {2019},
  publisher = {{Springer}},
  keywords = {IMPORTANT,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/bacciotti_2019_stability_and_control_of_linear_systems.pdf}
}

@inproceedings{baiReachableWorkspaceRobotassisted2022,
  title = {Reachable Workspace and {{Robot-assisted}} Personalized Rehabilitation Training of Upper Limb},
  booktitle = {2022 10th {{International Conference}} on {{Affective Computing}} and {{Intelligent Interaction Workshops}} and {{Demos}} ({{ACIIW}})},
  author = {Bai, Jing and Wen, Xiulan and Nie, Jieyan},
  date = {2022-10},
  pages = {1--6},
  doi = {10.1109/ACIIW57231.2022.10085998},
  abstract = {The increasing number of patients with upper extremity hemiplegia seriously affects their activities of daily living, and robot-assisted rehabilitation training reduce the burden on therapists. Due to the different conditions of patients, the need for personalized rehabilitation training methods is obvious. This paper proposes a method to meet the needs of personalized rehabilitation training. A 9-DOF upper limb kinematic model is constructed, including 2-DOF at the sternoclavicular joint, 3-DOF at the shoulder, 2-DOF at the elbow and 2-DOF at the wrist. A combination of the geometric method and the Levenberg-Marquardt (LM) algorithm is proposed to analyze the inverse kinematics of the upper limbs, the geometric method is used to calculate the initial value, and the LM is iteratively calculated to optimize the trajectory of the upper limbs. The reachable workspace is mapped to the working space of the robot, and a method of combining the reachable workspace of the upper limbs with the nonlinear potential field function is proposed to adjust the change of the force field. Thus, personalized rehabilitation training based on the motion range of the affected limb is realized to meet the needs of different patients. Finally, the effectiveness of the proposed method is verified by experiments.},
  eventtitle = {2022 10th {{International Conference}} on {{Affective Computing}} and {{Intelligent Interaction Workshops}} and {{Demos}} ({{ACIIW}})},
  keywords = {Affective computing,Conferences,Force,kinematics,Kinematics,nonlinear potential field function,reachable workspace,READ,robot,Shoulder,STABILITY,Training,Wrist},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/bai_et_al_2022_reachable_workspace_and_robot-assisted_personalized_rehabilitation_training_of.pdf}
}

@article{balattiMethodAutonomousRobotic2020,
  title = {A Method for Autonomous Robotic Manipulation through Exploratory Interactions with Uncertain Environments},
  author = {Balatti, Pietro and Kanoulas, Dimitrios and Tsagarakis, Nikos and Ajoudani, Arash},
  date = {2020-11-01},
  journaltitle = {Autonomous Robots},
  shortjournal = {Auton Robot},
  volume = {44},
  number = {8},
  pages = {1395--1410},
  issn = {1573-7527},
  doi = {10.1007/s10514-020-09933-w},
  url = {https://doi.org/10.1007/s10514-020-09933-w},
  urldate = {2022-08-09},
  abstract = {Expanding robot autonomy can deliver functional flexibility and enable fast deployment of robots in challenging and unstructured environments. In this direction, significant advances have been recently made in visual-perception driven autonomy, which is mainly due to the availability of rich sensory data-sets. However, current robots’ physical interaction autonomy levels still remain at a basic level. Towards providing a systematic approach to this problem, this paper presents a new context-aware and adaptive method that allows a robotic platform to interact with unknown environments. In particular, a multi-axes self-tuning impedance controller is introduced to regulate quasi-static parameters of the robot based on previous experience in interacting with similar environments and the real-time sensory data. The proposed method is also capable of differentiating internal and external disruptions, and responding to them accordingly and appropriately. An agricultural experiment with different deformable material is presented to validate robot interaction autonomy improvements, and the capability of the proposed methodology in detecting and responding to unexpected events (e.g., faults).},
  langid = {english},
  keywords = {Adaptive control,Impedance control,Interaction autonomy,READ,Robotic manipulation},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/balatti_et_al_2020_a_method_for_autonomous_robotic_manipulation_through_exploratory_interactions.pdf}
}

@book{baoProcessControlPassive2007,
  title = {Process Control: The Passive Systems Approach},
  shorttitle = {Process Control},
  author = {Bao, Jie and Lee, Peter L.},
  date = {2007},
  series = {Advances in Industrial Control},
  publisher = {{Springer}},
  location = {{London}},
  isbn = {978-1-84628-892-0 978-1-84628-893-7},
  langid = {english},
  pagetotal = {253},
  keywords = {IMPORTANT,Passivity-based control,Process control,STABILITY},
  annotation = {OCLC: ocm86168239},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/bao_lee_2007_process_control.pdf}
}

@incollection{batkaiKoopmanSemigroups2017,
  title = {Koopman {{Semigroups}}},
  booktitle = {Positive {{Operator Semigroups}}: {{From Finite}} to {{Infinite Dimensions}}},
  author = {Bátkai, András and Fijavž, Marjeta Kramar and Rhandi, Abdelaziz},
  editor = {Bátkai, András and Kramar Fijavž, Marjeta and Rhandi, Abdelaziz},
  date = {2017},
  series = {Operator {{Theory}}: {{Advances}} and {{Applications}}},
  pages = {253--267},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-42813-0_16},
  url = {https://doi.org/10.1007/978-3-319-42813-0_16},
  urldate = {2022-12-14},
  abstract = {We present here a class of positive operator semigroups that arise in studying dynamical systems. The main idea is to linearize a given (nonlinear) system by considering another state space. The linear operator which acts on this new space is called the Koopman operator. It is named after B. O. Koopman, who used this in the 1930s together with G. D. Birkhoff and J. von Neumann to prove the so-called ergodic theorems.},
  isbn = {978-3-319-42813-0},
  langid = {english},
  keywords = {Banach Lattice,Continuous Semigroup,Contraction Semigroup,Invariant Measure,Invariant Probability Measure},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/bátkai_et_al_2017_koopman_semigroups.pdf}
}

@article{bednarczykEMGBasedVariableImpedance2022,
  title = {{{EMG-Based Variable Impedance Control With Passivity Guarantees}} for {{Collaborative Robotics}}},
  author = {Bednarczyk, Maciej and Omran, Hassan and Bayle, Bernard},
  date = {2022-04},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {2},
  pages = {4307--4312},
  issn = {2377-3766},
  doi = {10.1109/LRA.2022.3149575},
  abstract = {In this paper, a new methodology is developed for safely changing the interaction dynamics of a collaborative robot. A strategy based on electromyography is proposed to distinguish operator forces from those resulting from interactions with the environment. This allows to obtain information about the operator intentions and include it into the robot control strategy for an enhanced physical human–robot interaction. The safety of the resulting variable impedance control is guaranteed by imposing the passivity of the interaction. Experimental validation shows a good performance of the proposed method and illustrates the advantages of such a strategy in cases where human, robot and environment interact with each other.},
  eventtitle = {{{IEEE Robotics}} and {{Automation Letters}}},
  keywords = {Collaboration,Compliance and Impedance Control,Electromyography,Force measurement,Human-Robot Collaboration,Impedance,Physical Human-Robot Interaction,Robot sensing systems,Robots,Task analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/bednarczyk_et_al_2022_emg-based_variable_impedance_control_with_passivity_guarantees_for.pdf}
}

@inproceedings{bednarczykModelPredictiveImpedance2020,
  title = {Model {{Predictive Impedance Control}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Bednarczyk, Maciej and Omran, Hassan and Bayle, Bernard},
  date = {2020-05},
  pages = {4702--4708},
  publisher = {{IEEE}},
  location = {{Paris, France}},
  doi = {10.1109/ICRA40945.2020.9196969},
  url = {https://ieeexplore.ieee.org/document/9196969/},
  urldate = {2022-05-02},
  abstract = {Robots are more and more often designed in order to perform tasks in synergy with human operators. In this context, a current research focus for collaborative robotics lies in the design of high-performance control solutions, which ensure security in spite of unmodeled external forces. The present work provides a method based on Model Predictive Control (MPC) to allow compliant behavior when interacting with an environment, while respecting practical robotic constraints. The study shows in particular how to define the impedance control problem as a MPC problem. The approach is validated with an experimental setup including a collaborative robot. The obtained results emphasize the ability of this control strategy to solve constraints like speed, energy or jerk limits, which have a direct impact on the operator’s security during human-robot compliant interactions.},
  eventtitle = {2020 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  isbn = {978-1-72817-395-5},
  langid = {english},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/bednarczyk_et_al_2020_model_predictive_impedance_control.pdf}
}

@inproceedings{bednarczykPassivityFilterVariable2020,
  title = {Passivity {{Filter}} for {{Variable Impedance Control}}},
  booktitle = {2020 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Bednarczyk, Maciej and Omran, Hassan and Bayle, Bernard},
  date = {2020-10},
  pages = {7159--7164},
  issn = {2153-0866},
  doi = {10.1109/IROS45743.2020.9340758},
  abstract = {While impedance control is one of the most commonly used strategies for robot interaction control, variable impedance control is a more recent preoccupation. If designing impedance control with varying parameters allows increasing the system flexibility and dexterity, it is still a challenging issue, as it may result in a loss of passivity of the control system. This has an important impact on the stability and therefore on the safety of the interaction. In this paper, we propose methods to design passivity filters that guarantee passivity of the interaction. They aim at either checking whether a desired impedance profile is passive, or modifying it if required.},
  eventtitle = {2020 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  keywords = {Control systems,Design methodology,Impedance,IMPORTANT,Intelligent robots,READ,Safety,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/bednarczyk_et_al_2020_passivity_filter_for_variable_impedance_control.pdf}
}

@unpublished{bellemareDistributionalPerspectiveReinforcement2017,
  title = {A {{Distributional Perspective}} on {{Reinforcement Learning}}},
  author = {Bellemare, Marc G. and Dabney, Will and Munos, Rémi},
  date = {2017-07-21},
  eprint = {1707.06887},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1707.06887},
  urldate = {2020-07-02},
  abstract = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,distributional RL,model-free,q-learning,reinforcement learning,Statistics - Machine Learning}
}

@unpublished{bellemareUnifyingCountBasedExploration2016,
  title = {Unifying {{Count-Based Exploration}} and {{Intrinsic Motivation}}},
  author = {Bellemare, Marc G. and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
  date = {2016-11-07},
  eprint = {1606.01868},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1606.01868},
  urldate = {2020-07-02},
  abstract = {We consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across observations. Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. Drawing inspiration from the intrinsic motivation literature, we use density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary density model. This technique enables us to generalize count-based exploration algorithms to the non-tabular case. We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We transform these pseudo-counts into intrinsic rewards and obtain significantly improved exploration in a number of hard games, including the infamously difficult Montezuma's Revenge.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,exploration,intrinsic motivation,reinforcement learning,Statistics - Machine Learning}
}

@online{BenchmarksSpinningImplementations,
  title = {Benchmarks for {{Spinning Up Implementations}} — {{Spinning Up}} Documentation},
  url = {https://spinningup.openai.com/en/latest/spinningup/bench.html},
  urldate = {2020-06-29},
  keywords = {#nosource,benchmarks,reinforcement learning},
  annotation = {00000}
}

@article{benziEnergyBasedControlArchitecture2022,
  title = {An {{Energy-Based Control Architecture}} for {{Shared Autonomy}}},
  author = {Benzi, Federico and Ferraguti, Federica and Riggio, Giuseppe and Secchi, Cristian},
  date = {2022},
  journaltitle = {IEEE Transactions on Robotics},
  pages = {1--19},
  issn = {1941-0468},
  doi = {10.1109/TRO.2022.3180885},
  abstract = {In robotic applications where the autonomy is shared between the human and the robot, the autonomous behavior of the robotic system is determined considering mainly the task to be executed and the data collected from the environment using, e.g., formal methods and machine learning techniques. Nevertheless, it is important to correctly translate high-level decision into low-level control inputs in order to avoid an unstable behavior due to a naive implementation of the autonomy. In this article, we propose an energy-based architecture for shared autonomy that allows to reproduce as closely as possible the desired behavior, while ensuring a robust stability of the robotic system. The proposed architecture is experimentally validated in two application scenarios: shared control of a multirobot system and variable admittance control in human robot collaboration},
  eventtitle = {{{IEEE Transactions}} on {{Robotics}}},
  keywords = {Behavioral sciences,Collaboration,Couplings,Human-centered robotics,Multi-robot systems,multirobot systems,Optimization,optimization and optimal control,physical human–robot interaction,Robots,Task analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/benzi_et_al_2022_an_energy-based_control_architecture_for_shared_autonomy.pdf}
}

@inproceedings{benziOptimizationApproachRobust2021,
  title = {An {{Optimization Approach}} for a {{Robust}} and {{Flexible Control}} in {{Collaborative Applications}}},
  booktitle = {2021 {{Ieee International Conference}} on {{Robotics}} and {{Automation}} (Icra 2021)},
  author = {Benzi, Federico and Secchi, Cristian},
  date = {2021},
  pages = {3575--3581},
  publisher = {{Ieee}},
  location = {{New York}},
  issn = {1050-4729},
  doi = {10.1109/ICRA48506.2021.9561098},
  url = {https://ieeexplore.ieee.org/document/9561098},
  urldate = {2022-08-09},
  abstract = {In Human-Robot Collaboration, the robot operates in a highly dynamic environment. Thus, it is pivotal to guarantee the robust stability of the system during the interaction but also a high flexibility of the robot behavior in order to ensure safety and reactivity to the variable conditions of the collaborative scenario. In this paper we propose a control architecture capable of maximizing the flexibility of the robot while guaranteeing a stable behavior when physically interacting with the environment. This is achieved by combining an energy tank based variable admittance architecture with control barrier functions. The proposed architecture is experimentally validated on a collaborative robot.},
  isbn = {978-1-72819-077-8},
  langid = {english},
  keywords = {robot,safety,strategy},
  annotation = {WOS:000765738802122},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/benzi_secchi_2021_an_optimization_approach_for_a_robust_and_flexible_control_in_collaborative.pdf}
}

@online{berkenkampSafeModelbasedReinforcement2017,
  title = {Safe {{Model-based Reinforcement Learning}} with {{Stability Guarantees}}},
  author = {Berkenkamp, Felix and Turchetta, Matteo and Schoellig, Angela P. and Krause, Andreas},
  date = {2017-11-13},
  eprint = {1705.08551},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1705.08551},
  url = {http://arxiv.org/abs/1705.08551},
  urldate = {2022-08-23},
  abstract = {Reinforcement learning is a powerful paradigm for learning optimal policies from experimental data. However, to find optimal policies, most reinforcement learning algorithms explore all possible actions, which may be harmful for real-world systems. As a consequence, learning algorithms are rarely applied on safety-critical systems in the real world. In this paper, we present a learning algorithm that explicitly considers safety, defined in terms of stability guarantees. Specifically, we extend control-theoretic results on Lyapunov stability verification and show how to use statistical models of the dynamics to obtain high-performance control policies with provable stability certificates. Moreover, under additional regularity assumptions in terms of a Gaussian process prior, we prove that one can effectively and safely collect data in order to learn about the dynamics and thus both improve control performance and expand the safe region of the state space. In our experiments, we show how the resulting algorithm can safely optimize a neural network policy on a simulated inverted pendulum, without the pendulum ever falling down.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,READ,Statistics - Machine Learning},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/berkenkamp_et_al_2017_safe_model-based_reinforcement_learning_with_stability_guarantees.pdf}
}

@article{bevandaDatadrivenLQRKoopmanizing2022,
  title = {Towards {{Data-driven LQR}} with {{Koopmanizing Flows}}⋆},
  author = {Bevanda, Petar and Beier, Max and Heshmati-Alamdari, Shahab and Sosnowski, Stefan and Hirche, Sandra},
  date = {2022-01-01},
  journaltitle = {IFAC-PapersOnLine},
  shortjournal = {IFAC-PapersOnLine},
  series = {6th {{IFAC Conference}} on {{Intelligent Control}} and {{Automation Sciences ICONS}} 2022},
  volume = {55},
  number = {15},
  pages = {13--18},
  issn = {2405-8963},
  doi = {10.1016/j.ifacol.2022.07.601},
  url = {https://www.sciencedirect.com/science/article/pii/S2405896322010126},
  urldate = {2023-01-30},
  abstract = {We propose a novel framework for learning linear time-invariant (LTI) models for a class of continuous-time non-autonomous nonlinear dynamics based on a representation of Koopman operators. In general, the operator is infinite-dimensional but, crucially, linear. To utilize it for effcient LTI control design, we learn a finite representation of the Koopman operator that is linear in controls while concurrently learning meaningful lifting coordinates. For the latter, we rely on Koopmanizing Flows - a diffeomorphism-based representation of Koopman operators and extend it to systems with linear control entry. With such a learned model, we can replace the nonlinear optimal control problem with quadratic cost to that of a linear quadratic regulator (LQR), facilitating efficacious optimal control for nonlinear systems. The superior control performance of the proposed method is demonstrated on simulation examples.},
  langid = {english},
  keywords = {Koopman operators,Learning for control,Learning Systems,Machine learning,Neural networks,READ,Representation Learning},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/bevanda_et_al_2022_towards_data-driven_lqr_with_koopmanizing_flows⋆.pdf}
}

@online{bevandaDiffeomorphicallyLearningStable2022,
  title = {Diffeomorphically {{Learning Stable Koopman Operators}}},
  author = {Bevanda, Petar and Beier, Max and Kerz, Sebastian and Lederer, Armin and Sosnowski, Stefan and Hirche, Sandra},
  date = {2022-05-30},
  eprint = {2112.04085},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  doi = {10.48550/arXiv.2112.04085},
  url = {http://arxiv.org/abs/2112.04085},
  urldate = {2023-01-18},
  abstract = {System representations inspired by the infinite-dimensional Koopman operator (generator) are increasingly considered for predictive modeling. Due to the operator's linearity, a range of nonlinear systems admit linear predictor representations - allowing for simplified prediction, analysis and control. However, finding meaningful finite-dimensional representations for prediction is difficult as it involves determining features that are both Koopman-invariant (evolve linearly under the dynamics) as well as relevant (spanning the original state) - a generally unsupervised problem. In this work, we present Koopmanizing Flows - a novel continuous-time framework for supervised learning of linear predictors for a class of nonlinear dynamics. In our model construction a latent diffeomorphically related linear system unfolds into a linear predictor through the composition with a monomial basis. The lifting, its linear dynamics and state reconstruction are learned simultaneously, while an unconstrained parameterization of Hurwitz matrices ensures asymptotic stability regardless of the operator approximation accuracy. The superior efficacy of Koopmanizing Flows is demonstrated in comparison to a state-of-the-art method on the well-known LASA handwriting benchmark.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,READ,TO READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/bevanda_et_al_2022_diffeomorphically_learning_stable_koopman_operators.pdf}
}

@article{bevandaKoopmanOperatorDynamical2021,
  title = {Koopman Operator Dynamical Models: {{Learning}}, Analysis and Control},
  shorttitle = {Koopman Operator Dynamical Models},
  author = {Bevanda, Petar and Sosnowski, Stefan and Hirche, Sandra},
  date = {2021-01-01},
  journaltitle = {Annual Reviews in Control},
  shortjournal = {Annual Reviews in Control},
  volume = {52},
  pages = {197--212},
  issn = {1367-5788},
  doi = {10.1016/j.arcontrol.2021.09.002},
  url = {https://www.sciencedirect.com/science/article/pii/S1367578821000729},
  urldate = {2023-01-19},
  abstract = {The Koopman operator allows for handling nonlinear systems through a globally linear representation. In general, the operator is infinite-dimensional – necessitating finite approximations – for which there is no overarching framework. Although there are principled ways of learning such finite approximations, they are in many instances overlooked in favor of, often ill-posed and unstructured methods. Also, Koopman operator theory has long-standing connections to known system-theoretic and dynamical system notions that are not universally recognized. Given the former and latter realities, this work aims to bridge the gap between various concepts regarding both theory and tractable realizations. Firstly, we review data-driven representations (both unstructured and structured) for Koopman operator dynamical models, categorizing various existing methodologies and highlighting their differences. Furthermore, we provide concise insight into the paradigm’s relation to system-theoretic notions and analyze the prospect of using the paradigm for modeling control systems. Additionally, we outline the current challenges and comment on future perspectives.},
  langid = {english},
  keywords = {Data-based control,Dynamical models,Koopman operator,READ,Representation learning,REVIEW,STABILITY,System analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/bevanda_et_al_2021_koopman_operator_dynamical_models.pdf}
}

@online{bevandaLearningKoopmanEigendecomposition2022,
  title = {Learning the {{Koopman Eigendecomposition}}: {{A Diffeomorphic Approach}}},
  shorttitle = {Learning the {{Koopman Eigendecomposition}}},
  author = {Bevanda, Petar and Kirmayr, Johannes and Sosnowski, Stefan and Hirche, Sandra},
  date = {2022-05-30},
  eprint = {2110.07786},
  eprinttype = {arxiv},
  eprintclass = {cs, eess, math},
  url = {http://arxiv.org/abs/2110.07786},
  urldate = {2022-12-05},
  abstract = {We present a novel data-driven approach for learning linear representations of a class of stable nonlinear systems using Koopman eigenfunctions. By learning the conjugacy map between a nonlinear system and its Jacobian linearization through a Normalizing Flow one can guarantee the learned function is a diffeomorphism. Using this diffeomorphism, we construct eigenfunctions of the nonlinear system via the spectral equivalence of conjugate systems - allowing the construction of linear predictors for nonlinear systems. The universality of the diffeomorphism learner leads to the universal approximation of the nonlinear system's Koopman eigenfunctions. The developed method is also safe as it guarantees the model is asymptotically stable regardless of the representation accuracy. To our best knowledge, this is the first work to close the gap between the operator, system and learning theories. The efficacy of our approach is shown through simulation examples.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,Mathematics - Dynamical Systems,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/bevanda_et_al_2022_learning_the_koopman_eigendecomposition.pdf}
}

@unpublished{bharadhwajDataEfficientFrameworkTraining2018,
  title = {A {{Data-Efficient Framework}} for {{Training}} and {{Sim-to-Real Transfer}} of {{Navigation Policies}}},
  author = {Bharadhwaj, Homanga and Wang, Zihan and Bengio, Yoshua and Paull, Liam},
  date = {2018},
  eprint = {1810.04871},
  eprinttype = {arxiv},
  keywords = {#nosource,robotic grasping,sim-to-real,tno internship}
}

@inproceedings{bicchiRoboticGraspingContact2000,
  title = {Robotic Grasping and Contact: A Review},
  shorttitle = {Robotic Grasping and Contact},
  booktitle = {Proceedings 2000 {{ICRA}}. {{Millennium Conference}}. {{IEEE International Conference}} on {{Robotics}} and {{Automation}}. {{Symposia Proceedings}} ({{Cat}}. {{No}}.{{00CH37065}})},
  author = {Bicchi, A. and Kumar, V.},
  date = {2000},
  volume = {1},
  pages = {348--353},
  publisher = {{IEEE}},
  location = {{San Francisco, CA, USA}},
  doi = {10.1109/ROBOT.2000.844081},
  url = {http://ieeexplore.ieee.org/document/844081/},
  urldate = {2019-05-06},
  abstract = {In this paper, we survey the field of robotic grasping and the work that has been done in this area over the last two decades, with a slight bias toward the development of the theoretical framework and analytical results in this area.},
  eventtitle = {2000 {{ICRA}}. {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  isbn = {978-0-7803-5886-7},
  langid = {english},
  keywords = {#nosource,machine learning,object grasping,robotic grasping,tno internship},
  annotation = {00908}
}

@inproceedings{blocherLearningStableDynamical2017,
  title = {Learning Stable Dynamical Systems Using Contraction Theory},
  booktitle = {2017 14th {{International Conference}} on {{Ubiquitous Robots}} and {{Ambient Intelligence}} ({{URAI}})},
  author = {Blocher, Caroline and Saveriano, Matteo and Lee, Dongheui},
  date = {2017},
  pages = {124--129},
  publisher = {{IEEE}},
  keywords = {Asymptotic stability,Convergence,Jacobian matrices,Learning contracting systems. Stable discrete movements. Learning from demonstration. Contraction theory,Lyapunov methods,READ,Robots,Stability analysis,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/blocher_et_al_2017_learning_stable_dynamical_systems_using_contraction_theory.pdf}
}

@unpublished{blundellModelFreeEpisodicControl2016,
  title = {Model-{{Free Episodic Control}}},
  author = {Blundell, Charles and Uria, Benigno and Pritzel, Alexander and Li, Yazhe and Ruderman, Avraham and Leibo, Joel Z. and Rae, Jack and Wierstra, Daan and Hassabis, Demis},
  date = {2016-06-14},
  eprint = {1606.04460},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio, stat},
  url = {http://arxiv.org/abs/1606.04460},
  urldate = {2020-07-02},
  abstract = {State of the art deep reinforcement learning algorithms take many millions of interactions to attain human-level performance. Humans, on the other hand, can very quickly exploit highly rewarding nuances of an environment upon first discovery. In the brain, such rapid learning is thought to depend on the hippocampus and its capacity for episodic memory. Here we investigate whether a simple model of hippocampal episodic control can learn to solve difficult sequential decision-making tasks. We demonstrate that it not only attains a highly rewarding strategy significantly faster than state-of-the-art deep reinforcement learning algorithms, but also achieves a higher overall reward on some of the more challenging domains.},
  keywords = {#nosource,Computer Science - Machine Learning,memory,Quantitative Biology - Neurons and Cognition,reinforcement learning,Statistics - Machine Learning}
}

@inproceedings{boffiLearningStabilityCertificates2021,
  title = {Learning {{Stability Certificates}} from {{Data}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Robot Learning}}},
  author = {Boffi, Nicholas and Tu, Stephen and Matni, Nikolai and Slotine, Jean-Jacques and Sindhwani, Vikas},
  date = {2021-10-04},
  pages = {1341--1350},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v155/boffi21a.html},
  urldate = {2023-01-23},
  abstract = {Many existing tools in nonlinear control theory for establishing stability or safety of a dynamical system can be distilled to the construction of a certificate function which guarantees a desired property. However, algorithms for synthesizing certificate functions typically require a closed-form analytical expression of the underlying dynamics, which rules out their use on many modern robotic platforms. To circumvent this issue, we develop algorithms for learning certificate functions only from trajectory data. We establish bounds on the generalization error – the probability that a certificate will not certify a new, unseen trajectory – when learning from trajectories, and we convert such generalization error bounds into global stability guarantees. We demonstrate empirically that certificates for complex dynamics can be efficiently learned, and that the learned certificates can be used for downstream tasks such as adaptive control.},
  eventtitle = {Conference on {{Robot Learning}}},
  langid = {english},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/boffi_et_al_2021_learning_stability_certificates_from_data.pdf}
}

@article{bogdanovicLearningVariableImpedance2020,
  title = {Learning {{Variable Impedance Control}} for {{Contact Sensitive Tasks}}},
  author = {Bogdanovic, Miroslav and Khadiv, Majid and Righetti, Ludovic},
  date = {2020-10},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {5},
  number = {4},
  pages = {6129--6136},
  issn = {2377-3766},
  doi = {10.1109/LRA.2020.3011379},
  abstract = {Reinforcement learning algorithms have shown great success in solving different problems ranging from playing video games to robotics. However, they struggle to solve delicate robotic problems, especially those involving contact interactions. Though in principle a policy directly outputting joint torques should be able to learn to perform these tasks, in practice we see that it has difficulty to robustly solve the problem without any given structure in the action space. In this letter, we investigate how the choice of action space can give robust performance in presence of contact uncertainties. We propose learning a policy giving as output impedance and desired position in joint space and compare the performance of that approach to torque and position control under different contact uncertainties. Furthermore, we propose an additional reward term designed to regularize these variable impedance control policies, giving them interpretability and facilitating their transfer to real systems. We present extensive experiments in simulation of both floating and fixed-base systems in tasks involving contact uncertainties, as well as results for running the learned policies on a real system (accompanying videos can be seen here: https://youtu.be/AQuuQ-h4dBM).},
  eventtitle = {{{IEEE Robotics}} and {{Automation Letters}}},
  keywords = {Aerospace electronics,compliance and impedance control,Impedance,Joint impedance,motion control,PD control,READ,Reinforcement learning,Robots,Robustness,Task analysis,Uncertainty},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/bogdanovic_et_al_2020_learning_variable_impedance_control_for_contact_sensitive_tasks.pdf}
}

@article{bohgDataDrivenGraspSynthesis2014,
  title = {Data-{{Driven Grasp Synthesis}}—{{A Survey}}},
  author = {Bohg, J. and Morales, A. and Asfour, T. and Kragic, D.},
  date = {2014-04},
  journaltitle = {IEEE Transactions on Robotics},
  volume = {30},
  number = {2},
  pages = {289--309},
  issn = {1552-3098},
  doi = {10.1109/TRO.2013.2289018},
  abstract = {We review the work on data-driven grasp synthesis and the methodologies for sampling and ranking candidate grasps. We divide the approaches into three groups based on whether they synthesize grasps for known, familiar, or unknown objects. This structure allows us to identify common object representations and perceptual processes that facilitate the employed data-driven grasp synthesis technique. In the case of known objects, we concentrate on the approaches that are based on object recognition and pose estimation. In the case of familiar objects, the techniques use some form of a similarity matching to a set of previously encountered objects. Finally, for the approaches dealing with unknown objects, the core part is the extraction of specific features that are indicative of good grasps. Our survey provides an overview of the different methodologies and discusses open problems in the area of robot grasping. We also draw a parallel to the classical approaches that rely on analytic formulations.},
  keywords = {#nosource,candidate grasp ranking,candidate grasp sampling,common object representations,data-driven grasp synthesis technique,Databases,feature extraction,Feature extraction,Grasp planning,grasp synthesis,Grasping,grippers,image matching,Measurement,object grasping and manipulation,object recognition,object recognition and classification,perceptual processes,pose estimation,Robot sensing systems,robotic grasping,sampling methods,similarity matching,tno internship,visual perception,visual representations}
}

@article{boschPlanningImagesDeep,
  title = {Planning from {{Images}} with {{Deep Latent Gaussian Process Dynamics}}},
  author = {Bosch, Nathanael and Achterhold, Jan and Leal-Taixe, Laura},
  pages = {11},
  abstract = {Planning is a powerful approach to control problems with known environment dynamics. In unknown environments the agent needs to learn a model of the system dynamics to make planning applicable. This is particularly challenging when the underlying states are only indirectly observable through images. We propose to learn a deep latent Gaussian process dynamics (DLGPD) model that learns low-dimensional system dynamics from environment interactions with visual observations. The method infers latent state representations from observations using neural networks and models the system dynamics in the learned latent space with Gaussian processes. All parts of the model can be trained jointly by optimizing a lower bound on the likelihood of transitions in image space. We evaluate the proposed approach on the pendulum swing-up task while using the learned dynamics model for planning in latent space in order to solve the control problem. We also demonstrate that our method can quickly adapt a trained agent to changes in the system dynamics from just a few rollouts. We compare our approach to a state-of-the-art purely deep learning based method and demonstrate the advantages of combining Gaussian processes with deep learning for data efficiency and transfer learning.},
  langid = {english},
  keywords = {#nosource}
}

@article{bousmalisUsingSimulationDomain2018,
  title = {Using {{Simulation}} and {{Domain Adaptation}} to {{Improve Efficiency}} of {{Deep Robotic Grasping}}},
  author = {Bousmalis, Konstantinos and Irpan, Alex and Wohlhart, Paul and Bai, Yunfei and Kelcey, Matthew and Kalakrishnan, Mrinal and Downs, Laura and Ibarz, Julian and Pastor, Peter and Konolige, Kurt and Levine, Sergey and Vanhoucke, Vincent},
  date = {2018},
  journaltitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages = {4243--4250},
  doi = {10.1109/ICRA.2018.8460875},
  abstract = {Instrumenting and collecting annotated visual grasping datasets to train modern machine learning algorithms can be extremely time-consuming and expensive. An appealing alternative is to use off-the-shelf simulators to render synthetic data for which ground-truth annotations are generated automatically. Unfortunately, models trained purely on simulated data often fail to generalize to the real world. We study how randomized simulated environments and domain adaptation methods can be extended to train a grasping system to grasp novel objects from raw monocular RGB images. We extensively evaluate our approaches with a total of more than 25,000 physical test grasps, studying a range of simulation conditions and domain adaptation methods, including a novel extension of pixel-level domain adaptation that we term the GraspGAN. We show that, by using synthetic data and domain adaptation, we are able to reduce the number of real-world samples needed to achieve a given level of performance by up to 50 times, using only randomly generated simulated objects. We also show that by using only unlabeled real-world data and our GraspGAN methodology, we obtain real-world grasping performance without any real-world labels that is similar to that achieved with 939,777 labeled real-world samples.},
  keywords = {#nosource,3D modeling,Domain adaptation,Experiment,Image resolution,Instrumentation (computer programming),Machine learning,Pixel,Procedural generation,Randomized algorithm,Requirement,robotic grasping,simulation,Synthetic data,Synthetic intelligence,tno internship}
}

@article{breyerComparingTaskSimplifications2019,
  title = {Comparing Task Simplifications to Learn Closed-Loop Object Picking Using Deep Reinforcement Learning},
  author = {Breyer, M. and Furrer, F. and Novkovic, T. and Siegwart, R. and Nieto, J.},
  date = {2019},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {4},
  number = {2},
  pages = {1549--1556},
  doi = {10.1109/LRA.2019.2896467},
  abstract = {Enabling autonomous robots to interact in unstructured environments with dynamic objects requires manipulation capabilities that can deal with clutter, changes, and objects' variability. This letter presents a comparison of different reinforcement learning-based approaches for object picking with a robotic manipulator. We learn closed-loop policies mapping depth camera inputs to motion commands and compare different approaches to keep the problem tractable, including reward shaping, curriculum learning, and using a policy pre-trained on a task with a reduced action set to warm-start the full problem. For efficient and more flexible data collection, we train in simulation and transfer the policies to a real robot. We show that using curriculum learning, policies learned with a sparse reward formulation can be trained at similar rates as with a shaped reward. These policies result in success rates comparable to the policy initialized on the simplified task. We could successfully transfer these policies to the real robot with only minor modifications of the depth image filtering. We found that using a heuristic to warm-start the training was useful to enforce desired behavior, while the policies trained from scratch using a curriculum learned better to cope with unseen scenarios where objects are removed. © 2016 IEEE.},
  keywords = {#nosource,curriculum learning,deep reinforcement learning,Grasping,object grasping,reinforcement learning,robotic grasping,tno internship,visual servoing},
  annotation = {00000}
}

@article{buchliLearningVariableImpedance2011,
  title = {Learning Variable Impedance Control},
  author = {Buchli, Jonas and Stulp, Freek and Theodorou, Evangelos and Schaal, Stefan},
  date = {2011-06-01},
  journaltitle = {The International Journal of Robotics Research},
  shortjournal = {The International Journal of Robotics Research},
  volume = {30},
  number = {7},
  pages = {820--833},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {0278-3649},
  doi = {10.1177/0278364911402527},
  url = {https://doi.org/10.1177/0278364911402527},
  urldate = {2022-04-29},
  abstract = {One of the hallmarks of the performance, versatility, and robustness of biological motor control is the ability to adapt the impedance of the overall biomechanical system to different task requirements and stochastic disturbances. A transfer of this principle to robotics is desirable, for instance to enable robots to work robustly and safely in everyday human environments. It is, however, not trivial to derive variable impedance controllers for practical high degree-of-freedom (DOF) robotic tasks. In this contribution, we accomplish such variable impedance control with the reinforcement learning (RL) algorithm PI2 (PolicyImprovement withPath Integrals). PI2is a model-free, sampling-based learning method derived from first principles of stochastic optimal control. The PI2algorithm requires no tuning of algorithmic parameters besides the exploration noise. The designer can thus fully focus on the cost function design to specify the task. From the viewpoint of robotics, a particular useful property of PI2is that it can scale to problems of many DOFs, so that reinforcement learning on real robotic systems becomes feasible. We sketch the PI2algorithm and its theoretical properties, and how it is applied to gain scheduling for variable impedance control. We evaluate our approach by presenting results on several simulated and real robots. We consider tasks involving accurate tracking through via points, and manipulation tasks requiring physical contact with the environment. In these tasks, the optimal strategy requires both tuning of a reference trajectory and the impedance of the end-effector. The results show that we can use path integral based reinforcement learning not only for planning but also to derive variable gain feedback controllers in realistic scenarios. Thus, the power of variable impedance control is made available to a wide variety of robotic systems and practical applications.},
  langid = {english},
  keywords = {compliant control,gain scheduling,IMPORTANT,motion primitives,READ,Reinforcement learning,stochastic optimal control,variable impedance control},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/buchli_et_al_2011_learning_variable_impedance_control.pdf}
}

@unpublished{buckmanSampleEfficientReinforcementLearning2019,
  title = {Sample-{{Efficient Reinforcement Learning}} with {{Stochastic Ensemble Value Expansion}}},
  author = {Buckman, Jacob and Hafner, Danijar and Tucker, George and Brevdo, Eugene and Lee, Honglak},
  date = {2019-06-07},
  eprint = {1807.01675},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1807.01675},
  urldate = {2020-07-02},
  abstract = {Integrating model-free and model-based approaches in reinforcement learning has the potential to achieve the high performance of model-free algorithms with low sample complexity. However, this is difficult because an imperfect dynamics model can degrade the performance of the learning algorithm, and in sufficiently complex environments, the dynamics model will almost always be imperfect. As a result, a key challenge is to combine model-based approaches with model-free learning in such a way that errors in the model do not degrade performance. We propose stochastic ensemble value expansion (STEVE), a novel model-based technique that addresses this issue. By dynamically interpolating between model rollouts of various horizon lengths for each individual example, STEVE ensures that the model is only utilized when doing so does not introduce significant errors. Our approach outperforms model-free baselines on challenging continuous control benchmarks with an order-of-magnitude increase in sample efficiency, and in contrast to previous model-based approaches, performance does not degrade in complex environments.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,learned model,model-based,reinforcement learning,Statistics - Machine Learning}
}

@unpublished{burdaExplorationRandomNetwork2018,
  title = {Exploration by {{Random Network Distillation}}},
  author = {Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
  date = {2018-10-30},
  eprint = {1810.12894},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1810.12894},
  urldate = {2020-07-02},
  abstract = {We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,exploration,intrinsic motivation,reinforcement learning,Statistics - Machine Learning},
  annotation = {00197}
}

@unpublished{burdaLargeScaleStudyCuriosityDriven2018,
  title = {Large-{{Scale Study}} of {{Curiosity-Driven Learning}}},
  author = {Burda, Yuri and Edwards, Harri and Pathak, Deepak and Storkey, Amos and Darrell, Trevor and Efros, Alexei A.},
  date = {2018-08-13},
  eprint = {1808.04355},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1808.04355},
  urldate = {2020-07-02},
  abstract = {Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the hand-designed extrinsic rewards of many game environments. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at https://pathak22.github.io/large-scale-curiosity/},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,exploration,intrinsic motivation,reinforcement learning,Statistics - Machine Learning}
}

@book{burgerZhangCameraCalibration2016,
  title = {Zhang's {{Camera Calibration Algorithm}}: {{In-Depth Tutorial}} and {{Implementation}}},
  shorttitle = {Zhang's {{Camera Calibration Algorithm}}},
  author = {Burger, Wilhelm},
  date = {2016-05-16},
  abstract = {This report details the algorithmic steps involved in the well-known camera calibration method by Zhang and describes an associated open-source Java implementation that depends only upon the Apache Commons Math library. Key terms: Computer vision, camera calibration, Zhang's method, camera projection, imaging geometry, image rectification, Java implementation.},
  keywords = {#nosource,robotic grasping,tno internship}
}

@unpublished{cabiIntentionalUnintentionalAgent2017,
  title = {The {{Intentional Unintentional Agent}}: {{Learning}} to {{Solve Many Continuous Control Tasks Simultaneously}}},
  shorttitle = {The {{Intentional Unintentional Agent}}},
  author = {Cabi, Serkan and Colmenarejo, Sergio Gómez and Hoffman, Matthew W. and Denil, Misha and Wang, Ziyu and family=Freitas, given=Nando, prefix=de, useprefix=true},
  date = {2017-07-11},
  eprint = {1707.03300},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1707.03300},
  urldate = {2020-07-02},
  abstract = {This paper introduces the Intentional Unintentional (IU) agent. This agent endows the deep deterministic policy gradients (DDPG) agent for continuous control with the ability to solve several tasks simultaneously. Learning to solve many tasks simultaneously has been a long-standing, core goal of artificial intelligence, inspired by infant development and motivated by the desire to build flexible robot manipulators capable of many diverse behaviours. We show that the IU agent not only learns to solve many tasks simultaneously but it also learns faster than agents that target a single task at-a-time. In some cases, where the single task DDPG method completely fails, the IU agent successfully solves the task. To demonstrate this, we build a playroom environment using the MuJoCo physics engine, and introduce a grounded formal language to automatically generate tasks.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,multitask RL,reinforcement learning,transfer learning}
}

@article{calancaReviewAlgorithmsCompliant2016,
  title = {A {{Review}} of {{Algorithms}} for {{Compliant Control}} of {{Stiff}} and {{Fixed-Compliance Robots}}},
  author = {Calanca, Andrea and Muradore, Riccardo and Fiorini, Paolo},
  date = {2016-04},
  journaltitle = {IEEE/ASME Transactions on Mechatronics},
  volume = {21},
  number = {2},
  pages = {613--624},
  issn = {1941-014X},
  doi = {10.1109/TMECH.2015.2465849},
  abstract = {This survey presents the state of the art of basic compliant control algorithms in a unified view of past and present literature. Compliant control is fundamental when dealing with unstructured environments, as in the case of human-robot interaction. This is because it implicitly controls the energy transfer to the environment, providing a safe interaction. In this review, we analyze solutions from traditional robotics, usually involving stiff joints, and recent literature to find common control concepts and differences. To this aim, we bring back every schemas and relative mathematics formulation to a common and simplified scenario. Then, for each schema, we explain its intuitive meaning and report issues raised in the literature. We also propose an expansion of taxonomy to account for recent research.},
  eventtitle = {{{IEEE}}/{{ASME Transactions}} on {{Mechatronics}}},
  keywords = {Active compliance,Active Compliance,Actuators,Admittance,admittance control,Admittance Control,compliance,Compliance,flexible joints,Flexible Joints,Force,force control,Force control,Force Control,Impedance,impedance control,Impedance Control,interaction control,Interaction Control,Joints,REVIEW,Robots,series elastic actuators,Series Elastic Actuators},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/calanca_et_al_2016_a_review_of_algorithms_for_compliant_control_of_stiff_and_fixed-compliance.pdf}
}

@article{calandraMoreFeelingLearning2018,
  title = {More than a Feeling: {{Learning}} to Grasp and Regrasp Using Vision and Touch},
  shorttitle = {More than a Feeling},
  author = {Calandra, Roberto and Owens, Andrew and Jayaraman, Dinesh and Lin, Justin and Yuan, Wenzhen and Malik, Jitendra and Adelson, Edward H. and Levine, Sergey},
  date = {2018},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {3},
  number = {4},
  pages = {3300--3307},
  keywords = {#nosource,grasp pose detection,neural network,object grasping,robotic grasping,tno internship},
  annotation = {00018}
}

@article{calderaReviewDeepLearning2018,
  title = {Review of {{Deep Learning Methods}} in {{Robotic Grasp Detection}}},
  author = {Caldera, Shehan and Rassau, Alexander and Chai, Douglas},
  date = {2018-09-07},
  journaltitle = {Multimodal Technologies and Interaction},
  volume = {2},
  number = {3},
  pages = {57},
  issn = {2414-4088},
  doi = {10.3390/mti2030057},
  url = {http://www.mdpi.com/2414-4088/2/3/57},
  urldate = {2019-04-14},
  abstract = {For robots to attain more general-purpose utility, grasping is a necessary skill to master. Such general-purpose robots may use their perception abilities to visually identify grasps for a given object. A grasp describes how a robotic end-effector can be arranged to securely grab an object and successfully lift it without slippage. Traditionally, grasp detection requires expert human knowledge to analytically form the task-specific algorithm, but this is an arduous and time-consuming approach. During the last five years, deep learning methods have enabled significant advancements in robotic vision, natural language processing, and automated driving applications. The successful results of these methods have driven robotics researchers to explore the use of deep learning methods in task-generalised robotic applications. This paper reviews the current state-of-the-art in regards to the application of deep learning methods to generalised robotic grasping and discusses how each element of the deep learning approach has improved the overall performance of robotic grasp detection. Several of the most promising approaches are evaluated and the most suitable for real-time grasp detection is identified as the one-shot detection method. The availability of suitable volumes of appropriate training data is identified as a major obstacle for effective utilisation of the deep learning approaches, and the use of transfer learning techniques is proposed as a potential mechanism to address this. Finally, current trends in the field and future potential research directions are discussed.},
  langid = {english},
  keywords = {#nosource,robotic grasping,tno internship}
}

@article{califanoTaskBasedPostImpactSafety2022,
  title = {A {{Task-Based Post-Impact Safety Protocol Based}} on {{Energy Tanks}}},
  author = {Califano, Federico and family=Dijk, given=Daniël, prefix=van, useprefix=true and Roozing, Wesley},
  date = {2022-10},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {4},
  pages = {8791--8798},
  issn = {2377-3766},
  doi = {10.1109/LRA.2022.3187254},
  abstract = {As situations with robots working in human environments are becoming increasingly important, the aspect of safety is crucial. As a consequence, the design of human friendly robots has been intensively researched in the past years, producing both mechanical and control design advancements. In this work, a safety-aware control architecture is proposed in a scenario in which a predefined task has to be executed. At the core of its implementation, a novel dynamic energy injection protocol is introduced for energy tanks. Contrarily to their most common employment in the past, energy tanks are used to provide relevant information on energy flows used by the safety protocol, rather than to guarantee passivity of the controlled robot, which does not imply safety. The algorithm is able to detect collisions and divergence from nominal task execution by identifying unwanted energy flows, encoded in the energy tanks. A reaction strategy based on low impedance control is implemented in case of collision detection. Experiments on a 7-DoF manipulator successfully validate the proposed strategy.},
  eventtitle = {{{IEEE Robotics}} and {{Automation Letters}}},
  keywords = {Collision avoidance,Compliance and Impedance Control,Impedance,Optimization,Protocols,READ,Robot Safety,Robots,Safety,Task analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/califano_et_al_2022_a_task-based_post-impact_safety_protocol_based_on_energy_tanks2.pdf}
}

@inproceedings{califanoUseEnergyTanks2023,
  title = {On the {{Use}} of {{Energy Tanks}} for {{Robotic Systems}}},
  booktitle = {Human-{{Friendly Robotics}} 2022},
  author = {Califano, Federico and Rashad, Ramy and Secchi, Cristian and Stramigioli, Stefano},
  editor = {Borja, Pablo and Della Santina, Cosimo and Peternel, Luka and Torta, Elena},
  date = {2023},
  series = {Springer {{Proceedings}} in {{Advanced Robotics}}},
  pages = {174--188},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-031-22731-8_13},
  abstract = {In this document we describe and discuss energy tanks, a control algorithm which has gained popularity inside the robotics and control community over the last years. This article has the threefold scope of i) introducing to the reader the topic in a simple yet precise way, starting with a throughout description of the energy-aware framework, where energy tanks find their genesis; ii) summarising the range of applications of energy tanks, including an original reflection about different formulations of those; iii) discussing limits and future challenges involving energy tanks and energy-aware control in general.},
  isbn = {978-3-031-22731-8},
  langid = {english},
  keywords = {REVIEW},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/califano_et_al_2023_on_the_use_of_energy_tanks_for_robotic_systems.pdf}
}

@inproceedings{calinonEncodingTimeSpace2011,
  title = {Encoding the Time and Space Constraints of a Task in Explicit-Duration Hidden {{Markov}} Model},
  booktitle = {2011 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Calinon, Sylvain and Pistillo, Antonio and Caldwell, Darwin G.},
  date = {2011},
  pages = {3413--3418},
  publisher = {{IEEE}},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/calinon_et_al_2011_encoding_the_time_and_space_constraints_of_a_task_in_explicit-duration_hidden.pdf}
}

@inproceedings{calinonLearningbasedControlStrategy2010,
  title = {Learning-Based Control Strategy for Safe Human-Robot Interaction Exploiting Task and Robot Redundancies},
  booktitle = {2010 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Calinon, Sylvain and Sardellitti, Irene and Caldwell, Darwin G.},
  date = {2010-10},
  pages = {249--254},
  issn = {2153-0866},
  doi = {10.1109/IROS.2010.5648931},
  abstract = {We propose a control strategy for a robotic manipulator operating in an unstructured environment while interacting with a human operator. The proposed system takes into account the important characteristics of the task and the redundancy of the robot to determine a controller that is safe for the user. The constraints of the task are first extracted using several examples of the skill demonstrated to the robot through kinesthetic teaching. An active control strategy based on task-space control with variable stiffness is proposed, and combined with a safety strategy for tasks requiring humans to move in the vicinity of robots. A risk indicator for human-robot collision is defined, which modulates a repulsive force distorting the spatial and temporal characteristics of the movement according to the task constraints. We illustrate the approach with two human-robot interaction experiments, where the user teaches the robot first how to move a tray, and then shows it how to iron a napkin.},
  eventtitle = {2010 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  keywords = {Collision avoidance,Force,Redundancy,Robots,Safety,Tracking,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/calinon_et_al_2010_learning-based_control_strategy_for_safe_human-robot_interaction_exploiting2.pdf}
}

@article{calinonTutorialTaskparameterizedMovement2016,
  title = {A Tutorial on Task-Parameterized Movement Learning and Retrieval},
  author = {Calinon, Sylvain},
  date = {2016},
  journaltitle = {Intelligent service robotics},
  volume = {9},
  number = {1},
  pages = {1--29},
  publisher = {{Springer}},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/calinon_2016_a_tutorial_on_task-parameterized_movement_learning_and_retrieval.pdf}
}

@article{callensFrameworkRecognitionPrediction2020,
  title = {A Framework for Recognition and Prediction of Human Motions in Human-Robot Collaboration Using Probabilistic Motion Models},
  author = {Callens, Thomas and family=Have, given=Tuur, prefix=van der, useprefix=true and Van Rossom, Sam and De Schutter, Joris and Aertbeliën, Erwin},
  date = {2020},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {5},
  number = {4},
  pages = {5151--5158},
  publisher = {{IEEE}},
  keywords = {Databases,Estimation,Hidden Markov models,Learning from demonstration,motion prediction,motion recognition,Predictive models,Probabilistic logic,probability and statistical methods,Robots,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/callens_et_al_2020_a_framework_for_recognition_and_prediction_of_human_motions_in_human-robot.pdf}
}

@thesis{calliActiveGraspSynthesis2014,
  title = {Active Grasp Synthesis for Grasping Unknown Objects.},
  author = {Çalli, Berk},
  date = {2014},
  institution = {{[s.n.]}},
  location = {{S.l.}},
  langid = {english},
  keywords = {#nosource,robotic grasping,tno internship},
  annotation = {OCLC: 911671026}
}

@article{capelliPassivityControlBarrier2022,
  title = {Passivity and {{Control Barrier Functions}}: {{Optimizing}} the {{Use}} of {{Energy}}},
  shorttitle = {Passivity and {{Control Barrier Functions}}},
  author = {Capelli, Beatrice and Secchi, Cristian and Sabattini, Lorenzo},
  date = {2022-04},
  journaltitle = {Ieee Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {7},
  number = {2},
  pages = {1356--1363},
  publisher = {{Ieee-Inst Electrical Electronics Engineers Inc}},
  location = {{Piscataway}},
  issn = {2377-3766},
  doi = {10.1109/LRA.2021.3139951},
  url = {https://ieeexplore.ieee.org/document/9669000},
  urldate = {2022-08-09},
  abstract = {Passivity-based control ensures the implementation of a desired behavior in a controlled system, while preserving robust stability. In this letter, we propose a new method to guarantee passivity, based on energy-tank control and Control Barrier Functions. The goal is to accomplish a desired behavior with minimal modification, while ensuring passivity. The proposed method is suitable for a wide range of applications: whenever some desired control action may disrupt the passivity of the system, the designed Control Barrier Function will modify the behavior to enforce the preservation of passivity. Simulations and real experiments were carried out to prove the effectiveness and the flexibility of the proposed method: in particular, a simple case of variable stiffness in a mass-spring-damper system and a multi-robot controller based on a time-varying artificial potential field.},
  langid = {english},
  keywords = {Dynamics,feedback,optimization and optimal control,READ,robust/adaptive control,strategy},
  annotation = {WOS:000742180000010},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/capelli_et_al_2022_passivity_and_control_barrier_functions.pdf}
}

@article{castainOnlineDynamicTrajectory1984,
  title = {An On-Line Dynamic Trajectory Generator},
  author = {Castain, Ralph H. and Paul, Richard P.},
  date = {1984},
  journaltitle = {The International Journal of Robotics Research},
  volume = {3},
  number = {1},
  pages = {68--72},
  publisher = {{Sage Publications Sage CA: Thousand Oaks, CA}},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/castain_paul_1984_an_on-line_dynamic_trajectory_generator.pdf}
}

@article{castroDopamineResearchFramework2018,
  title = {Dopamine: {{A Research Framework}} for {{Deep Reinforcement Learning}}},
  shorttitle = {Dopamine},
  author = {Castro, Pablo Samuel and Moitra, Subhodeep and Gelada, Carles and Kumar, Saurabh and Bellemare, Marc G.},
  date = {2018-09-27},
  url = {https://openreview.net/forum?id=ByG_3s09KX},
  urldate = {2020-07-02},
  abstract = {Deep reinforcement learning (deep RL) research has grown significantly in recent years. A number of software offerings now exist that provide stable, comprehensive implementations for benchmarking....},
  keywords = {#nosource,distributional RL,model-free,reinforcement learning}
}

@online{changImpedanceAdaptationReinforcement2022,
  title = {Impedance {{Adaptation}} by {{Reinforcement Learning}} with {{Contact Dynamic Movement Primitives}}},
  author = {Chang, Chunyang and Haninger, Kevin and Shi, Yunlei and Yuan, Chengjie and Chen, Zhaopeng and Zhang, Jianwei},
  date = {2022-03-21},
  eprint = {2203.07191},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2203.07191},
  url = {http://arxiv.org/abs/2203.07191},
  urldate = {2022-10-30},
  abstract = {Dynamic movement primitives (DMPs) allow complex position trajectories to be efficiently demonstrated to a robot. In contact-rich tasks, where position trajectories alone may not be safe or robust over variation in contact geometry, DMPs have been extended to include force trajectories. However, different task phases or degrees of freedom may require the tracking of either position or force -- e.g., once contact is made, it may be more important to track the force demonstration trajectory in the contact direction. The robot impedance balances between following a position or force reference trajectory, where a high stiffness tracks position and a low stiffness tracks force. This paper proposes using DMPs to learn position and force trajectories from demonstrations, then adapting the impedance parameters online with a higher-level control policy trained by reinforcement learning. This allows one-shot demonstration of the task with DMPs, and improved robustness and performance from the impedance adaptation. The approach is validated on peg-in-hole and adhesive strip application tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/chang_et_al_2022_impedance_adaptation_by_reinforcement_learning_with_contact_dynamic_movement.pdf}
}

@inproceedings{changNeuralLyapunovControl2019,
  title = {Neural {{Lyapunov Control}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Chang, Ya-Chien and Roohi, Nima and Gao, Sicun},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2019/hash/2647c1dba23bc0e0f9cdf75339e120d2-Abstract.html},
  urldate = {2022-12-06},
  abstract = {We propose new methods for learning control policies and neural network Lyapunov functions for nonlinear control problems, with provable guarantee of stability. The framework consists of a learner that attempts to find the control and Lyapunov functions, and a falsifier that finds counterexamples to quickly guide the learner towards solutions. The procedure terminates when no counterexample is found by the falsifier, in which case the controlled nonlinear system is provably stable. The approach significantly simplifies the process of Lyapunov control design, provides end-to-end correctness guarantee, and can obtain much larger regions of attraction than existing methods such as LQR and SOS/SDP. We show experiments on how the new methods obtain high-quality solutions for challenging robot control problems such as path tracking for wheeled vehicles and humanoid robot balancing.},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/chang_et_al_2019_neural_lyapunov_control.pdf}
}

@online{chanMeasuringReliabilityReinforcement2020,
  title = {Measuring the {{Reliability}} of {{Reinforcement Learning Algorithms}}},
  author = {Chan, Stephanie C. Y. and Fishman, Samuel and Canny, John and Korattikara, Anoop and Guadarrama, Sergio},
  date = {2020-02-12},
  eprint = {1912.05663},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1912.05663},
  urldate = {2024-03-02},
  abstract = {Lack of reliability is a well-known issue for reinforcement learning (RL) algorithms. This problem has gained increasing attention in recent years, and efforts to improve it have grown substantially. To aid RL researchers and production users with the evaluation and improvement of reliability, we propose a set of metrics that quantitatively measure different aspects of reliability. In this work, we focus on variability and risk, both during training and after learning (on a fixed policy). We designed these metrics to be general-purpose, and we also designed complementary statistical tests to enable rigorous comparisons on these metrics. In this paper, we first describe the desired properties of the metrics and their design, the aspects of reliability that they measure, and their applicability to different scenarios. We then describe the statistical tests and make additional practical recommendations for reporting results. The metrics and accompanying statistical tools have been made available as an open-source library at https://github.com/google-research/rl-reliability-metrics. We apply our metrics to a set of common RL algorithms and environments, compare them, and analyze the results.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/Chan et al_2020_Measuring the Reliability of Reinforcement Learning Algorithms.pdf}
}

@article{cheahLearningImpedanceControl1998,
  title = {Learning Impedance Control for Robotic Manipulators},
  author = {Cheah, Chien-Chern and Wang, Danwei},
  date = {1998},
  journaltitle = {IEEE Transactions on robotics and automation},
  volume = {14},
  number = {3},
  pages = {452--465},
  publisher = {{IEEE}},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/cheah_wang_1998_learning_impedance_control_for_robotic_manipulators.pdf}
}

@inproceedings{chebotarLearningRobotTactile2014,
  title = {Learning Robot Tactile Sensing for Object Manipulation},
  booktitle = {2014 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Chebotar, Yevgen and Kroemer, Oliver and Peters, Jan},
  date = {2014-09},
  pages = {3368--3375},
  issn = {2153-0866},
  doi = {10.1109/IROS.2014.6943031},
  abstract = {Tactile sensing is a fundamental component of object manipulation and tool handling skills. With robots entering unstructured environments, tactile feedback also becomes an important ability for robot manipulation. In this work, we explore how a robot can learn to use tactile sensing in object manipulation tasks. We first address the problem of in-hand object localization and adapt three pose estimation algorithms from computer vision. Second, we employ dynamic motor primitives to learn robot movements from human demonstrations and record desired tactile signal trajectories. Then, we add tactile feedback to the control loop and apply relative entropy policy search to learn the parameters of the tactile coupling. Additionally, we show how the learning of tactile feedback can be performed more efficiently by reducing the dimensionality of the tactile information through spectral clustering and principal component analysis. Our approach is implemented on a real robot, which learns to perform a scraping task with a spatula in an altered environment.},
  eventtitle = {2014 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  keywords = {Estimation,Kernel,READ,Tactile sensors,Trajectory,Vectors},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/chebotar_et_al_2014_learning_robot_tactile_sensing_for_object_manipulation.pdf}
}

@inproceedings{chenAutomateRobotReaching2017,
  title = {Automate Robot Reaching Task with Learning from Demonstration},
  booktitle = {2017 18th {{International Conference}} on {{Advanced Robotics}} ({{ICAR}})},
  author = {Chen, Jie and Ren, Hongliang and Lau, Henry Y K},
  date = {2017-07},
  pages = {543--548},
  doi = {10.1109/ICAR.2017.8023663},
  abstract = {Over the last decades, robots have been moved from industries to domestic environments. Robot Learning from Demonstration (LfD) is one of the most significant methods to facilitate this trend. In this work, we first discuss details about a efficient motion planning strategy, e.g., Stable Estimator of Dynamical Systems (SEDS). A human first demonstrates reaching tasks several times, and Gaussian Mixture Regression is used to roughly encode the demonstrations into a set of differential equations. Then based on Lyapunov Stability Theorem, a constrained nonlinear optimization problem is formulated to iteratively refine the previously learned differential equations and SEDS is thus obtained. Experiments have been conducted on a KUKA LBR iiwa robot to verify two properties of the proposed method, e.g., asymptotical stability and adaptation to spatial perturbations.},
  eventtitle = {2017 18th {{International Conference}} on {{Advanced Robotics}} ({{ICAR}})},
  keywords = {Asymptotic stability,Planning,READ,Robot kinematics,Service robots,STABILITY,Stability analysis,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/chen_et_al_2017_automate_robot_reaching_task_with_learning_from_demonstration.pdf}
}

@inproceedings{chenClosedLoopVariableStiffness2021,
  title = {Closed-{{Loop Variable Stiffness Control}} of {{Dynamical Systems}}},
  booktitle = {2020 {{IEEE-RAS}} 20th {{International Conference}} on {{Humanoid Robots}} ({{Humanoids}})},
  author = {Chen, Xiao and Michel, Youssef and Lee, Dongheui},
  date = {2021-07},
  pages = {163--169},
  issn = {2164-0580},
  doi = {10.1109/HUMANOIDS47582.2021.9555795},
  abstract = {In this paper, we present an approach to encode variable stiffness behaviors into Dynamical Systems (DS), controlled in a closed-loop configuration. Given a desired robot motion represented as a first order DS, as well as a desired stiffness profile, our approach generates a new DS, called Variable Stiffness DS (VSDS), that controls the robot motion and impedance simultaneously. The robot shows a symmetric spring-like attraction behavior around a reference path described by one of the integral curves of the DS with an interactive behavior prescribed by the desired stiffness. Our approach is validated in simulations and in real robot experiments, displaying a safe and compliant interaction in the face of disturbances or possible collisions. Furthermore, we validate our approach in a contact task that requires continuously varying stiffness levels.},
  eventtitle = {2020 {{IEEE-RAS}} 20th {{International Conference}} on {{Humanoid Robots}} ({{Humanoids}})},
  keywords = {Humanoid robots,Impedance,IMPORTANT,Perturbation methods,READ,Robot motion,Safety,Sockets,Stability analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/chen_et_al_2021_closed-loop_variable_stiffness_control_of_dynamical_systems.pdf;/home/ricks/Zotero/storage/MBFXFJFA/Chen et al. - 2021 - Closed-Loop Variable Stiffness Control of Dynamica.pdf}
}

@inproceedings{chenConceptualApproachPassive2021,
  title = {A {{Conceptual Approach}} of {{Passive Human-Intention-Orientated Variable Admittance Control}} Using {{Power Envelope}}},
  booktitle = {2021 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Chen, Jingdong and Ro, Paul I.},
  date = {2021-09},
  pages = {7300--7306},
  issn = {2153-0866},
  doi = {10.1109/IROS51168.2021.9636487},
  abstract = {Two main challenges that need to be addressed in physical human-robot interaction (pHRI) are efficient recognition of human intention and interaction safety. In this paper, a general human intention framework was summarized, firstly, according to the robot's roles: a passive follower and a compliant leader. Secondly, we proposed variable admittance control models governed by human intentions. Power envelope approaches were then proposed to impose constraints on the variable admittance parameters inferred from human intention for maintaining passivity conservatively. Our passivity preserving approaches were validated via simulation and shown to avoid mismatching of time-varying admittance parameters that restrain drastic changes of admittance controller dynamics, which usually result in instability. Finally, the relationship between the robot's passivity and stability when it interacts with the human was analyzed.},
  eventtitle = {2021 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  keywords = {Admittance,Cartesian impedance,Human-robot interaction,Intelligent robots,Safety,STABILITY,Stability analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/chen_ro_2021_a_conceptual_approach_of_passive_human-intention-orientated_variable_admittance.pdf}
}

@inproceedings{chenDrawingElonMusk2021,
  title = {Drawing {{Elon Musk}}: {{A Robot Avatar}} for {{Remote Manipulation}}},
  shorttitle = {Drawing {{Elon Musk}}},
  booktitle = {2021 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Chen, Lingyun and Swikir, Abdalla and Haddadin, Sami},
  date = {2021-09},
  pages = {4244--4251},
  issn = {2153-0866},
  doi = {10.1109/IROS51168.2021.9635879},
  abstract = {The fast growth of communication technologies such as 5G provides high bandwidth and low latency wireless internet access. This enables both high definition video stream and real-time robot commands transmitted between robots and operators in the context of telepresence and teleoperation. Although there has been substantial research to establish algorithms that convert images to robot motions and telerobotic systems, little effort was made in establishing a clear scheme that enable artists to draw portraits using telerobotic systems. In this paper, we provide an easy-to-follow structure and implementation of a robot avatar for portrait drawing by artists through remote manipulation. The proposed telerobotic system uses a digital tablet and motion capture suit as input devices, which provides accurate drawing and continuous motion data stream respectively. With sensor fusion of the input data on the robot side, the drawing process presented in this work uses a unified force and impedance controller to ensure smooth and uniform pen-strokes. The proposed scheme was used to synthesise a system that was used by an artist to successfully finish the portrait drawing of Elon Musk. Finally, we show the effectiveness of the introduced control framework through an experiment. In particular, we validate the benefit of combining unified force and impedance control with sensor fusion of the digital tablet and motion capture suit data.},
  eventtitle = {2021 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  keywords = {Avatars,Force,Input devices,Process control,READ,Robot sensing systems,Sensor fusion,Wireless sensor networks},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/chen_et_al_2021_drawing_elon_musk.pdf}
}

@inproceedings{chenLearningAdaptiveReaching2016,
  title = {Learning {{Adaptive Reaching Skills}} with {{Nonlinear Dynamical Systems Directly}} from {{Human Demonstrations}}},
  booktitle = {2016 {{Ieee Workshop}} on {{Advanced Robotics}} and {{Its Social Impacts}} (Arso)},
  author = {Chen, Jie and Lau, Henry Y. K.},
  date = {2016},
  pages = {232--237},
  publisher = {{Ieee}},
  location = {{New York}},
  issn = {2162-7568},
  url = {https://ieeexplore.ieee.org/document/7736287},
  urldate = {2022-09-11},
  abstract = {In this work, we first discuss details about a novel motion planning approach for robot point to point reaching tasks called stable estimator of dynamical systems (SEDS). A human operator first demonstrates reaching movements several times, and Gaussian Mixture Model and Gaussian Mixture Regression are used to roughly encode human demonstrations through a first order ordinary differential equation. Then based on Lyapunov Stability Theorem, a constrained nonlinear optimization problem is formulated to iteratively refine the previously learned differential model and SEDS is derived. Since during human demonstrations, the velocity is usually quite low which heavily restricts the kinetic capability of the robot, and sometimes we expect the robot to move more fast, such as to catch flying objects and to avoid fast moving obstacles. Therefore, it is extremely significant to develop a method to control the velocity and duration of the robot movement. In this paper, we define a nonlinear function based on the distance between the robot and the target to adjust the velocity of the robot. Experiments have been conducted in simulation environments to verify three properties of the proposed method, namely global asymptotical stability, adaptation to spatial perturbations and velocity controllability.},
  isbn = {978-1-5090-4077-3},
  langid = {english},
  keywords = {READ,STABILITY},
  annotation = {WOS:000392692500042},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/chen_lau_2016_learning_adaptive_reaching_skills_with_nonlinear_dynamical_systems_directly.pdf}
}

@inproceedings{chenLearningExecutingPrimitive2020,
  title = {Learning and {{Executing Primitive Skills Based}} on {{Adaptive Control}}},
  booktitle = {2020 35th {{Youth Academic Annual Conference}} of {{Chinese Association}} of {{Automation}} ({{YAC}})},
  author = {Chen, Wenshi and Ma, Yinsong and Kong, Linghuan and He, Wei},
  date = {2020-10},
  pages = {13--17},
  doi = {10.1109/YAC51587.2020.9337683},
  abstract = {In this paper, we focus on the tasks learning and executing problem for manipulators. Dynamic movement primitives (DMPs) are employed as the basic motion models, which enable the robot to learn skills from human kinesthetic teaching. A stage teaching strategy is proposed to improve the generality of the framework, such that complex tasks for multi-joint manipulators can be learned. A DMPs joining method is integrated to concatenate complex movement sequences with smooth and accurate transitions in position and velocity space. Besides, motions can be generalized to different goals and durations. Furthermore, an adaptive controller with neural networks is introduced to improve the learning performance and to ensure the performance of motion execution.},
  eventtitle = {2020 35th {{Youth Academic Annual Conference}} of {{Chinese Association}} of {{Automation}} ({{YAC}})},
  keywords = {Artificial neural networks,dynamic movement primitives (DMPs),Dynamics,Education,Heuristic algorithms,Manipulator dynamics,neural networks (NNs),READ,Robot learning from demonstrations (LfDs),Task analysis,Uncertainty},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/chen_et_al_2020_learning_and_executing_primitive_skills_based_on_adaptive_control.pdf}
}

@article{chenVariableInteractionControl2020,
  title = {Variable {{Interaction Control}} with {{Dynamical Systems}}},
  author = {Chen, Xiao},
  date = {2020},
  keywords = {REVIEW},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/chen_2020_variable_interaction_control_with_dynamical_systems.pdf}
}

@online{ChForceControl,
  title = {Ch. 7 - {{Force Control}}},
  url = {https://manipulation.csail.mit.edu/force.html},
  urldate = {2022-05-28}
}

@online{chowLyapunovbasedApproachSafe2018,
  title = {A {{Lyapunov-based Approach}} to {{Safe Reinforcement Learning}}},
  author = {Chow, Yinlam and Nachum, Ofir and Duenez-Guzman, Edgar and Ghavamzadeh, Mohammad},
  date = {2018-05-20},
  eprint = {1805.07708},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1805.07708},
  url = {http://arxiv.org/abs/1805.07708},
  urldate = {2022-08-23},
  abstract = {In many real-world reinforcement learning (RL) problems, besides optimizing the main objective function, an agent must concurrently avoid violating a number of constraints. In particular, besides optimizing performance it is crucial to guarantee the safety of an agent during training as well as deployment (e.g. a robot should avoid taking actions - exploratory or not - which irrevocably harm its hardware). To incorporate safety in RL, we derive algorithms under the framework of constrained Markov decision problems (CMDPs), an extension of the standard Markov decision problems (MDPs) augmented with constraints on expected cumulative costs. Our approach hinges on a novel \textbackslash emph\{Lyapunov\} method. We define and present a method for constructing Lyapunov functions, which provide an effective way to guarantee the global safety of a behavior policy during training via a set of local, linear constraints. Leveraging these theoretical underpinnings, we show how to use the Lyapunov approach to systematically transform dynamic programming (DP) and RL algorithms into their safe counterparts. To illustrate their effectiveness, we evaluate these algorithms in several CMDP planning and decision-making tasks on a safety benchmark domain. Our results show that our proposed method significantly outperforms existing baselines in balancing constraint satisfaction and performance.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,READ,Statistics - Machine Learning},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/chow_et_al_2018_a_lyapunov-based_approach_to_safe_reinforcement_learning.pdf}
}

@online{chowLyapunovbasedSafePolicy2019a,
  title = {Lyapunov-Based {{Safe Policy Optimization}} for {{Continuous Control}}},
  author = {Chow, Yinlam and Nachum, Ofir and Faust, Aleksandra and Duenez-Guzman, Edgar and Ghavamzadeh, Mohammad},
  date = {2019-02-11},
  eprint = {1901.10031},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1901.10031},
  url = {http://arxiv.org/abs/1901.10031},
  urldate = {2023-07-05},
  abstract = {We study continuous action reinforcement learning problems in which it is crucial that the agent interacts with the environment only through safe policies, i.e.,\textasciitilde policies that do not take the agent to undesirable situations. We formulate these problems as constrained Markov decision processes (CMDPs) and present safe policy optimization algorithms that are based on a Lyapunov approach to solve them. Our algorithms can use any standard policy gradient (PG) method, such as deep deterministic policy gradient (DDPG) or proximal policy optimization (PPO), to train a neural network policy, while guaranteeing near-constraint satisfaction for every policy update by projecting either the policy parameter or the action onto the set of feasible solutions induced by the state-dependent linearized Lyapunov constraints. Compared to the existing constrained PG algorithms, ours are more data efficient as they are able to utilize both on-policy and off-policy data. Moreover, our action-projection algorithm often leads to less conservative policy updates and allows for natural integration into an end-to-end PG training pipeline. We evaluate our algorithms and compare them with the state-of-the-art baselines on several simulated (MuJoCo) tasks, as well as a real-world indoor robot navigation problem, demonstrating their effectiveness in terms of balancing performance and constraint satisfaction. Videos of the experiments can be found in the following link: https://drive.google.com/file/d/1pzuzFqWIE710bE2U6DmS59AfRzqK2Kek/view?usp=sharing.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/Chow et al_2019_Lyapunov-based Safe Policy Optimization for Continuous Control.pdf;/home/ricks/Zotero/storage/UN4URN36/1901.html}
}

@unpublished{christianoDeepReinforcementLearning2017,
  title = {Deep Reinforcement Learning from Human Preferences},
  author = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
  date = {2017-07-13},
  eprint = {1706.03741},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1706.03741},
  urldate = {2020-07-02},
  abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,reinforcement learning,safety,Statistics - Machine Learning}
}

@unpublished{chuaDeepReinforcementLearning2018,
  title = {Deep {{Reinforcement Learning}} in a {{Handful}} of {{Trials}} Using {{Probabilistic Dynamics Models}}},
  author = {Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
  date = {2018-05-30},
  eprint = {1805.12114},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1805.12114},
  urldate = {2019-04-18},
  abstract = {Model-based reinforcement learning (RL) algorithms can attain excellent sample efficiency, but often lag behind the best model-free algorithms in terms of asymptotic performance. This is especially true with high-capacity parametric function approximators, such as deep networks. In this paper, we study how to bridge this gap, by employing uncertainty-aware dynamics models. We propose a new algorithm called probabilistic ensembles with trajectory sampling (PETS) that combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation. Our comparison to state-of-the-art model-based and model-free deep RL algorithms shows that our approach matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks, while requiring significantly fewer samples (e.g., 8 and 125 times fewer samples than Soft Actor Critic and Proximal Policy Optimization respectively on the half-cheetah task).},
  langid = {english},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,machine learning control,safe-rl,Statistics - Machine Learning}
}

@unpublished{chuRealworldMultiobjectMultigrasp2018,
  title = {Real-World {{Multi-object}}, {{Multi-grasp Detection}}},
  author = {Chu, Fu-Jen and Xu, Ruinian and Vela, Patricio A.},
  date = {2018-02-01},
  eprint = {1802.00520},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1802.00520},
  urldate = {2019-05-06},
  abstract = {A deep learning architecture is proposed to predict graspable locations for robotic manipulation. It considers situations where no, one, or multiple object(s) are seen. By defining the learning problem to be classification with null hypothesis competition instead of regression, the deep neural network with RGB-D image input predicts multiple grasp candidates for a single object or multiple objects, in a single shot. The method outperforms state-of-the-art approaches on the Cornell dataset with 96.0\% and 96.1\% accuracy on image-wise and object-wise splits, respectively. Evaluation on a multi-object dataset illustrates the generalization capability of the architecture. Grasping experiments achieve 96.0\% grasp localization and 89.0\% grasping success rates on a test set of household objects. The real-time process takes less than .25 s from image to plan.},
  langid = {english},
  keywords = {#nosource,Computer Science - Robotics,grasp pose detection,neural network,object grasping,robotic grasping,tno internship},
  annotation = {00000}
}

@unpublished{claveraModelBasedReinforcementLearning2018,
  title = {Model-{{Based Reinforcement Learning}} via {{Meta-Policy Optimization}}},
  author = {Clavera, Ignasi and Rothfuss, Jonas and Schulman, John and Fujita, Yasuhiro and Asfour, Tamim and Abbeel, Pieter},
  date = {2018-09-13},
  eprint = {1809.05214},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1809.05214},
  urldate = {2020-07-02},
  abstract = {Model-based reinforcement learning approaches carry the promise of being data efficient. However, due to challenges in learning dynamics models that sufficiently match the real-world dynamics, they struggle to achieve the same asymptotic performance as model-free methods. We propose Model-Based Meta-Policy-Optimization (MB-MPO), an approach that foregoes the strong reliance on accurate learned dynamics models. Using an ensemble of learned dynamic models, MB-MPO meta-learns a policy that can quickly adapt to any model in the ensemble with one policy gradient step. This steers the meta-policy towards internalizing consistent dynamics predictions among the ensemble while shifting the burden of behaving optimally w.r.t. the model discrepancies towards the adaptation step. Our experiments show that MB-MPO is more robust to model imperfections than previous model-based approaches. Finally, we demonstrate that our approach is able to match the asymptotic performance of model-free methods while requiring significantly less experience.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,learned model,model-based,reinforcement learning,Statistics - Machine Learning}
}

@article{cohenMotionAdaptationBased2021,
  title = {Motion {{Adaptation Based}} on {{Learning}} the {{Manifold}} of {{Task}} and {{Dynamic Movement Primitive Parameters}}},
  author = {Cohen, Yosef and Bar-Shira, Or and Berman, Sigal},
  date = {2021-07},
  journaltitle = {Robotica},
  shortjournal = {Robotica},
  volume = {39},
  number = {7},
  pages = {1299--1315},
  issn = {0263-5747, 1469-8668},
  doi = {10.1017/S0263574720001186},
  url = {https://www.cambridge.org/core/product/identifier/S0263574720001186/type/journal_article},
  urldate = {2022-10-31},
  abstract = {Dynamic movement primitives (DMP) are motion building blocks suitable for real-world tasks. We suggest a methodology for learning the manifold of task and DMP parameters, which facilitates runtime adaptation to changes in task requirements while ensuring predictable and robust performance. For efficient learning, the parameter space is analyzed using principal component analysis and locally linear embedding. Two manifold learning methods: kernel estimation and deep neural networks, are investigated for a ball throwing task in simulation and in a physical environment. Low runtime estimation errors are obtained for both learning methods, with an advantage to kernel estimation when data sets are small.},
  langid = {english},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/cohen_et_al_2021_motion_adaptation_based_on_learning_the_manifold_of_task_and_dynamic_movement.pdf}
}

@online{colasHitchhikerGuideStatistical2022,
  title = {A {{Hitchhiker}}'s {{Guide}} to {{Statistical Comparisons}} of {{Reinforcement Learning Algorithms}}},
  author = {Colas, Cédric and Sigaud, Olivier and Oudeyer, Pierre-Yves},
  date = {2022-08-29},
  eprint = {1904.06979},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1904.06979},
  urldate = {2024-03-02},
  abstract = {Consistently checking the statistical significance of experimental results is the first mandatory step towards reproducible science. This paper presents a hitchhiker's guide to rigorous comparisons of reinforcement learning algorithms. After introducing the concepts of statistical testing, we review the relevant statistical tests and compare them empirically in terms of false positive rate and statistical power as a function of the sample size (number of seeds) and effect size. We further investigate the robustness of these tests to violations of the most common hypotheses (normal distributions, same distributions, equal variances). Beside simulations, we compare empirical distributions obtained by running Soft-Actor Critic and Twin-Delayed Deep Deterministic Policy Gradient on Half-Cheetah. We conclude by providing guidelines and code to perform rigorous comparisons of RL algorithm performances.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Methodology},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/Colas et al_2022_A Hitchhiker's Guide to Statistical Comparisons of Reinforcement Learning.pdf}
}

@article{colasHowManyRandom,
  title = {How {{Many Random Seeds}}? {{Statistical Power Analysis}} in {{Deep Reinforcement Learning Experiments}}},
  author = {Colas, Cédric and Sigaud, Olivier and Oudeyer, Pierre-Yves},
  abstract = {Consistently checking the statistical significance of experimental results is one of the mandatory methodological steps to address the so-called “reproducibility crisis” in deep reinforcement learning. In this tutorial paper, we explain how the number of random seeds relates to the probabilities of statistical errors. For both the t-test and the bootstrap confidence interval test, we recall theoretical guidelines to determine the number of random seeds one should use to provide a statistically significant comparison of the performance of two algorithms. Finally, we discuss the influence of deviations from the assumptions usually made by statistical tests. We show that they can lead to inaccurate evaluations of statistical errors and provide guidelines to counter these negative effects. We make our code available to perform the tests1.},
  langid = {english},
  file = {/home/ricks/Zotero/storage/H8SW423T/Colas et al. - How Many Random Seeds Statistical Power Analysis .pdf}
}

@inproceedings{colbertUsingTrajectoryMeasurements2018,
  title = {Using Trajectory Measurements to Estimate the Region of Attraction of Nonlinear Systems},
  booktitle = {2018 {{IEEE Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Colbert, Brendon K. and Peet, Matthew M.},
  date = {2018},
  pages = {2341--2347},
  publisher = {{IEEE}},
  keywords = {lyapunov functions,READ,stability regions},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/colbert_peet_2018_using_trajectory_measurements_to_estimate_the_region_of_attraction_of_nonlinear.pdf}
}

@inproceedings{conkeyActiveLearningProbabilistic2019,
  title = {Active {{Learning}} of {{Probabilistic Movement Primitives}}},
  booktitle = {2019 {{IEEE-RAS}} 19th {{International Conference}} on {{Humanoid Robots}} ({{Humanoids}})},
  author = {Conkey, Adam and Hermans, Tucker},
  date = {2019-10},
  pages = {1--8},
  issn = {2164-0580},
  doi = {10.1109/Humanoids43949.2019.9035026},
  abstract = {A Probabilistic Movement Primitive (ProMP) defines a distribution over trajectories with an associated feedback policy. ProMPs are typically initialized from human demonstrations and achieve task generalization through probabilistic operations. However, there is currently no principled guidance in the literature to determine how many demonstrations a teacher should provide and what constitutes a “good” demonstration for promoting generalization. In this paper, we present an active learning approach to learning a library of ProMPs capable of task generalization over a given space. We utilize uncertainty sampling techniques to generate a task instance for which a teacher should provide a demonstration. The provided demonstration is incorporated into an existing ProMP if possible, or a new ProMP is created from the demonstration if it is determined that it is too dissimilar from existing demonstrations. We provide a qualitative comparison between common active learning metrics; motivated by this comparison we present a novel uncertainty sampling approach named “Greatest Mahalanobis Distance.” We perform grasping experiments on a real KUKA robot and show our novel active learning measure achieves better task generalization with fewer demonstrations than a random sampling over the space.},
  eventtitle = {2019 {{IEEE-RAS}} 19th {{International Conference}} on {{Humanoid Robots}} ({{Humanoids}})},
  keywords = {Grasping,Libraries,Probabilistic logic,READ,Robots,Task analysis,Trajectory,Uncertainty},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/conkey_hermans_2019_active_learning_of_probabilistic_movement_primitives.pdf}
}

@article{cordoniVariableStochasticAdmittance2020,
  title = {A Variable Stochastic Admittance Control Framework with Energy Tank},
  author = {Cordoni, Francesco and Persio, Luca Di and Muradore, Riccardo},
  date = {2020-01-01},
  journaltitle = {IFAC-PapersOnLine},
  shortjournal = {IFAC-PapersOnLine},
  series = {21st {{IFAC World Congress}}},
  volume = {53},
  number = {2},
  pages = {9986--9991},
  issn = {2405-8963},
  doi = {10.1016/j.ifacol.2020.12.2716},
  url = {https://www.sciencedirect.com/science/article/pii/S2405896320334790},
  urldate = {2022-04-29},
  abstract = {In this paper we address the problem of implementing a stochastic variable admittance control. Both the variable part of the admittance control and the noise affecting the system may concur to the instability of the system. We propose an energy tank approach, based on the theory of stochastic port–Hamiltonian systems and weak passivity, where the energy dissipated by the stochastic system, if any, is stored into the tank to implement the desired actions. As we consider a non–vanishing noise, we need to consider weaker notion of passivity and convergence. We will show how the notion of weak passivity can be properly defined so that equipping a stochastic system with a suitable energy tank, variable admittance control can be efficiently implemented. We prove that the overall system is weakly passive and it converges toward an invariant measure. Simulation results show the effectiveness of the derived theoretical framework.},
  langid = {english},
  keywords = {Cartesian impedance,Passivity,READ,STABILITY,Stochastic port–Hamiltonian systems,Ultimately stochastic passive,Variable admittance control},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/cordoni_et_al_2020_a_variable_stochastic_admittance_control_framework_with_energy_tank.pdf}
}

@online{CornelliusgpGpytorch,
  title = {Cornellius-Gp/Gpytorch},
  url = {https://github.com/cornellius-gp/gpytorch},
  urldate = {2021-05-07},
  abstract = {A highly efficient and modular implementation of Gaussian Processes in PyTorch - cornellius-gp/gpytorch},
  langid = {english},
  organization = {{GitHub}},
  keywords = {#nosource,code,repository}
}

@online{coulombeGeneratingStableCollisionFree2022,
  title = {Generating {{Stable}} and {{Collision-Free Policies}} through {{Lyapunov Function Learning}}},
  author = {Coulombe, Alexandre and Lin, Hsiu-Chin},
  date = {2022-11-16},
  eprint = {2211.08976},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2211.08976},
  urldate = {2022-12-09},
  abstract = {The need for rapid and reliable robot deployment is on the rise. Imitation Learning (IL) has become popular for producing motion planning policies from a set of demonstrations. However, many methods in IL are not guaranteed to produce stable policies. The generated policy may not converge to the robot target, reducing reliability, and may collide with its environment, reducing the safety of the system. Stable Estimator of Dynamic Systems (SEDS) produces stable policies by constraining the Lyapunov stability criteria during learning, but the Lyapunov candidate function had to be manually selected. In this work, we propose a novel method for learning a Lyapunov function and a policy using a single neural network model. The method can be equipped with an obstacle avoidance module for convex object pairs to guarantee no collisions. We demonstrated our method is capable of finding policies in several simulation environments and transfer to a real-world scenario.},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/coulombe_lin_2022_generating_stable_and_collision-free_policies_through_lyapunov_function_learning.pdf}
}

@unpublished{dabneyDistributionalReinforcementLearning2017,
  title = {Distributional {{Reinforcement Learning}} with {{Quantile Regression}}},
  author = {Dabney, Will and Rowland, Mark and Bellemare, Marc G. and Munos, Rémi},
  date = {2017-10-27},
  eprint = {1710.10044},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1710.10044},
  urldate = {2020-07-02},
  abstract = {In reinforcement learning an agent interacts with the environment by taking actions and observing the next state and reward. When sampled probabilistically, these state transitions, rewards, and actions can all induce randomness in the observed long-term return. Traditionally, reinforcement learning algorithms average over this randomness to estimate the value function. In this paper, we build on recent work advocating a distributional approach to reinforcement learning in which the distribution over returns is modeled explicitly instead of only estimating the mean. That is, we examine methods of learning the value distribution instead of the value function. We give results that close a number of gaps between the theoretical and algorithmic results given by Bellemare, Dabney, and Munos (2017). First, we extend existing results to the approximate distribution setting. Second, we present a novel distributional reinforcement learning algorithm consistent with our theoretical formulation. Finally, we evaluate this new algorithm on the Atari 2600 games, observing that it significantly outperforms many of the recent improvements on DQN, including the related distributional algorithm C51.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,distributional RL,model-free,q-learning,reinforcement learning,Statistics - Machine Learning}
}

@unpublished{dabneyImplicitQuantileNetworks2018,
  title = {Implicit {{Quantile Networks}} for {{Distributional Reinforcement Learning}}},
  author = {Dabney, Will and Ostrovski, Georg and Silver, David and Munos, Rémi},
  date = {2018-06-14},
  eprint = {1806.06923},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1806.06923},
  urldate = {2020-07-02},
  abstract = {In this work, we build on recent advances in distributional reinforcement learning to give a generally applicable, flexible, and state-of-the-art distributional variant of DQN. We achieve this by using quantile regression to approximate the full quantile function for the state-action return distribution. By reparameterizing a distribution over the sample space, this yields an implicitly defined return distribution and gives rise to a large class of risk-sensitive policies. We demonstrate improved performance on the 57 Atari 2600 games in the ALE, and use our algorithm's implicitly defined distributions to study the effects of risk-sensitive policies in Atari games.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,distributional RL,model-free,reinforcement learning,Statistics - Machine Learning}
}

@unpublished{dalalSafeExplorationContinuous2018,
  title = {Safe {{Exploration}} in {{Continuous Action Spaces}}},
  author = {Dalal, Gal and Dvijotham, Krishnamurthy and Vecerik, Matej and Hester, Todd and Paduraru, Cosmin and Tassa, Yuval},
  date = {2018-01-26},
  eprint = {1801.08757},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1801.08757},
  urldate = {2020-07-02},
  abstract = {We address the problem of deploying a reinforcement learning (RL) agent on a physical system such as a datacenter cooling unit or robot, where critical constraints must never be violated. We show how to exploit the typically smooth dynamics of these systems and enable RL algorithms to never violate constraints during learning. Our technique is to directly add to the policy a safety layer that analytically solves an action correction formulation per each state. The novelty of obtaining an elegant closed-form solution is attained due to a linearized model, learned on past trajectories consisting of arbitrary actions. This is to mimic the real-world circumstances where data logs were generated with a behavior policy that is implausible to describe mathematically; such cases render the known safety-aware off-policy methods inapplicable. We demonstrate the efficacy of our approach on new representative physics-based environments, and prevail where reward shaping fails by maintaining zero constraint violations.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,constrained policy optimization,reinforcement learning,safety}
}

@inproceedings{davoodiProbabilisticMovementPrimitive2021,
  title = {Probabilistic {{Movement Primitive Control}} via {{Control Barrier Functions}}},
  booktitle = {2021 {{IEEE}} 17th {{International Conference}} on {{Automation Science}} and {{Engineering}} ({{CASE}})},
  author = {Davoodi, Mohammadreza and Iqbal, Asif and Cloud, Joseph M. and Beksi, William J. and Gans, Nicholas R.},
  date = {2021},
  pages = {697--703},
  publisher = {{IEEE}},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/davoodi_et_al_2021_probabilistic_movement_primitive_control_via_control_barrier_functions.pdf}
}

@article{davoodiRuleBasedSafeProbabilistic2022,
  title = {Rule-{{Based Safe Probabilistic Movement Primitive Control}} via {{Control Barrier Functions}}},
  author = {Davoodi, Mohammadreza and Iqbal, Asif and Cloud, Joseph M. and Beksi, William J. and Gans, Nicholas R.},
  date = {2022},
  journaltitle = {IEEE Transactions on Automation Science and Engineering},
  pages = {1--15},
  issn = {1558-3783},
  doi = {10.1109/TASE.2022.3217468},
  abstract = {In this paper, we develop a novel and safe control design approach that takes demonstrations provided by a human teacher to enable a robot to accomplish complex manipulation scenarios in dynamic environments. First, an overall task is divided into multiple simpler subtasks that are more appropriate for learning and control objectives. Then, by collecting human demonstrations, the subtasks that require robot movement are modeled by probabilistic movement primitives (ProMPs). We also study two strategies for modifying the ProMPs to avoid collisions with environmental obstacles. Finally, we introduce a rule-base control technique by utilizing a finite-state machine along with a unique means of control design for ProMPs. For the ProMP controller, we propose control barrier and Lyapunov functions to guide the system along a trajectory within the distribution defined by a ProMP while guaranteeing that the system state never leaves more than a desired distance from the distribution mean. This allows for better performance on nonlinear systems and offers solid stability and known bounds on the system state. A series of simulations and experimental studies demonstrate the efficacy of our approach and show that it can run in real time. Note to Practitioners—This paper is motivated by the need to create a teach-by-demonstration framework that captures the strengths of movement primitives and verifiable, safe control. We provide a framework that learns safe control laws from a probability distribution of robot trajectories through the use of advanced nonlinear control that incorporates safety constraints. Typically, such distributions are stochastic, making it difficult to offer any guarantees on safe operation. Our approach ensures that the distribution of allowed robot trajectories is within an envelope of safety and allows for robust operation of a robot. Furthermore, using our framework various probability distributions can be combined to represent complex scenarios in the environment. It will benefit practitioners by making it substantially easier to test and deploy accurate, efficient, and safe robots in complex real-world scenarios. The approach is currently limited to scenarios involving static obstacles, with dynamic obstacle avoidance an avenue of future effort.},
  eventtitle = {{{IEEE Transactions}} on {{Automation Science}} and {{Engineering}}},
  keywords = {Collision avoidance,learning from demonstration,motion and path planning,Motion control,optimization and optimal control,READ,Robot kinematics,robot safety,Robots,Safety,Task analysis,Training,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/davoodi_et_al_2022_rule-based_safe_probabilistic_movement_primitive_control_via_control_barrier.pdf}
}

@article{davoodiSafeRobotTrajectory2022,
  title = {Safe {{Robot Trajectory Control Using Probabilistic Movement Primitives}} and {{Control Barrier Functions}}},
  author = {Davoodi, Mohammadreza and Iqbal, Asif and Cloud, Joseph M. and Beksi, William J. and Gans, Nicholas R.},
  date = {2022},
  journaltitle = {Frontiers in Robotics and AI},
  volume = {9},
  issn = {2296-9144},
  url = {https://www.frontiersin.org/articles/10.3389/frobt.2022.772228},
  urldate = {2022-12-01},
  abstract = {In this paper, we present a novel means of control design for probabilistic movement primitives (ProMPs). Our proposed approach makes use of control barrier functions and control Lyapunov functions defined by a ProMP distribution. Thus, a robot may move along a trajectory within the distribution while guaranteeing that the system state never leaves more than a desired distance from the distribution mean. The control employs feedback linearization to handle nonlinearities in the system dynamics and real-time quadratic programming to ensure a solution exists that satisfies all safety constraints while minimizing control effort. Furthermore, we highlight how the proposed method may allow a designer to emphasize certain safety objectives that are more important than the others. A series of simulations and experiments demonstrate the efficacy of our approach and show it can run in real time.},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/davoodi_et_al_2022_safe_robot_trajectory_control_using_probabilistic_movement_primitives_and.pdf}
}

@article{deisenrothEfficientReinforcementLearning,
  title = {Efficient {{Reinforcement Learning}} Using {{Gaussian Processes}}},
  author = {Deisenroth, Marc Peter},
  pages = {217},
  langid = {english},
  keywords = {#nosource}
}

@inproceedings{deisenrothPILCOModelbasedDataefficient2011,
  title = {{{PILCO}}: {{A}} Model-Based and Data-Efficient Approach to Policy Search},
  shorttitle = {{{PILCO}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on Machine Learning ({{ICML-11}})},
  author = {Deisenroth, Marc and Rasmussen, Carl E.},
  date = {2011},
  pages = {465--472},
  keywords = {#nosource,machine learning control}
}

@inproceedings{dekaKoopmanbasedNeuralLyapunov2022,
  title = {Koopman-Based {{Neural Lyapunov}} Functions for General Attractors},
  booktitle = {2022 {{IEEE}} 61st {{Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Deka, Shankar A. and Valle, Alonso M. and Tomlin, Claire J.},
  date = {2022-12},
  pages = {5123--5128},
  issn = {2576-2370},
  doi = {10.1109/CDC51059.2022.9992927},
  abstract = {Koopman spectral theory has grown in the past decade as a powerful tool for dynamical systems analysis and control. In this paper, we show how recent data-driven techniques for estimating Koopman-Invariant subspaces with neural networks can be leveraged to extract Lyapunov certificates for the underlying system. In our work, we specifically focus on systems with a limit-cycle, beyond just an isolated equilibrium point, and use Koopman eigenfunctions to efficiently parameterize candidate Lyapunov functions to construct forward-invariant sets under some (unknown) attractor dynamics. Additionally, when the dynamics are polynomial and when neural networks are replaced by polynomials as a choice of function approximators in our approach, one can further leverage Sum-of-Squares programs and/or nonlinear programs to yield provably correct Lyapunov certificates. In such a polynomial case, our Koopman-based approach for constructing Lyapunov functions uses significantly fewer decision variables compared to directly formulating and solving a Sum-of-Squares optimization problem.},
  eventtitle = {2022 {{IEEE}} 61st {{Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  keywords = {Approximation algorithms,Control systems,Eigenvalues and eigenfunctions,Limit-cycles,Neural networks,Nonlinear dynamical systems,READ,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/deka_et_al_2022_koopman-based_neural_lyapunov_functions_for_general_attractors.pdf}
}

@article{denisaLearningCompliantMovement2015,
  title = {Learning Compliant Movement Primitives through Demonstration and Statistical Generalization},
  author = {Deniša, Miha and Gams, Andrej and Ude, Aleš and Petrič, Tadej},
  date = {2015},
  journaltitle = {IEEE/ASME transactions on mechatronics},
  volume = {21},
  number = {5},
  pages = {2581--2594},
  publisher = {{IEEE}},
  keywords = {READ,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/deniša_et_al_2015_learning_compliant_movement_primitives_through_demonstration_and_statistical.pdf}
}

@unpublished{depewegLearningPolicySearch2016,
  title = {Learning and {{Policy Search}} in {{Stochastic Dynamical Systems}} with {{Bayesian Neural Networks}}},
  author = {Depeweg, Stefan and Hernández-Lobato, José Miguel and Doshi-Velez, Finale and Udluft, Steffen},
  date = {2016-05-23},
  eprint = {1605.07127},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1605.07127},
  urldate = {2019-04-18},
  abstract = {We present an algorithm for policy search in stochastic dynamical systems using model-based reinforcement learning. The system dynamics are described with Bayesian neural networks (BNNs) that include stochastic input variables. These input variables allow us to capture complex statistical patterns in the transition dynamics (e.g. multi-modality and heteroskedasticity), which are usually missed by alternative modeling approaches. After learning the dynamics, our BNNs are then fed into an algorithm that performs random roll-outs and uses stochastic optimization for policy learning. We train our BNNs by minimizing α-divergences with α = 0.5, which usually produces better results than other techniques such as variational Bayes. We illustrate the performance of our method by solving a challenging problem where model-based approaches usually fail and by obtaining promising results in real-world scenarios including the control of a gas turbine and an industrial benchmark.},
  langid = {english},
  keywords = {#nosource,Computer Science - Machine Learning,machine learning control,safe-rl,Statistics - Machine Learning}
}

@inproceedings{devonportBayesianSafeLearning2020,
  title = {Bayesian {{Safe Learning}} and {{Control}} with {{Sum-of-Squares Analysis}} and {{Polynomial Kernels}}},
  booktitle = {2020 59th {{IEEE Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Devonport, Alex and Yin, He and Arcak, Murat},
  date = {2020-12},
  pages = {3159--3165},
  issn = {2576-2370},
  doi = {10.1109/CDC42340.2020.9304397},
  abstract = {We propose an iterative method to safely learn the unmodeled dynamics of a control-affine nonlinear system using Bayesian Gaussian process (GP) models with polynomial kernel functions. The method maintains safety by ensuring that the system state stays within the region of attraction (ROA) of a stabilizing control policy while collecting data. A quadratic programming based exploration control policy is computed to keep the exploration trajectory inside an inner-approximation of the ROA and to maximize the information gained from the trajectory. A prior GP model, which incorporates prior information about the unknown dynamics, is used to construct an initial stabilizing policy. As the GP model is updated with data, it is used to synthesize a new policy and a larger ROA, which increases the range of safe exploration. The use of polynomial kernels allows us to compute ROA inner-approximations and stabilizing control laws for the model using sum-of-squares programming. We also provide a probabilistic guarantee of safety which ensures that the policy computed using the learned model stabilizes the true dynamics with high confidence.},
  eventtitle = {2020 59th {{IEEE Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  keywords = {Analytical models,Computational modeling,Data models,Kernel,Lyapunov methods,READ,Safety,STABILITY,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/devonport_et_al_2020_bayesian_safe_learning_and_control_with_sum-of-squares_analysis_and_polynomial.pdf}
}

@article{dietrichPassivationProjectionBasedNull2016,
  title = {Passivation of {{Projection-Based Null Space Compliance Control Via Energy Tanks}}},
  author = {Dietrich, Alexander and Ott, Christian and Stramigioli, Stefano},
  date = {2016-01},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {1},
  number = {1},
  pages = {184--191},
  issn = {2377-3766},
  doi = {10.1109/LRA.2015.2512937},
  abstract = {A manipulator with kinematic redundancy w.r.t. a main task enables to perform an additional, subordinate task in its null space. The classical hierarchy-based approach is to apply dynamically consistent null space projections such that this lower-priority task does not disturb the main task. The major problem is that the control task hierarchy is established at the expense of the passivity of the system, which causes problems in terms of stability, robustness, and safety. Here, we extend this classical approach and passivate the closed-loop system by means of an energy tank, which is filled by the dissipated power of the active damping on both hierarchy levels. We validate the method in simulations and experiments on a torque-controlled manipulator. The concept is predestined for physical human-robot interaction and safety-relevant applications, where passivity must be guaranteed.},
  eventtitle = {{{IEEE Robotics}} and {{Automation Letters}}},
  keywords = {Aerospace electronics,Asymptotic stability,Compliance and impedance control,Compliance and Impedance Control,Damping,Manipulator dynamics,Null space,READ,redundant robots,Redundant Robots,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/dietrich_et_al_2016_passivation_of_projection-based_null_space_compliance_control_via_energy_tanks.pdf}
}

@article{dietrichPassiveHierarchicalImpedance2017,
  title = {Passive {{Hierarchical Impedance Control Via Energy Tanks}}},
  author = {Dietrich, Alexander and Wu, Xuwei and Bussmann, Kristin and Ott, Christian and Albu-Schaeffer, Alin and Stramigioli, Stefano},
  date = {2017-04},
  journaltitle = {Ieee Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {2},
  number = {2},
  pages = {522--529},
  publisher = {{Ieee-Inst Electrical Electronics Engineers Inc}},
  location = {{Piscataway}},
  issn = {2377-3766},
  doi = {10.1109/LRA.2016.2645504},
  url = {https://ieeexplore.ieee.org/document/7801022},
  urldate = {2022-05-02},
  abstract = {Modern robotic systems with a large number of actuated degrees of freedom can be utilized to perform several tasks at the same time while following a given order of priority. The most frequently used method is to apply null space projections to realize such a strict hierarchy, where lower priority tasks are executed as long as they do not interfere with any higher priority objectives. However, introducing null space projectors inevitably destroys the beneficial and safety-relevant feature of passivity. Here, two controllers are proposed to restore the passivity: one with local energy tanks on each hierarchy level and one with a global tank for the entire system. The formal proofs of passivity show that no energy is generated by these controllers. Once the tanks are empty, passivity is still guaranteed at the cost of some control performance. Simulations and experiments on a torque-controlled robot validate the approaches and predestine them for the usage in safety-relevant applications.},
  langid = {english},
  keywords = {bilateral teleoperation,Compliance and impedance control,force control,motion control of   manipulators,READ,redundant robots,robot manipulators,STABILITY,topology},
  annotation = {WOS:000413736600020},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/dietrich_et_al_2017_passive_hierarchical_impedance_control_via_energy_tanks.pdf}
}

@inproceedings{dimeasProgressiveAutomationPeriodic2020,
  title = {Progressive {{Automation}} of {{Periodic Movements}}},
  booktitle = {Human-{{Friendly Robotics}} 2019},
  author = {Dimeas, Fotios and Kastritsi, Theodora and Papageorgiou, Dimitris and Doulgeri, Zoe},
  editor = {Ferraguti, Federica and Villani, Valeria and Sabattini, Lorenzo and Bonfè, Marcello},
  date = {2020},
  series = {Springer {{Proceedings}} in {{Advanced Robotics}}},
  pages = {58--72},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-42026-0_5},
  abstract = {This paper presents the extension of the progressive automation framework for periodic movements, where an operator kinesthetically demonstrates a movement and the robotic manipulator progressively takes the lead until it is able to execute the task autonomously. The basic frequency of the periodic movement in the operational space is determined using adaptive frequency oscillators with Fourier approximation. The multi-dimensionality issue of the demonstrated movement is handled by using a common canonical system and the attractor landscape is learned online with periodic Dynamic Movement Primitives. Based on the robot’s tracking error and the operator’s applied force, we continuously adjust the adaptation rate of the frequency and the waveform learning during the demonstration, as well as the target stiffness of the robot, while progressive automation is achieved. In this way, we enable the operator to intervene and demonstrate either small modifications or entirely new tasks and seamless transition between guided and autonomous operation of the robot, without distinguishing among a learning and a reproduction phase. The proposed method is verified experimentally with an operator demonstrating periodic tasks in the free-space and in contact with the environment for wiping a surface.},
  isbn = {978-3-030-42026-0},
  langid = {english},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/dimeas_et_al_2020_progressive_automation_of_periodic_movements.pdf}
}

@inproceedings{dingObserverBasedAdaptiveImpedance2021,
  title = {Observer-{{Based Adaptive Impedance Control}} for {{Robotic Systems With Predefined Task Space}}},
  booktitle = {2021 {{China Automation Congress}} ({{CAC}})},
  author = {Ding, Shuai and Peng, Jinzhu and Wang, Zhiqiang and Dong, Mengchao},
  date = {2021-10},
  pages = {1161--1166},
  issn = {2688-0938},
  doi = {10.1109/CAC53003.2021.9728321},
  abstract = {This paper presents an observer-based adaptive impedance control (OBAIC) scheme to coordinate the robotic systems with predefined task space (output constraint), where the robot dynamic model is estimated by neural network (NN). The reference trajectory is shaped by the impedance control model and constraint region to ensure the safety and compliant performance of the robotic systems. Under no knowledge of the system model, an observer is proposed to obtain the velocities of the robotic manipulator. According to the barrier Lyapunov function (BLF) and NN technology, the OBAIC scheme is proposed to tracking the shaped trajectory and achieve the prescribed constraint and steady-state performance. Finally, the simulation tests of the 2-degree of freedom (DOF) robotic manipulator with output constraints are conducted to verify the proposed OBAIC strategy, and the results show the feasibility of the OBAIC scheme.},
  eventtitle = {2021 {{China Automation Congress}} ({{CAC}})},
  keywords = {Adaptation models,adaptive control,Adaptive systems,Aerospace electronics,Artificial neural networks,impedance control,Observers,output constraints,Robot kinematics,robotic systems,state observer,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ding_et_al_2021_observer-based_adaptive_impedance_control_for_robotic_systems_with_predefined.pdf}
}

@online{DistributedPrioritizedExperience,
  title = {Distributed {{Prioritized Experience Replay}} | {{OpenReview}}},
  url = {https://openreview.net/forum?id=H1Dy---0Z},
  urldate = {2020-07-02},
  keywords = {#nosource,reinforcement learning,scaling RL}
}

@inproceedings{doAffordanceNetEndtoEndDeep2018,
  title = {{{AffordanceNet}}: {{An End-to-End Deep Learning Approach}} for {{Object Affordance Detection}}},
  shorttitle = {{{AffordanceNet}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Do, T. and Nguyen, A. and Reid, I.},
  date = {2018-05},
  pages = {1--5},
  doi = {10.1109/ICRA.2018.8460902},
  abstract = {We propose AffordanceNet, a new deep learning approach to simultaneously detect multiple objects and their affordances from RGB images. Our AffordanceNet has two branches: an object detection branch to localize and classify the object, and an affordance detection branch to assign each pixel in the object to its most probable affordance label. The proposed framework employs three key components for effectively handling the multiclass problem in the affordance mask: a sequence of deconvolutional layers, a robust resizing strategy, and a multi-task loss function. The experimental results on the public datasets show that our AffordanceNet outperforms recent state-of-the-art methods by a fair margin, while its end-to-end architecture allows the inference at the speed of 150ms per image. This makes our AffordanceNet well suitable for real-time robotic applications. Furthermore, we demonstrate the effectiveness of AffordanceNet in different testing environments and in real robotic applications. The source code is available at https://github.com/nqanh/affordance-net.},
  eventtitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  keywords = {#nosource,affordance label,affordance mask,AffordanceNet,Computer architecture,deconvolutional layer sequence,end-to-end architecture,end-to-end deep learning approach,Feature extraction,image classification,Image segmentation,learning (artificial intelligence),Machine learning,multiple object detection,multitask loss function,object affordance detection,object classification,object detection,Object detection,object grasping,object localization,real-time robotic applications,RGB images,robotic grasping,Robots,robust resizing strategy,testing environments,tno internship,Training},
  annotation = {00027}
}

@article{dongAdaptiveStiffnessDamping2019,
  title = {Adaptive {{Stiffness}} and {{Damping Impedance Control}} for {{Environmental Interactive Systems With Unknown Uncertainty}} and {{Disturbance}}},
  author = {Dong, Gangqi and Huang, Panfeng and Ma, Zhiqiang},
  date = {2019},
  journaltitle = {IEEE Access},
  doi = {10.1109/ACCESS.2019.2955961},
  abstract = {An impedance controller with adaptive stiffness and damping terms for environmental interactive system with unknown modeling error and external disturbance is presented and the stability is proved and the criterion of parameter selection is pointed out. This paper presents an impedance controller with adaptive stiffness and damping terms for environmental interactive system with unknown modeling error and external disturbance. The objective is to compensate the uncertainty and disturbance and form a desired close-loop impedance architecture. A disturbance observer for nonlinear system is employed to estimate the lumped uncertainty and disturbance. The stiffness and damping are adjusted by the state-variable-related negative power type parameters. Consequently, the control input is designed with the consideration of the estimation of lumped uncertainty and disturbance as well as the regulated stiffness and damping. The stability is proved and the criterion of parameter selection is pointed out. Although the state-variable-related stiffness and damping parameters will intuitively be singular, the system works without any singularity due to the independence of these parameters in stability analysis. To verify the proposed approach, a one degree of freedom mass system is adopted and numerical simulation is performed accordingly. The simulation results demonstrate the effectiveness and the superiority of the proposed control scheme for environmental interactive system with unknown modeling error and external disturbance.},
  keywords = {READ,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/dong_et_al_2019_adaptive_stiffness_and_damping_impedance_control_for_environmental_interactive.pdf}
}

@inproceedings{dongDMPbasedOnlineAdaptive2021,
  title = {A {{DMP-based Online Adaptive Stiffness Adjustment Method}}},
  booktitle = {{{IECON}} 2021 – 47th {{Annual Conference}} of the {{IEEE Industrial Electronics Society}}},
  author = {Dong, Jiale and Si, Weiyong and Yang, Chenguang},
  date = {2021-10},
  pages = {1--6},
  issn = {2577-1647},
  doi = {10.1109/IECON48115.2021.9589707},
  abstract = {Learning from demonstration (LfD) is a promising method for robots to learn and generalize human-like skills. It has the advantages of high programming efficiency, easy optimization, and non-professionals can also operate. There is a lot of research work that learn motion trajectories and stiffness curves from human demonstrations simutaneously to make the robot compliant, but previous work rarely consider the changes of environment. In this article, we propose an adaptive stiffness method that enables the robot to learn motion and stiffness trajectories from a single demonstration. When the environment changes, it can spontaneously tune the stiffness according to environmental feedback to ensure the smoothness of the task. Thus the robot has the ability to adapt to environmental changes. We first proved the theoretical feasibility of the method, and then we conducted physical experiments on the Baxter robot to verify the effectiveness of the proposed method.},
  eventtitle = {{{IECON}} 2021 – 47th {{Annual Conference}} of the {{IEEE Industrial Electronics Society}}},
  keywords = {Adaptation models,Conferences,Delay effects,Industrial electronics,Programming,READ,Task analysis,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/dong_et_al_2021_a_dmp-based_online_adaptive_stiffness_adjustment_method.pdf}
}

@article{dongImpedanceControlCoordinated2021,
  title = {Impedance {{Control}} for {{Coordinated Robots}} by {{State}} and {{Output Feedback}}},
  author = {Dong, Yiting and He, Wei and Kong, Linghuan and Hua, Xiang},
  date = {2021-08},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  volume = {51},
  number = {8},
  pages = {5056--5066},
  issn = {2168-2232},
  doi = {10.1109/TSMC.2019.2947453},
  abstract = {The impedance control for coordinated robots interacting with the unknown environment is investigated in this article, subject to unknown system dynamics and the environment with which coordinated robots come into contact. For the whole system, impedance control is developed for coordinated robots. The notable feature is that the robot-environment interaction performance is improved without any information about the environment, so that the robotic system follows the commanded position trajectory in noncontact phase, while the desired destination is obtained according to the force exerted on the environment during contact phase. Moreover, based on assumption that some system signals are unmeasurable, output feedback control is designed for coordinated robot systems, where a state observer based on neural network technique is designed, that can force the state estimate error converge to a small neighborhood of zero. Simulation results are provided to demonstrate the effectiveness of the proposed control algorithm.},
  eventtitle = {{{IEEE Transactions}} on {{Systems}}, {{Man}}, and {{Cybernetics}}: {{Systems}}},
  keywords = {Coordinated robots,Impedance,impedance control,neural networks,Neural networks,Observers,Output feedback,Robot kinematics,Robot sensing systems,state observer,uncertainty},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/dong_et_al_2021_impedance_control_for_coordinated_robots_by_state_and_output_feedback.pdf}
}

@article{dongUDEBasedVariableImpedance2019,
  title = {{{UDE-Based Variable Impedance Control}} of {{Uncertain Robot Systems}}},
  author = {Dong, Yiting and Ren, Beibei},
  date = {2019-12},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  volume = {49},
  number = {12},
  pages = {2487--2498},
  issn = {2168-2232},
  doi = {10.1109/TSMC.2017.2767566},
  abstract = {A fundamental requirement in robot control is the capability to improve the robot-environment interaction performance. Motivated by the fact that humans are able to adapt limb impedance to stably interact with various environments with skillful dexterity, this paper investigates the variable impedance control for robots, subject to uncertainties from plant model or environment. The proposed variable impedance control assists the robot to perform given interaction tasks with its unknown environment and improves the overall robot- environment system performance. The stiffness, damping, and inertia can be changed during interaction tasks, which results in configuration-dependent impedance dynamics. The uncertainty and disturbance estimator (UDE) is used to approximate the plant model with only partial information known. The prominent feature of the UDE-based control is that only the bandwidth information of the unknown plant model is needed for the control design. A stability condition for selecting the stiffness, damping, and inertia in the impedance model is provided to guarantee the stability of the control system. Extensive simulation studies are carried out to illustrate the effectiveness of the proposed method.},
  eventtitle = {{{IEEE Transactions}} on {{Systems}}, {{Man}}, and {{Cybernetics}}: {{Systems}}},
  keywords = {Damping,Force,Impedance,Impedance control,nonlinear systems,READ,robot control,Robot sensing systems,robot–environment interaction,STABILITY,Uncertainty,uncertainty and disturbance estimator (UDE)},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/dong_ren_2019_ude-based_variable_impedance_control_of_uncertain_robot_systems.pdf}
}

@article{douRobotSkillLearning2022,
  title = {A {{Robot Skill Learning Framework Based}} on {{Compliant Movement Primitives}}},
  author = {Dou, Saixiong and Xiao, Juliang and Zhao, Wei and Yuan, Hang and Liu, Haitao},
  date = {2022-03},
  journaltitle = {Journal of Intelligent \& Robotic Systems},
  shortjournal = {J. Intell. Robot. Syst.},
  volume = {104},
  number = {3},
  pages = {53},
  publisher = {{Springer}},
  location = {{Dordrecht}},
  issn = {0921-0296},
  doi = {10.1007/s10846-022-01605-4},
  url = {https://link.springer.com/article/10.1007/s10846-022-01605-4},
  urldate = {2022-04-29},
  abstract = {Collaborative robots are increasingly widely used in our lives, and at the same time, the skill learning ability of robots is becoming more and more important. For this reason, a robot skill learning framework based on compliant movement primitives is proposed in this paper. The framework consists of four modules: kinesthetic teaching, task learning, compliant movement primitive library, and task generalization. Specifically, the trajectories are collected from the kinematics of the robot, and the stiffness profiles are collected from the designed variable stiffness interface based on stiffness optimization; then the collected data is optimized, segmented, and learned to create the robot's compliant movement primitive library; the primitives in the library are adjusted and combined to generate the robot's desired trajectory and desired stiffness, which are then input into the dynamics-based variable impedance controller; thereafter the controller drives the robot to perform the desired compliant motion and complete various tasks. The framework covers the entire process of robot skill learning and application, and the proposed compliant movement primitives can simultaneously achieve the robot's trajectory learning and interactive compliance learning. The experiment of the robot learning to press buttons was carried out on a universal 6-DOF collaborative robot. The experimental results prove the effectiveness and safety of the framework and show its application value.},
  langid = {english},
  keywords = {Cartesian impedance,Compliant   movement primitives,Compliant movement primitives,design,Framework,Human-robot collaboration,IMPORTANT,Learning from demonstration,READ,STABILITY,teleoperation,Variable impedance control},
  annotation = {WOS:000767931700001},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/dou_et_al_2022_a_robot_skill_learning_framework_based_on_compliant_movement_primitives.pdf}
}

@article{douSoftRoboticManipulators2021,
  title = {Soft {{Robotic Manipulators}}: {{Designs}}, {{Actuation}}, {{Stiffness Tuning}}, and {{Sensing}}},
  shorttitle = {Soft {{Robotic Manipulators}}},
  author = {Dou, Weiqiang and Zhong, Guoliang and Cao, Jinglin and Shi, Zhun and Peng, Bowen and Jiang, Liangzhong},
  date = {2021},
  journaltitle = {Advanced Materials Technologies},
  volume = {6},
  number = {9},
  pages = {2100018},
  issn = {2365-709X},
  doi = {10.1002/admt.202100018},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/admt.202100018},
  urldate = {2022-07-22},
  abstract = {Soft robotic manipulators are continuum robots made of soft materials and flexible components. The goal of soft robotic manipulators research is to enable these manipulators to adapt their shapes to cluttered environments and to have safer interactions with human. However, there is still a problem with effectively using soft robotic manipulators in practical applications. The challenges of soft robotic manipulators in terms of materials and structure design, stiffness control, perception, and function control remain to be overcome. Here, an overview of recent advances in this field is presented, covering device architectures, actuation, variable stiffness, and sensing. Actuator technologies are discussed and roughly divided into three categories: a) tendon-driven actuation, b) fluidic actuation, and c) stimuli-responsive actuation, which is based on smart materials. Considering the working principle of stiffness variation technologies, stiffness variation technologies can be divided into two categories: a) using the interactions between structural elements, b) direct material rigidity tuning strategies. A briefly review of soft sensing technologies is presented. From the functional perspective, sensor technologies are divide into two categories: proprioception and exteroception. A conception for designing soft robotic manipulators is proposed from the perspective of soft robotic manipulator applications. Finally, the challenges this field faces are discussed.},
  langid = {english},
  keywords = {actuation,continuum robots,perceptions,soft robotic manipulators,variable stiffness},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/dou_et_al_2021_soft_robotic_manipulators.pdf}
}

@article{duanAdaptiveVariableImpedance2018,
  title = {Adaptive Variable Impedance Control for Dynamic Contact Force Tracking in Uncertain Environment},
  author = {Duan, Jinjun and Gan, Yahui and Chen, Ming and Dai, Xianzhong},
  date = {2018-04-01},
  journaltitle = {Robotics and Autonomous Systems},
  shortjournal = {Robotics and Autonomous Systems},
  volume = {102},
  pages = {54--65},
  issn = {0921-8890},
  doi = {10.1016/j.robot.2018.01.009},
  url = {https://www.sciencedirect.com/science/article/pii/S0921889017307480},
  urldate = {2022-05-03},
  abstract = {The traditional constant impedance control is a simple but effective method widely used in many fields including contact force tracking. Using this method, the location of the environment relative to the robot and the stiffness of the environment must be known, and usually the desired force is constant. However, for applications in dynamic contact force tracking in uncertain environment, it is not an effective solution. In this paper, a new adaptive variable impedance control is proposed for force tracking which has the capability to track the dynamic desired force and compensate for uncertainties (in terms of unknown geometrical and mechanical properties) in environment. In this study, the contact force model of robot end-effector and the environment is analyzed. Specifically, the contact force is used as the feedback force of a position-based impedance controller to actively track the dynamic desired force in uncertain environment. To adapt any environment stiffness uncertainties, a modified impedance control is proposed. To reduce the force tracking error caused by environment location uncertainty, an adaptive variable impedance control is implemented for the first time by adjusting the impedance parameters on-line based on the tracking error to compensate the unknown environment and the dynamic desired force. Furthermore, stability and convergence of the adaptive variable impedance control are demonstrated for a stable force tracking execution. Simulations and experiments to compare the performance of force tracking with the constant impedance control and the adaptive variable impedance control, perspectively, are conducted. The results strongly prove that the proposed approach can achieve better force tracking performance than the constant impedance control.},
  langid = {english},
  keywords = {Adaptive variable impedance control,Contact force tracking,Modified impedance control,STABILITY,Uncertain environment,Uncertainties},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/duan_et_al_2018_adaptive_variable_impedance_control_for_dynamic_contact_force_tracking_in.pdf}
}

@unpublished{duanBenchmarkingDeepReinforcement2016,
  title = {Benchmarking {{Deep Reinforcement Learning}} for {{Continuous Control}}},
  author = {Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
  date = {2016-05-27},
  eprint = {1604.06778},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1604.06778},
  urldate = {2020-07-02},
  abstract = {Recently, researchers have made significant progress combining the advances in deep learning for learning feature representations with reinforcement learning. Some notable examples include training agents to play Atari games based on raw pixel data and to acquire advanced manipulation skills using raw sensory inputs. However, it has been difficult to quantify progress in the domain of continuous control due to the lack of a commonly adopted benchmark. In this work, we present a benchmark suite of continuous control tasks, including classic tasks like cart-pole swing-up, tasks with very high state and action dimensionality such as 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure. We report novel findings based on the systematic evaluation of a range of implemented reinforcement learning algorithms. Both the benchmark and reference implementations are released at https://github.com/rllab/rllab in order to facilitate experimental reproducibility and to encourage adoption by other researchers.},
  keywords = {#nosource,benchmarks,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,reinforcement learning,reproducibility},
  annotation = {00852}
}

@article{duanFastStableLearning2019,
  title = {Fast and {{Stable Learning}} of {{Dynamical Systems Based}} on {{Extreme Learning Machine}}},
  author = {Duan, Jianghua and Ou, Yongsheng and Hu, Jianbing and Wang, Zhiyang and Jin, Shaokun and Xu, Chao},
  date = {2019-06},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  volume = {49},
  number = {6},
  pages = {1175--1185},
  issn = {2168-2232},
  doi = {10.1109/TSMC.2017.2705279},
  abstract = {The approach of dynamical system (DS) is promising for modeling robot motion, and provides a flexible means of realizing robot learning and control. Accuracy, stability, and learning speed are the three main factors to be considered when learning robot movements from human demonstrations with DS. Some approaches yield stable dynamical systems, but these may result in a poor reproduction performance, while some approaches yield good reproduction performance but are quite complex and time-consuming. In this paper, we address the accuracy-stability-speed issues simultaneously. We present a learning method named the fast and stable modeling for dynamical systems, which is based on the extreme learning machine to efficiently and accurately learn the parameters of the DS as well as to ensure the asymptotic stability at the target. We confirm the proposed approach by performing both 2-D tasks of learning handwriting motions and a set of robot experiments.},
  eventtitle = {{{IEEE Transactions}} on {{Systems}}, {{Man}}, and {{Cybernetics}}: {{Systems}}},
  keywords = {Asymptotic stability,Dynamics,Extreme learning machine (ELM),learn from demonstrations,nonlinear dynamical system,READ,Robot motion,stability analysis,Stability analysis,Training,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/duan_et_al_2019_fast_and_stable_learning_of_dynamical_systems_based_on_extreme_learning_machine.pdf}
}

@unpublished{duanRLFastReinforcement2016,
  title = {{{RL}}\$\^{}2\$: {{Fast Reinforcement Learning}} via {{Slow Reinforcement Learning}}},
  shorttitle = {{{RL}}\$\^{}2\$},
  author = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L. and Sutskever, Ilya and Abbeel, Pieter},
  date = {2016-11-09},
  eprint = {1611.02779},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1611.02779},
  urldate = {2020-07-02},
  abstract = {Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL\$\^{}2\$, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the "fast" RL algorithm on the current (previously unseen) MDP. We evaluate RL\$\^{}2\$ experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL\$\^{}2\$ is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL\$\^{}2\$ on a vision-based navigation task and show that it scales up to high-dimensional problems.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,meta-RL,reinforcement learning,Statistics - Machine Learning}
}

@article{duanSequentialLearningUnification2019,
  title = {Sequential Learning Unification Controller from Human Demonstrations for Robotic Compliant Manipulation},
  author = {Duan, Jianghua and Ou, Yongsheng and Xu, Sheng and Liu, Ming},
  date = {2019-11-13},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {366},
  pages = {35--45},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2019.07.081},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231219310884},
  urldate = {2022-04-28},
  abstract = {Robotic compliant manipulation not only contains robot motion but also embodies interaction with the environment. Frequently endowing the compliant manipulation skills to the robot by manual programming or off-line training is complicated and time-consuming. In this paper, we propose a sequential learning framework to take both kinematic profile and variable impedance parameter profile into consideration to model a unified control strategy with “motion generation” and “compliant control”. In order to acquire this unification controller efficiently, we use a sequential learning neural network to encode robot motion and a new force-based variable impedance learning algorithm to estimate varying damping and stiffness profiles in three directions. Furthermore, the state-independent stability constraints for variable impedance control are presented. The effectiveness of the proposed learning framework is validated by a set of experiments using the 4-DoF Barrett WAM.},
  langid = {english},
  keywords = {IMPORTANT,Physical human-robot interaction,READ,Robot learning from demonstration,Sequential learning,STABILITY,Stability analysis,Variable impedance control},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/duan_et_al_2019_sequential_learning_unification_controller_from_human_demonstrations_for.pdf}
}

@inproceedings{duchaineInvestigationHumanrobotInteraction2008,
  title = {Investigation of Human-Robot Interaction Stability Using {{Lyapunov}} Theory},
  booktitle = {2008 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Duchaine, Vincent and Gosselin, Clement M.},
  date = {2008},
  pages = {2189--2194},
  publisher = {{IEEE}},
  keywords = {Cartesian impedance,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/duchaine_gosselin_2008_investigation_of_human-robot_interaction_stability_using_lyapunov_theory.pdf}
}

@inproceedings{duchaineSafeStableIntuitive2009,
  title = {Safe, {{Stable}} and {{Intuitive Control}} for {{Physical Human-Robot Interaction}}},
  booktitle = {2009 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Duchaine, Vincent and Gosselin, Clement},
  date = {2009-05},
  pages = {3383--3388},
  issn = {1050-4729},
  doi = {10.1109/ROBOT.2009.5152664},
  abstract = {For physical human-robot interaction, safety and dependability are of utmost importance due to the potential risk a relatively powerful robot poses for human beings. From the control standpoint, it is possible to increase this level of safety by guaranteeing that the robot will never exhibit any unstable behaviour. However, stability is not the only concern in the design of a controller for such a robot. During human-robot interaction, the resulting cooperative motion should be truly intuitive and should not restrict in any way the human performance. For this purpose, we have designed a new variable admittance control law that guarantees the stability of the robot during constrained motion and also provides a very intuitive human interaction. The first characteristic is provided by the design of a stability observer while the other is based on a variable admittance control scheme that uses the force derivative as a way to predict human intention. The stability observer is based on a previous stability investigation of cooperative motion which implies the knowledge of the interaction stiffness. A method to accurately estimate this stiffness online using the data coming from the encoder and from a multi-axis force sensor at the end effector is also provided. The stability and intuitivity of the control law were verified in a user study during a cooperative drawing task with a 3 degree-of-freedom (dof) parallel robot.},
  eventtitle = {2009 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  keywords = {Admittance,Cartesian impedance,Force sensors,Human robot interaction,Humanoid robots,Impedance,Motion control,Online Stability Methods,pHRI,Robotics and automation,Safety,Service robots,Stability,STABILITY,Variable impedance},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/duchaine_gosselin_2009_safe,_stable_and_intuitive_control_for_physical_human-robot_interaction.pdf}
}

@article{duindamPortBasedAsymptoticCurve2004,
  title = {Port-{{Based Asymptotic Curve Tracking}} for {{Mechanical Systems}}},
  author = {Duindam, Vincent and Stramigioli, Stefano},
  date = {2004-01-01},
  journaltitle = {European Journal of Control},
  shortjournal = {European Journal of Control},
  volume = {10},
  number = {5},
  pages = {411--420},
  issn = {0947-3580},
  doi = {10.3166/ejc.10.411-420},
  url = {https://www.sciencedirect.com/science/article/pii/S0947358004703891},
  urldate = {2022-08-17},
  abstract = {We examine the control problem of curve-tracking for a fully actuated mechanical system. Using a coordinate transformation on the momentum variables, we split the kinetic energy of the system in a desired and an undesired part, and then design an (intrinsically passive) controller as an interconnection of port- Hamiltonian subsystems, in such a way that asymptotic convergence to the desired curve is obtained. We illustrate the performance in a simulation.},
  langid = {english},
  keywords = {Hamiltonian Control Systems,Mechanical Systems,Nonlinear Control},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/duindam_stramigioli_2004_port-based_asymptotic_curve_tracking_for_mechanical_systems.pdf}
}

@inproceedings{duttaLearningStableMovement2018,
  title = {Learning {{Stable Movement Primitives}} by {{Finding}} a {{Suitable Fuzzy Lyapunov Function}} from {{Kinesthetic Demonstrations}}},
  booktitle = {2018 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Dutta, Samrat and Kumar, Swagat and Behera, Laxmidhar},
  date = {2018-07},
  pages = {1--8},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2018.8489055},
  abstract = {Transferring skills to roUots through human demonstrations is an interesting problem. Locally generated demonstrations of reaching motion, given by a human teacher are generally encoded in a dynamical model. Stability of this encoding system demands great attention while learning the model parameters. In that context, we present a new architecture of dynamical system to learn movement primitives from multiple demonstrations exploiting a fuzzy Lyapunov function (FLF). We assume that there exists a natural Lyapunov function (LF) that associates the demonstrations. The proposed FLF tries to approximate that LF. First, the dynamics of the demonstrations are encoded in a regressive model, learnt using Gaussian mixture regression with EM algorithm. Then the FLF is searched involving the learnt dynamics in an optimization process. The FLF in turn helps to learn a fuzzy controller. Our architecture is new in a sense that it combines the probabilistic model with a fuzzy controller to create a globally asymptotically stable motion model. The proposed algorithm can simultaneously learn position and orientation profiles in a single model. The algorithm is experimentally validated on a commercially available manipulator and also compared with a state-of-the-art technique.},
  eventtitle = {2018 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  keywords = {Asymptotic stability,dynamics,Dynamics,Heuristic algorithms,Lyapunov methods,READ,Robots,Stability analysis,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/dutta_et_al_2018_learning_stable_movement_primitives_by_finding_a_suitable_fuzzy_lyapunov.pdf}
}

@article{duttaSkillLearningHuman2021,
  title = {Skill {{Learning From Human Demonstrations Using Dynamical Regressive Models}} for {{Multitask Applications}}},
  author = {Dutta, Samrat and Behera, Laxmidhar and Nahavandi, Saeid},
  date = {2021-01},
  journaltitle = {Ieee Transactions on Systems Man Cybernetics-Systems},
  shortjournal = {IEEE Trans. Syst. Man Cybern. -Syst.},
  volume = {51},
  number = {1},
  pages = {659--672},
  publisher = {{Ieee-Inst Electrical Electronics Engineers Inc}},
  location = {{Piscataway}},
  issn = {2168-2216},
  doi = {10.1109/TSMC.2018.2885481},
  url = {https://ieeexplore.ieee.org/document/8587145},
  urldate = {2022-09-11},
  abstract = {This paper is concerned with the motor skill learning from human demonstrations using the framework of dynamic regressive models (DRMs). The DRM-based motion planner is preferred as it generates the end-effector trajectory dynamically based on the current state of the end-effector. Within existing frameworks, a single DRM can learn a single motion profile. In addition, such learned DRMs from the data may not be stable. This paper addresses both these issues in a comprehensive manner. In this paper a single DRM has been used to encode human demonstrations involving multitask profiles and multiple task-equilibriums which is novel. We have introduced the idea that the learned DRM will generate human-like stable motion if the energy dissipation rate (EDR) of the generated trajectory follows that of the human demonstration. Thus, the DRM structure has been modified by adding a continuous guiding signal which can be called as the control signal. This signal has been derived using control theoretic principle to ensure asymptotic stability while maintaining the EDR equivalent to that of the human demonstration. The asymptotic stability of the learned DRM has been established by involving a nonmonotonic Lyapunov function consisting of first derivative of a quadratic function and the energy function associated with the DRM. The proposed framework can be learned using many existing regression techniques in this paper Gaussian mixture regression, locally weighted projection regression, and support vector regression techniques have been used successfully. During the pick and place tasks, human demonstrations involving multiple task profiles and multiple task-equilibriums are generated using a 7 DOF commercial robot manipulator. Experimental validations show that the DRMs learned using these three regression schemes are able to guide the robot along the multitask profiles in a stable manner.},
  langid = {english},
  keywords = {behaviors,Dynamical systems,imitation,learning by demonstration,Lyapunov function,READ,regressive models,skill transfer},
  annotation = {WOS:000607806700050},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/dutta_et_al_2021_skill_learning_from_human_demonstrations_using_dynamical_regressive_models_for.pdf}
}

@inproceedings{ebenbauerDissipationInequalitiesSystems2009,
  title = {Dissipation Inequalities in Systems Theory: {{An}} Introduction and Recent Results},
  shorttitle = {Dissipation Inequalities in Systems Theory},
  booktitle = {Invited Lectures of the International Congress on Industrial and Applied Mathematics},
  author = {Ebenbauer, Christian and Raff, Tobias and Allgöwer, Frank},
  date = {2009},
  volume = {2007},
  pages = {23--42},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ebenbauer_et_al_2009_dissipation_inequalities_in_systems_theory.pdf}
}

@unpublished{ebertRobustnessRetryingClosedLoop2018,
  title = {Robustness via {{Retrying}}: {{Closed-Loop Robotic Manipulation}} with {{Self-Supervised Learning}}},
  shorttitle = {Robustness via {{Retrying}}},
  author = {Ebert, Frederik and Dasari, Sudeep and Lee, Alex X. and Levine, Sergey and Finn, Chelsea},
  date = {2018-10-06},
  eprint = {1810.03043},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1810.03043},
  urldate = {2019-05-09},
  abstract = {Prediction is an appealing objective for self-supervised learning of behavioral skills, particularly for autonomous robots. However, effectively utilizing predictive models for control, especially with raw image inputs, poses a number of major challenges. How should the predictions be used? What happens when they are inaccurate? In this paper, we tackle these questions by proposing a method for learning robotic skills from raw image observations, using only autonomously collected experience. We show that even an imperfect model can complete complex tasks if it can continuously retry, but this requires the model to not lose track of the objective (e.g., the object of interest). To enable a robot to continuously retry a task, we devise a self-supervised algorithm for learning image registration, which can keep track of objects of interest for the duration of the trial. We demonstrate that this idea can be combined with a video-prediction based controller to enable complex behaviors to be learned from scratch using only raw visual inputs, including grasping, repositioning objects, and non-prehensile manipulation. Our real-world experiments demonstrate that a model trained with 160 robot hours of autonomously collected, unlabeled data is able to successfully perform complex manipulation tasks with a wide range of objects not seen during training.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics,object manipulation,robotic grasping,tno internship},
  annotation = {00004}
}

@article{elguea-aguinacoGoalConditionedReinforcementLearning2022,
  title = {Goal-{{Conditioned Reinforcement Learning}} within a {{Human-Robot Disassembly Environment}}},
  author = {Elguea-Aguinaco, Íñigo and Serrano-Muñoz, Antonio and Chrysostomou, Dimitrios and Inziarte-Hidalgo, Ibai and Bøgh, Simon and Arana-Arexolaleiba, Nestor},
  date = {2022-01},
  journaltitle = {Applied Sciences},
  volume = {12},
  number = {22},
  pages = {11610},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2076-3417},
  doi = {10.3390/app122211610},
  url = {https://www.mdpi.com/2076-3417/12/22/11610},
  urldate = {2023-05-04},
  abstract = {The introduction of collaborative robots in industrial environments reinforces the need to provide these robots with better cognition to accomplish their tasks while fostering worker safety without entering into safety shutdowns that reduce workflow and production times. This paper presents a novel strategy that combines the execution of contact-rich tasks, namely disassembly, with real-time collision avoidance through machine learning for safe human-robot interaction. Specifically, a goal-conditioned reinforcement learning approach is proposed, in which the removal direction of a peg, of varying friction, tolerance, and orientation, is subject to the location of a human collaborator with respect to a 7-degree-of-freedom manipulator at each time step. For this purpose, the suitability of three state-of-the-art actor-critic algorithms is evaluated, and results from simulation and real-world experiments are presented. In reality, the policy’s deployment is achieved through a new scalable multi-control framework that allows a direct transfer of the control policy to the robot and reduces response times. The results show the effectiveness, generalization, and transferability of the proposed approach with two collaborative robots against static and dynamic obstacles, leveraging the set of available solutions in non-monotonic tasks to avoid a potential collision with the human worker.},
  issue = {22},
  langid = {english},
  keywords = {collaborative robots,collision avoidance,contact-rich tasks,disassembly,machine learning,READ,reinforcement learning},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/elguea-aguinaco_et_al_2022_goal-conditioned_reinforcement_learning_within_a_human-robot_disassembly.pdf}
}

@article{elguea-aguinacoReviewReinforcementLearning2023,
  title = {A Review on Reinforcement Learning for Contact-Rich Robotic Manipulation Tasks},
  author = {Elguea-Aguinaco, Íñigo and Serrano-Muñoz, Antonio and Chrysostomou, Dimitrios and Inziarte-Hidalgo, Ibai and Bøgh, Simon and Arana-Arexolaleiba, Nestor},
  date = {2023-06-01},
  journaltitle = {Robotics and Computer-Integrated Manufacturing},
  shortjournal = {Robotics and Computer-Integrated Manufacturing},
  volume = {81},
  pages = {102517},
  issn = {0736-5845},
  doi = {10.1016/j.rcim.2022.102517},
  url = {https://www.sciencedirect.com/science/article/pii/S0736584522001995},
  urldate = {2023-05-02},
  abstract = {Research and application of reinforcement learning in robotics for contact-rich manipulation tasks have exploded in recent years. Its ability to cope with unstructured environments and accomplish hard-to-engineer behaviors has led reinforcement learning agents to be increasingly applied in real-life scenarios. However, there is still a long way ahead for reinforcement learning to become a core element in industrial applications. This paper examines the landscape of reinforcement learning and reviews advances in its application in contact-rich tasks from 2017 to the present. The analysis investigates the main research for the most commonly selected tasks for testing reinforcement learning algorithms in both rigid and deformable object manipulation. Additionally, the trends around reinforcement learning associated with serial manipulators are explored as well as the various technological challenges that this machine learning control technique currently presents. Lastly, based on the state-of-the-art and the commonalities among the studies, a framework relating the main concepts of reinforcement learning in contact-rich manipulation tasks is proposed. The final goal of this review is to support the robotics community in future development of systems commanded by reinforcement learning, discuss the main challenges of this technology and suggest future research directions in the domain.},
  langid = {english},
  keywords = {Contact-rich manipulation,Deformable object manipulation,Industrial manipulators,READ,Reinforcement learning,REVIEW,Rigid object manipulation},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/elguea-aguinaco_et_al_2023_a_review_on_reinforcement_learning_for_contact-rich_robotic_manipulation_tasks.pdf}
}

@article{elowitzSyntheticOscillatoryNetwork2000,
  title = {A Synthetic Oscillatory Network of Transcriptional Regulators},
  author = {Elowitz, Michael B. and Leibler, Stanislas},
  date = {2000-01},
  journaltitle = {Nature},
  volume = {403},
  number = {6767},
  pages = {335--338},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/35002125},
  url = {http://www.nature.com/articles/35002125},
  urldate = {2020-08-17},
  abstract = {Networks of interacting biomolecules carry out many essential functions in living cells1, but the ‘design principles’ underlying the functioning of such intracellular networks remain poorly understood, despite intensive efforts including quantitative analysis of relatively simple systems2. Here we present a complementary approach to this problem: the design and construction of a synthetic network to implement a particular function. We used three transcriptional repressor systems that are not part of any natural biological clock3,4,5 to build an oscillating network, termed the repressilator, in Escherichia coli. The network periodically induces the synthesis of green fluorescent protein as a readout of its state in individual cells. The resulting oscillations, with typical periods of hours, are slower than the cell-division cycle, so the state of the oscillator has to be transmitted from generation to generation. This artificial clock displays noisy behaviour, possibly because of stochastic fluctuations of its components. Such ‘rational network design’ may lead both to the engineering of new cellular behaviours and to an improved understanding of naturally occurring networks.},
  issue = {6767},
  langid = {english},
  keywords = {#nosource,protein synthesis,transcriptional regulators},
  annotation = {00000},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/Elowitz_Leibler_2000_A synthetic oscillatory network of transcriptional regulators.pdf}
}

@article{enayatiVariableImpedanceForceControl2020,
  title = {Variable-{{Impedance}} and {{Force Control}} for {{Robust Learning}} of {{Contact-rich Manipulation Tasks}} from {{User Demonstration}}},
  author = {Enayati, Nima and Mariani, Stefano and Wahrburg, Arne and Zanchettin, Andrea M.},
  date = {2020},
  journaltitle = {IFAC-PapersOnLine},
  volume = {53},
  number = {2},
  pages = {9834--9840},
  publisher = {{Elsevier}},
  keywords = {Gaussian mixture model,gaussian mixture regression,GMR,READ,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/enayati_et_al_2020_variable-impedance_and_force_control_for_robust_learning_of_contact-rich.pdf}
}

@inproceedings{engelReinforcementLearningGaussian2005,
  title = {Reinforcement Learning with {{Gaussian}} Processes},
  booktitle = {Proceedings of the 22nd International Conference on {{Machine}} Learning  - {{ICML}} '05},
  author = {Engel, Yaakov and Mannor, Shie and Meir, Ron},
  date = {2005},
  pages = {201--208},
  publisher = {{ACM Press}},
  location = {{Bonn, Germany}},
  doi = {10.1145/1102351.1102377},
  url = {http://portal.acm.org/citation.cfm?doid=1102351.1102377},
  urldate = {2021-04-18},
  abstract = {Gaussian Process Temporal Difference (GPTD) learning offers a Bayesian solution to the policy evaluation problem of reinforcement learning. In this paper we extend the GPTD framework by addressing two pressing issues, which were not adequately treated in the original GPTD paper (Engel et al., 2003). The first is the issue of stochasticity in the state transitions, and the second is concerned with action selection and policy improvement. We present a new generative model for the value function, deduced from its relation with the discounted return. We derive a corresponding on-line algorithm for learning the posterior moments of the value Gaussian process. We also present a SARSA based extension of GPTD, termed GPSARSA, that allows the selection of actions and the gradual improvement of policies without requiring a world-model.},
  eventtitle = {The 22nd International Conference},
  isbn = {978-1-59593-180-1},
  langid = {english},
  keywords = {#nosource}
}

@article{erdenAssistingManualWelding2011,
  title = {Assisting Manual Welding with Robot},
  author = {Erden, Mustafa Suphi and Marić, Bobby},
  date = {2011-08-01},
  journaltitle = {Robotics and Computer-Integrated Manufacturing},
  shortjournal = {Robotics and Computer-Integrated Manufacturing},
  series = {Conference Papers of {{Flexible Automation}} and {{Intelligent Manufacturing}}},
  volume = {27},
  number = {4},
  pages = {818--828},
  issn = {0736-5845},
  doi = {10.1016/j.rcim.2011.01.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0736584511000172},
  urldate = {2022-05-24},
  abstract = {This paper presents a first attempt to assist manual welding with a physically interactive robot. An interactive control scheme is developed to suppress the vibrations of torch during the welding of novice welders. The torch is attached to the end-effector of a haptic-robot. Human and robot act together on the welding torch: the human controls the direction and speed; the robot suppresses the sudden and abrupt motions. The control scheme is developed by experimenting with an air-paint-brush. The painting process emulates the actual welding. Such an emulating environment is useful to surmount the difficulties of experimentation with actual welding. The impedance parameters of the control scheme are investigated. A damping value is determined for an effective vibration suppression and minimum human effort. A variable impedance control scheme is applied to ease the manipulation of the torch while not welding. The results of real welding of novice welders with and without robot assistance are presented. There is a considerable improvement in the performance of the welders when they are assisted with the robot.},
  langid = {english},
  keywords = {Assistance,Human–robot interaction,Robot,Skill,Welding},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/erden_marić_2011_assisting_manual_welding_with_robot.pdf}
}

@article{erdenRoboticAssistanceImpedance2016,
  title = {Robotic {{Assistance}} by {{Impedance Compensation}} for {{Hand Movements While Manual Welding}}},
  author = {Erden, Mustafa Suphi and Billard, Aude},
  date = {2016-11},
  journaltitle = {IEEE Transactions on Cybernetics},
  volume = {46},
  number = {11},
  pages = {2459--2472},
  issn = {2168-2275},
  doi = {10.1109/TCYB.2015.2478656},
  abstract = {In this paper, we present a robotic assistance scheme which allows for impedance compensation with stiffness, damping, and mass parameters for hand manipulation tasks and we apply it to manual welding. The impedance compensation does not assume a preprogrammed hand trajectory. Rather, the intention of the human for the hand movement is estimated in real time using a smooth Kalman filter. The movement is restricted by compensatory virtual impedance in the directions perpendicular to the estimated direction of movement. With airbrush painting experiments, we test three sets of values for the impedance parameters as inspired from impedance measurements with manual welding. We apply the best of the tested sets for assistance in manual welding and perform welding experiments with professional and novice welders. We contrast three conditions: 1) welding with the robot's assistance; 2) with the robot when the robot is passive; and 3) welding without the robot. We demonstrate the effectiveness of the assistance through quantitative measures of both task performance and perceived user's satisfaction. The performance of both the novice and professional welders improves significantly with robotic assistance compared to welding with a passive robot. The assessment of user satisfaction shows that all novice and most professional welders appreciate the robotic assistance as it suppresses the tremors in the directions perpendicular to the movement for welding.},
  eventtitle = {{{IEEE Transactions}} on {{Cybernetics}}},
  keywords = {Assistive technology,Damping,Impedance,Impedance measurement,Kalman filters,man machine systems,manual welding,Manuals,physical human-robot interaction,robotic assistance,Robots,Welding},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/erden_billard_2016_robotic_assistance_by_impedance_compensation_for_hand_movements_while_manual.pdf}
}

@unpublished{espeholtIMPALAScalableDistributed2018,
  title = {{{IMPALA}}: {{Scalable Distributed Deep-RL}} with {{Importance Weighted Actor-Learner Architectures}}},
  shorttitle = {{{IMPALA}}},
  author = {Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and Legg, Shane and Kavukcuoglu, Koray},
  date = {2018-06-28},
  eprint = {1802.01561},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1802.01561},
  urldate = {2020-07-02},
  abstract = {In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,reinforcement learning,scaling RL}
}

@unpublished{eysenbachDiversityAllYou2018,
  title = {Diversity Is {{All You Need}}: {{Learning Skills}} without a {{Reward Function}}},
  shorttitle = {Diversity Is {{All You Need}}},
  author = {Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
  date = {2018-10-09},
  eprint = {1802.06070},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1802.06070},
  urldate = {2020-07-02},
  abstract = {Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose DIAYN ('Diversity is All You Need'), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Robotics,reinforcement learning,unsupervised RL},
  annotation = {00168}
}

@unpublished{eysenbachLeaveNoTrace2017,
  title = {Leave No {{Trace}}: {{Learning}} to {{Reset}} for {{Safe}} and {{Autonomous Reinforcement Learning}}},
  shorttitle = {Leave No {{Trace}}},
  author = {Eysenbach, Benjamin and Gu, Shixiang and Ibarz, Julian and Levine, Sergey},
  date = {2017-11-17},
  eprint = {1711.06782},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1711.06782},
  urldate = {2020-07-02},
  abstract = {Deep reinforcement learning algorithms can learn complex behavioral skills, but real-world application of these methods requires a large amount of experience to be collected by the agent. In practical settings, such as robotics, this involves repeatedly attempting a task, resetting the environment between each attempt. However, not all tasks are easily or automatically reversible. In practice, this learning process requires extensive human intervention. In this work, we propose an autonomous method for safe and efficient reinforcement learning that simultaneously learns a forward and reset policy, with the reset policy resetting the environment for a subsequent attempt. By learning a value function for the reset policy, we can automatically determine when the forward policy is about to enter a non-reversible state, providing for uncertainty-aware safety aborts. Our experiments illustrate that proper use of the reset policy can greatly reduce the number of manual resets required to learn a task, can reduce the number of unsafe actions that lead to non-reversible states, and can automatically induce a curriculum.},
  keywords = {#nosource,Computer Science - Machine Learning,Computer Science - Robotics,reinforcement learning,safety}
}

@inproceedings{fangerGaussianProcessesDynamic2016,
  title = {Gaussian Processes for Dynamic Movement Primitives with Application in Knowledge-Based Cooperation},
  booktitle = {2016 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Fanger, Yunis and Umlauft, Jonas and Hirche, Sandra},
  date = {2016-10},
  pages = {3913--3919},
  publisher = {{IEEE}},
  location = {{Daejeon, South Korea}},
  doi = {10.1109/IROS.2016.7759576},
  url = {http://ieeexplore.ieee.org/document/7759576/},
  urldate = {2022-09-12},
  abstract = {Dynamic Movement Primitives (DMPs) represent stable goal-directed or periodic movements, which are learned from observations or demonstrations. They rely on proper function approximators, which are sufficiently flexible to represent arbitrary movements but also ensure goal convergence in pointto-point motions. This work shows that Gaussian Processes (GPs) are suitable as a regressor for learning movements with DMPs ensuring stability. In addition, GPs provide a measure for the uncertainty about the current movement, which we exploit by proposing a new cooperation scheme for DMPs: For better reproduction of demonstrations, we follow the intuition, that individuals with more knowledge lead towards the goal, while others follow and focus on cooperation. Along with simulation results, we validate the presented methods in a robotic cooperative object manipulation task.},
  eventtitle = {2016 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  isbn = {978-1-5090-3762-9},
  langid = {english},
  keywords = {Convergence,Gaussian processes,READ,Robot kinematics,Stability analysis,Training data,Uncertainty},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/fanger_et_al_2016_gaussian_processes_for_dynamic_movement_primitives_with_application_in.pdf}
}

@online{fanLearningStableKoopman2021,
  title = {Learning {{Stable Koopman Embeddings}}},
  author = {Fan, Fletcher and Yi, Bowen and Rye, David and Shi, Guodong and Manchester, Ian R.},
  date = {2021-10-13},
  eprint = {2110.06509},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2110.06509},
  urldate = {2022-12-02},
  abstract = {In this paper, we present a new data-driven method for learning stable models of nonlinear systems. Our model lifts the original state space to a higher-dimensional linear manifold using Koopman embeddings. Interestingly, we prove that every discrete-time nonlinear contracting model can be learnt in our framework. Another significant merit of the proposed approach is that it allows for unconstrained optimization over the Koopman embedding and operator jointly while enforcing stability of the model, via a direct parameterization of stable linear systems, greatly simplifying the computations involved. We validate our method on a simulated system and analyze the advantages of our parameterization compared to alternatives.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/fan_et_al_2021_learning_stable_koopman_embeddings.pdf}
}

@article{farajiparvarBriefSurveyTelerobotic2020,
  title = {A {{Brief Survey}} of {{Telerobotic Time Delay Mitigation}}},
  author = {Farajiparvar, Parinaz and Ying, Hao and Pandya, Abhilash},
  date = {2020},
  journaltitle = {Frontiers in Robotics and AI},
  volume = {7},
  issn = {2296-9144},
  url = {https://www.frontiersin.org/article/10.3389/frobt.2020.578805},
  urldate = {2022-06-23},
  abstract = {There is a substantial number of telerobotics and teleoperation applications ranging from space operations, ground/aerial robotics, drive-by-wire systems to medical interventions. Major obstacles for such applications include latency, channel corruptions, and bandwidth which limit teleoperation efficacy. This survey reviews the time delay problem in teleoperation systems. We briefly review different solutions from early approaches which consist of control-theory-based models and user interface designs and focus on newer approaches developed since 2014. Future solutions to the time delay problem will likely be hybrid solutions which include modeling of user intent, prediction of robot movements, and time delay prediction all potentially using time series prediction methods. Hence, we examine methods that are primarily based on time series prediction. Recent prediction approaches take advantage of advances in nonlinear statistical models as well as machine learning and neural network techniques. We review Recurrent Neural Networks, Long Short-Term Memory, Sequence to Sequence, and Generative Adversarial Network models and examine each of these approaches for addressing time delay. As time delay is still an unsolved problem, we suggest some possible future research directions from information-theory-based modeling, which may lead to promising new approaches to advancing the field.},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/farajiparvar_et_al_2020_a_brief_survey_of_telerobotic_time_delay_mitigation.pdf}
}

@unpublished{feinbergModelBasedValueEstimation2018,
  title = {Model-{{Based Value Estimation}} for {{Efficient Model-Free Reinforcement Learning}}},
  author = {Feinberg, Vladimir and Wan, Alvin and Stoica, Ion and Jordan, Michael I. and Gonzalez, Joseph E. and Levine, Sergey},
  date = {2018-02-28},
  eprint = {1803.00101},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1803.00101},
  urldate = {2020-07-02},
  abstract = {Recent model-free reinforcement learning algorithms have proposed incorporating learned dynamics models as a source of additional data with the intention of reducing sample complexity. Such methods hold the promise of incorporating imagined data coupled with a notion of model uncertainty to accelerate the learning of continuous control tasks. Unfortunately, they rely on heuristics that limit usage of the dynamics model. We present model-based value expansion, which controls for uncertainty in the model by only allowing imagination to fixed depth. By enabling wider use of learned dynamics models within a model-free reinforcement learning algorithm, we improve value estimation, which, in turn, reduces the sample complexity of learning.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,learned model,model-based,reinforcement learning,Statistics - Machine Learning}
}

@inproceedings{fengDeepmovePredictingHuman2018,
  title = {Deepmove: {{Predicting}} Human Mobility with Attentional Recurrent Networks},
  shorttitle = {Deepmove},
  booktitle = {Proceedings of the 2018 {{World Wide Web Conference}} on {{World Wide Web}}},
  author = {Feng, Jie and Li, Yong and Zhang, Chao and Sun, Funing and Meng, Fanchao and Guo, Ang and Jin, Depeng},
  date = {2018},
  pages = {1459--1468},
  publisher = {{International World Wide Web Conferences Steering Committee}},
  keywords = {#nosource,path planning,robotic grasping,tno internship},
  annotation = {00016}
}

@inproceedings{fengEffectiveTrainingStrategy2022,
  title = {An {{Effective Training Strategy}} for {{Upper-limb Rehabilitation Robots Based}} on {{Visual-haptic Feedback Using Potential Field}}},
  booktitle = {2022 12th {{International Conference}} on {{CYBER Technology}} in {{Automation}}, {{Control}}, and {{Intelligent Systems}} ({{CYBER}})},
  author = {Feng, Guang and Zhang, Jiaji and Chai, Guohong and Li, Maoqin and Zuo, Guokun and Yang, Lei},
  date = {2022-07},
  pages = {678--681},
  issn = {2642-6633},
  doi = {10.1109/CYBER55403.2022.9907062},
  abstract = {Visual and haptic feedback are crucial to enhance the effectiveness of robot-assisted rehabilitation. To improve the performance of clinical rehabilitation training for patients with motor dysfunction, we propose an effective training strategy based on visual and haptic feedback. Haptic feedback is generated by a designed artificial potential field, which allows patients to perceive the correct training direction. The effectiveness of the proposed training strategy is initially verified, by recruiting three healthy subjects to perform circle drawing tasks on an upper limb rehabilitation robot. Experimental results showed that higher training accuracies were obtained using visual-haptic feedback compared to those with unimodal feedback. The proposed strategy can enhance the users' perception of the training process and corrects the incorrect movements in real-time, simultaneously. The current training strategy can be applied to commercial rehabilitation robots and meet the rehabilitation training needs of the users with impaired vision or the vision is unavailable.},
  eventtitle = {2022 12th {{International Conference}} on {{CYBER Technology}} in {{Automation}}, {{Control}}, and {{Intelligent Systems}} ({{CYBER}})},
  keywords = {Assistive robots,Automation,Control systems,Haptic interfaces,READ,Real-time systems,Training,Visualization},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/feng_et_al_2022_an_effective_training_strategy_for_upper-limb_rehabilitation_robots_based_on.pdf}
}

@unpublished{fernandoPathNetEvolutionChannels2017,
  title = {{{PathNet}}: {{Evolution Channels Gradient Descent}} in {{Super Neural Networks}}},
  shorttitle = {{{PathNet}}},
  author = {Fernando, Chrisantha and Banarse, Dylan and Blundell, Charles and Zwols, Yori and Ha, David and Rusu, Andrei A. and Pritzel, Alexander and Wierstra, Daan},
  date = {2017-01-30},
  eprint = {1701.08734},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1701.08734},
  urldate = {2020-07-02},
  abstract = {For artificial general intelligence (AGI) it would be efficient if multiple users trained the same giant neural network, permitting parameter reuse, without catastrophic forgetting. PathNet is a first step in this direction. It is a neural network algorithm that uses agents embedded in the neural network whose task is to discover which parts of the network to re-use for new tasks. Agents are pathways (views) through the network which determine the subset of parameters that are used and updated by the forwards and backwards passes of the backpropogation algorithm. During learning, a tournament selection genetic algorithm is used to select pathways through the neural network for replication and mutation. Pathway fitness is the performance of that pathway measured according to a cost function. We demonstrate successful transfer learning; fixing the parameters along a path learned on task A and re-evolving a new population of paths for task B, allows task B to be learned faster than it could be learned from scratch or after fine-tuning. Paths evolved on task B re-use parts of the optimal path evolved on task A. Positive transfer was demonstrated for binary MNIST, CIFAR, and SVHN supervised learning classification tasks, and a set of Atari and Labyrinth reinforcement learning tasks, suggesting PathNets have general applicability for neural network training. Finally, PathNet also significantly improves the robustness to hyperparameter choices of a parallel asynchronous reinforcement learning algorithm (A3C).},
  keywords = {#nosource,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,multitask RL,reinforcement learning,transfer learning},
  annotation = {00293}
}

@article{ferragutiEnergyTankBasedInteractive2015,
  title = {An {{Energy Tank-Based Interactive Control Architecture}} for {{Autonomous}} and {{Teleoperated Robotic Surgery}}},
  author = {Ferraguti, Federica and Preda, Nicola and Manurung, Auralius and Bonfè, Marcello and Lambercy, Olivier and Gassert, Roger and Muradore, Riccardo and Fiorini, Paolo and Secchi, Cristian},
  date = {2015-10},
  journaltitle = {IEEE Transactions on Robotics},
  volume = {31},
  number = {5},
  pages = {1073--1088},
  issn = {1941-0468},
  doi = {10.1109/TRO.2015.2455791},
  abstract = {Introducing some form of autonomy in robotic surgery is being considered by the medical community to better exploit the potential of robots in the operating room. However, significant technological steps have to occur before even the smallest autonomous task is ready to be presented to the regulatory authorities. In this paper, we address the initial steps of this process, in particular the development of control concepts satisfying the basic safety requirements of robotic surgery, i.e., providing the robot with the necessary dexterity and a stable and smooth behavior of the surgical tool. Two specific situations are considered: the automatic adaptation to changing tissue stiffness and the transition from autonomous to teleoperated mode. These situations replicate real-life cases when the surgeon adapts the stiffness of her/his arm to penetrate tissues of different consistency and when, due to an unexpected event, the surgeon has to take over the control of the surgical robot. To address the first case, we propose a passivity-based interactive control architecture that allows us to implement stable time-varying interactive behaviors. For the second case, we present a two-layered bilateral control architecture that ensures a stable behavior during the transition between autonomy and teleoperation and, after the switch, limits the effect of initial mismatch between master and slave poses. The proposed solutions are validated in the realistic surgical scenario developed within the EU-funded I-SUR project, using a surgical robot prototype specifically designed for the autonomous execution of surgical tasks like the insertion of needles into the human body.},
  eventtitle = {{{IEEE Transactions}} on {{Robotics}}},
  keywords = {Admittance,Energy tanks,Impedance,interactive control,Medical robotics,medical robots and systems,STABILITY,Surgery,Switches,telerobotics},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ferraguti_et_al_2015_an_energy_tank-based_interactive_control_architecture_for_autonomous_and.pdf}
}

@article{ferragutiOptimizedPowerModulation2021,
  title = {Optimized {{Power Modulation}} in {{Wave-Based Bilateral Teleoperation}}},
  author = {Ferraguti, Federica and Bonfè, Marcello and Fantuzzi, Cesare and Secchi, Cristian},
  date = {2021-02},
  journaltitle = {IEEE/ASME Transactions on Mechatronics},
  volume = {26},
  number = {1},
  pages = {276--287},
  issn = {1941-014X},
  doi = {10.1109/TMECH.2020.3013978},
  abstract = {A common approach for stabilizing the delayed communication channel in a bilateral teleoperation architecture is using wave variables to make the exchange of information equivalent to a passive physical dynamics. However, such a dynamics is felt by the user and it affects the transparency of the system. In this article, we exploit the wave variables for storing the energy exchanged between master and slave, but we shape the incoming power for reproducing a desired, transparent behavior. First, we propose a passivity preserving modulation of the incoming power, and then, we achieve possibly scaled desired forces and velocities. Finally, we formulate an optimization problem for computing the best values of forces and velocities to be implemented. A validation of the proposed architecture and the comparison of its performances with respect to the standard wave-based approach are provided.},
  eventtitle = {{{IEEE}}/{{ASME Transactions}} on {{Mechatronics}}},
  keywords = {Communication channels,Computer architecture,Delays,Force,Impedance,Robot programming,robots,Robots,STABILITY,Standards,telerobotics},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ferraguti_et_al_2021_optimized_power_modulation_in_wave-based_bilateral_teleoperation.pdf}
}

@inproceedings{ferragutiTankbasedApproachImpedance2013,
  title = {A Tank-Based Approach to Impedance Control with Variable Stiffness},
  booktitle = {2013 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Ferraguti, Federica and Secchi, Cristian and Fantuzzi, Cesare},
  date = {2013-05},
  pages = {4948--4953},
  issn = {1050-4729},
  doi = {10.1109/ICRA.2013.6631284},
  abstract = {In this paper, we present a new impedance control strategy that allows to reproduce a time-varying stiffness. By properly controlling the energy exchanged during the action, we guarantee the system passivity for any choice of the stiffness matrix, especially in case of time-varying stiffness, and therefore a stable behavior of the robot both in free motion and in interaction with an environment.},
  eventtitle = {2013 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  keywords = {Cartesian impedance,Clamping,Force,Human environment interaction,Impedance,IMPORTANT,Manipulators,Needles,READ,STABILITY,Trajectory,Vectors},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ferraguti_et_al_2013_a_tank-based_approach_to_impedance_control_with_variable_stiffness.pdf}
}

@inproceedings{ferragutiTwolayerApproachShared2015,
  title = {A Two-Layer Approach for Shared Control in Semi-Autonomous Robotic Surgery},
  booktitle = {2015 {{European Control Conference}} ({{ECC}})},
  author = {Ferraguti, Federica and Preda, Nicola and De Rossi, Giacomo and Bonfè, Marcello and Muradore, Riccardo and Fiorini, Paolo and Secchi, Cristian},
  date = {2015-07},
  pages = {747--752},
  doi = {10.1109/ECC.2015.7330632},
  abstract = {In autonomous robotic surgery, the supervision of the surgeon cannot be avoided due to the unforeseenable emergencies and complications that can take place during an operation. When necessary, the surgeon has to take over the surgical system switching it from an autonomous mode to a teleoperation mode. In this paper we propose a two-layer bilateral control architecture that ensures a safe behavior during the switch and high performance during the teleoperation. Experiments are proposed for validating the architecture proposed in the paper.},
  eventtitle = {2015 {{European Control Conference}} ({{ECC}})},
  keywords = {Couplings,Force,Robots,Steady-state,Surgery,Switches},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ferraguti_et_al_2015_a_two-layer_approach_for_shared_control_in_semi-autonomous_robotic_surgery.pdf}
}

@article{ferragutiVariableAdmittanceControl2019,
  title = {A Variable Admittance Control Strategy for Stable Physical Human–Robot Interaction},
  author = {Ferraguti, Federica and Talignani Landi, Chiara and Sabattini, Lorenzo and Bonfè, Marcello and Fantuzzi, Cesare and Secchi, Cristian},
  date = {2019-05-01},
  journaltitle = {The International Journal of Robotics Research},
  shortjournal = {The International Journal of Robotics Research},
  volume = {38},
  number = {6},
  pages = {747--765},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {0278-3649},
  doi = {10.1177/0278364919840415},
  url = {https://doi.org/10.1177/0278364919840415},
  urldate = {2022-05-24},
  abstract = {Admittance control allows a desired dynamic behavior to be reproduced on a non-backdrivable manipulator and it has been widely used for interaction control and, in particular, for human–robot collaboration. Nevertheless, stability problems arise when the environment (e.g. the human) the robot is interacting with becomes too stiff. In this paper, we investigate the stability issues related to a change of stiffness of the human arm during the interaction with an admittance-controlled robot. We propose a novel method for detecting the rise of instability and a passivity-preserving strategy for restoring a stable behavior. The results of the paper are validated on two robotic setups and with 50 users performing two tasks that emulate industrial operations.},
  langid = {english},
  keywords = {admittance control,cooperative manipulators,human-in-the-loop,physical human–robot interaction},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ferraguti_et_al_2019_a_variable_admittance_control_strategy_for_stable_physical_human–robot.pdf}
}

@online{ficheraLinearizationIdentificationMultipleAttractor2022,
  title = {Linearization and {{Identification}} of {{Multiple-Attractor Dynamical Systems}} through {{Laplacian Eigenmaps}}},
  author = {Fichera, Bernardo and Billard, Aude},
  date = {2022-11-22},
  eprint = {2202.09171},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2202.09171},
  urldate = {2022-12-07},
  abstract = {Dynamical Systems (DS) are fundamental to the modeling and understanding time evolving phenomena, and have application in physics, biology and control. As determining an analytical description of the dynamics is often difficult, data-driven approaches are preferred for identifying and controlling nonlinear DS with multiple equilibrium points. Identification of such DS has been treated largely as a supervised learning problem. Instead, we focus on an unsupervised learning scenario where we know neither the number nor the type of dynamics. We propose a Graph-based spectral clustering method that takes advantage of a velocity-augmented kernel to connect data points belonging to the same dynamics, while preserving the natural temporal evolution. We study the eigenvectors and eigenvalues of the Graph Laplacian and show that they form a set of orthogonal embedding spaces, one for each sub-dynamics. We prove that there always exist a set of 2-dimensional embedding spaces in which the sub-dynamics are linear and n-dimensional embedding spaces where they are quasi-linear. We compare the clustering performance of our algorithm to Kernel K-Means, Spectral Clustering and Gaussian Mixtures and show that, even when these algorithms are provided with the correct number of sub-dynamics, they fail to cluster them correctly. We learn a diffeomorphism from the Laplacian embedding space to the original space and show that the Laplacian embedding leads to good reconstruction accuracy and a faster training time through an exponential decaying loss compared to the state-of-the-art diffeomorphism-based approaches.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/fichera_billard_2022_linearization_and_identification_of_multiple-attractor_dynamical_systems.pdf}
}

@article{ficucielloVariableImpedanceControl2015,
  title = {Variable {{Impedance Control}} of {{Redundant Manipulators}} for {{Intuitive Human}}–{{Robot Physical Interaction}}},
  author = {Ficuciello, Fanny and Villani, Luigi and Siciliano, Bruno},
  date = {2015-08},
  journaltitle = {IEEE Transactions on Robotics},
  volume = {31},
  number = {4},
  pages = {850--863},
  issn = {1941-0468},
  doi = {10.1109/TRO.2015.2430053},
  abstract = {This paper presents an experimental study on human-robot comanipulation in the presence of kinematic redundancy. The objective of the work is to enhance the performance during human-robot physical interaction by combining Cartesian impedance modulation and redundancy resolution. Cartesian impedance control is employed to achieve a compliant behavior of the robot's end effector in response to forces exerted by the human operator. Different impedance modulation strategies, which take into account the human's behavior during the interaction, are selected with the support of a simulation study and then experimentally tested on a 7-degree-of-freedom KUKA LWR4. A comparative study to establish the most effective redundancy resolution strategy has been made by evaluating different solutions compatible with the considered task. The experiments have shown that the redundancy, when used to ensure a decoupled apparent inertia at the end effector, allows enlarging the stability region in the impedance parameters space and improving the performance. On the other hand, the variable impedance with a suitable modulation strategy for parameters' tuning outperforms the constant impedance, in the sense that it enhances the comfort perceived by humans during manual guidance and allows reaching a favorable compromise between accuracy and execution time.},
  eventtitle = {{{IEEE Transactions}} on {{Robotics}}},
  keywords = {Cartesian impedance,co-manipulation,Dynamics,End effectors,Force control,Impedance,Joints,physical human–robot interaction,Redundancy,redundant robots,STABILITY,Stability analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ficuciello_et_al_2015_variable_impedance_control_of_redundant_manipulators_for_intuitive_human–robot.pdf}
}

@article{fieldLearningTrajectoriesRobot2016,
  title = {Learning {{Trajectories}} for {{Robot Programing}} by {{Demonstration Using}} a {{Coordinated Mixture}} of {{Factor Analyzers}}},
  author = {Field, Matthew and Stirling, David and Pan, Zengxi and Naghdy, Fazel},
  date = {2016-03},
  journaltitle = {IEEE Transactions on Cybernetics},
  volume = {46},
  number = {3},
  pages = {706--717},
  issn = {2168-2275},
  doi = {10.1109/TCYB.2015.2414277},
  abstract = {This paper presents an approach for learning robust models of humanoid robot trajectories from demonstration. In this formulation, a model of the joint space trajectory is represented as a sequence of motion primitives where a nonlinear dynamical system is learned by constructing a hidden Markov model (HMM) predicting the probability of residing in each motion primitive. With a coordinated mixture of factor analyzers as the emission probability density of the HMM, we are able to synthesize motion from a dynamic system acting along a manifold shared by both demonstrator and robot. This provides significant advantages in model complexity for kinematically redundant robots and can reduce the number of corresponding observations required for further learning. A stability analysis shows that the system is robust to deviations from the expected trajectory as well as transitional motion between manifolds. This approach is demonstrated experimentally by recording human motion with inertial sensors, learning a motion primitive model and correspondence map between the human and robot, and synthesizing motion from the manifold to control a 19 degree-of-freedom humanoid robot.},
  eventtitle = {{{IEEE Transactions}} on {{Cybernetics}}},
  keywords = {Hidden Markov models,Human motion,humanoid robot,imitation,inertial sensors,Manifolds,nonlinear dynamical system control,Nonlinear dynamical systems,READ,Robot kinematics,robot programing by demonstration (PbD),Robot sensing systems,stability,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/field_et_al_2016_learning_trajectories_for_robot_programing_by_demonstration_using_a_coordinated.pdf}
}

@inproceedings{figueroafernandezModelingCompositionsImpedancebased2018,
  title = {Modeling {{Compositions}} of {{Impedance-based Primitives}} via {{Dynamical Systems}}.},
  booktitle = {Proceedings of the {{Workshop}} on {{Cognitive Whole-Body Control}} for {{Compliant Robot Manipulation}} ({{COWB-COMP}})},
  author = {Figueroa Fernandez, Nadia Barbara and Billard, Aude},
  date = {2018},
  issue = {CONF},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/figueroa_fernandez_billard_2018_modeling_compositions_of_impedance-based_primitives_via_dynamical_systems.pdf}
}

@report{figueroafernandezPhysicallyconsistentBayesianNonparametric2018,
  title = {A Physically-Consistent Bayesian Non-Parametric Mixture Model for Dynamical System Learning},
  author = {Figueroa Fernandez, Nadia Barbara and Billard, Aude},
  date = {2018},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/figueroa_fernandez_billard_2018_a_physically-consistent_bayesian_non-parametric_mixture_model_for_dynamical.pdf}
}

@article{figueroaLocallyActiveGlobally2022,
  title = {Locally Active Globally Stable Dynamical Systems: {{Theory}}, Learning, and Experiments},
  shorttitle = {Locally Active Globally Stable Dynamical Systems},
  author = {Figueroa, Nadia and Billard, Aude},
  date = {2022-03-01},
  journaltitle = {The International Journal of Robotics Research},
  volume = {41},
  number = {3},
  pages = {312--347},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {0278-3649},
  doi = {10.1177/02783649211030952},
  url = {https://doi.org/10.1177/02783649211030952},
  urldate = {2022-12-02},
  abstract = {State-dependent dynamical systems (DSs) offer adaptivity, reactivity, and robustness to perturbations in motion planning and physical human?robot interaction tasks. Learning DS-based motion plans from non-linear reference trajectories is an active research area in robotics. Most approaches focus on learning DSs that can (i) accurately mimic the demonstrated motion, while (ii) ensuring convergence to the target, i.e., they are globally asymptotically (or exponentially) stable. When subject to perturbations, a compliant robot guided with a DS will continue following the next integral curves of the DS towards the target. If the task requires the robot to track a specific reference trajectory, this approach will fail. To alleviate this shortcoming, we propose the locally active globally stable DS (LAGS-DS), a novel DS formulation that provides both global convergence and stiffness-like symmetric attraction behaviors around a reference trajectory in regions of the state space where trajectory tracking is important. This allows for a unified approach towards motion and impedance encoding in a single DS-based motion model, i.e., stiffness is embedded in the DS. To learn LAGS-DS from demonstrations we propose a learning strategy based on Bayesian non-parametric Gaussian mixture models, Gaussian processes, and a sequence of constrained optimization problems that ensure estimation of stable DS parameters via Lyapunov theory. We experimentally validated LAGS-DS on writing tasks with a KUKA LWR 4+ arm and on navigation and co-manipulation tasks with iCub humanoid robots.},
  langid = {english},
  keywords = {IMPORTANT,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/figueroa_billard_2022_locally_active_globally_stable_dynamical_systems.pdf}
}

@unpublished{finnGuidedCostLearning2016,
  title = {Guided {{Cost Learning}}: {{Deep Inverse Optimal Control}} via {{Policy Optimization}}},
  shorttitle = {Guided {{Cost Learning}}},
  author = {Finn, Chelsea and Levine, Sergey and Abbeel, Pieter},
  date = {2016-05-27},
  eprint = {1603.00448},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1603.00448},
  urldate = {2020-07-02},
  abstract = {Reinforcement learning can acquire complex behaviors from high-level specifications. However, defining a cost function that can be optimized effectively and encodes the correct task is challenging in practice. We explore how inverse optimal control (IOC) can be used to learn behaviors from demonstrations, with applications to torque control of high-dimensional robotic systems. Our method addresses two key challenges in inverse optimal control: first, the need for informative features and effective regularization to impose structure on the cost, and second, the difficulty of learning the cost function under unknown dynamics for high-dimensional continuous systems. To address the former challenge, we present an algorithm capable of learning arbitrary nonlinear cost functions, such as neural networks, without meticulous feature engineering. To address the latter challenge, we formulate an efficient sample-based approximation for MaxEnt IOC. We evaluate our method on a series of simulated tasks and real-world robotic manipulation problems, demonstrating substantial improvement over prior methods both in terms of task complexity and sample efficiency.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,imitation learning,inverse RL learning,reinforcement learning},
  annotation = {00385}
}

@unpublished{finnModelAgnosticMetaLearningFast2017,
  title = {Model-{{Agnostic Meta-Learning}} for {{Fast Adaptation}} of {{Deep Networks}}},
  author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  date = {2017-07-18},
  eprint = {1703.03400},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1703.03400},
  urldate = {2020-07-02},
  abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,meta-RL,reinforcement learning}
}

@article{florchingerLyapunovlikeTechniquesStochastic1995,
  title = {Lyapunov-like Techniques for Stochastic Stability},
  author = {Florchinger, Patrick},
  date = {1995},
  journaltitle = {SIAM Journal on Control and optimization},
  volume = {33},
  number = {4},
  pages = {1151--1169},
  publisher = {{SIAM}},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/florchinger_1995_lyapunov-like_techniques_for_stochastic_stability.pdf}
}

@unpublished{florenceDenseObjectNets2018,
  title = {Dense {{Object Nets}}: {{Learning Dense Visual Object Descriptors By}} and {{For Robotic Manipulation}}},
  shorttitle = {Dense {{Object Nets}}},
  author = {Florence, Peter R. and Manuelli, Lucas and Tedrake, Russ},
  date = {2018-06-22},
  eprint = {1806.08756},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1806.08756},
  urldate = {2019-05-09},
  abstract = {What is the right object representation for manipulation? We would like robots to visually perceive scenes and learn an understanding of the objects in them that (i) is task-agnostic and can be used as a building block for a variety of manipulation tasks, (ii) is generally applicable to both rigid and non-rigid objects, (iii) takes advantage of the strong priors provided by 3D vision, and (iv) is entirely learned from self-supervision. This is hard to achieve with previous methods: much recent work in grasping does not extend to grasping specific objects or other tasks, whereas task-specific learning may require many trials to generalize well across object configurations or other tasks. In this paper we present Dense Object Nets, which build on recent developments in self-supervised dense descriptor learning, as a consistent object representation for visual understanding and manipulation. We demonstrate they can be trained quickly (approximately 20 minutes) for a wide variety of previously unseen and potentially non-rigid objects. We additionally present novel contributions to enable multi-object descriptor learning, and show that by modifying our training procedure, we can either acquire descriptors which generalize across classes of objects, or descriptors that are distinct for each object instance. Finally, we demonstrate the novel application of learned dense descriptors to robotic manipulation. We demonstrate grasping of specific points on an object across potentially deformed object configurations, and demonstrate using class general descriptors to transfer specific grasps across objects in a class.},
  keywords = {#nosource,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,object manipulation,robotic grasping,tno internship},
  annotation = {00005}
}

@inproceedings{fontanelliComparisonAssistiveMethods2018,
  title = {A {{Comparison}} of {{Assistive Methods}} for {{Suturing}} in {{MIRS}}},
  booktitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Fontanelli, Giuseppe Andrea and Yang, Guang-Zhong and Siciliano, Bruno},
  date = {2018-10},
  pages = {4389--4395},
  issn = {2153-0866},
  doi = {10.1109/IROS.2018.8593607},
  abstract = {In Minimally Invasive Robotic Surgery (MIRS) a robot is interposed between the surgeon and the surgical site to increase the precision, dexterity, and to reduce surgeon's effort and cognitive load with respect to the standard laparoscopic interventions. However, the modern robotic systems for MIRS are still based on the traditional telemanipulation paradigm, e.g. the robot behaviour is fully under surgeon's control, and no autonomy or assistance is implemented. In this work, supervised and shared controllers have been developed in a vision-free, human-in-the-Ioop, control framework to help surgeon during a surgical suturing procedure. Experiments conducted on the da Vinci Research Kit robot proves the effectiveness of the method indicating also the guidelines for improving results.},
  eventtitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  keywords = {Force,Force measurement,Needles,READ,Robots,STABILITY,Surgery,Task analysis,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/fontanelli_et_al_2018_a_comparison_of_assistive_methods_for_suturing_in_mirs.pdf}
}

@online{FrankaEmikaPanda,
  title = {Franka {{Emika Panda}} Robot - {{RoboDK}}},
  url = {https://robodk.com/robot/Franka/Emika-Panda},
  urldate = {2022-09-14}
}

@online{FrankaWorldFaBu,
  title = {Franka {{World}} 发布！\_{{Franka中国}}},
  url = {https://www.franka.cn/highlights/13.html},
  urldate = {2022-09-14}
}

@article{frankenBilateralTelemanipulationTime2011,
  title = {Bilateral {{Telemanipulation With Time Delays}}: {{A Two-Layer Approach Combining Passivity}} and {{Transparency}}},
  shorttitle = {Bilateral {{Telemanipulation With Time Delays}}},
  author = {Franken, Michel and Stramigioli, Stefano and Misra, Sarthak and Secchi, Cristian and Macchelli, Alessandro},
  date = {2011-08},
  journaltitle = {IEEE Transactions on Robotics},
  volume = {27},
  number = {4},
  pages = {741--756},
  issn = {1941-0468},
  doi = {10.1109/TRO.2011.2142430},
  abstract = {In this paper, a two-layer approach is presented to guarantee the stable behavior of bilateral telemanipulation systems in the presence of time-varying destabilizing factors such as hard contacts, relaxed user grasps, stiff control settings, and/or communication delays. The approach splits the control architecture into two separate layers. The hierarchical top layer is used to implement a strategy that addresses the desired transparency, and the lower layer ensures that no “virtual” energy is generated. This means that any bilateral controller can be implemented in a passive manner. Separate communication channels connect the layers at the slave and master sides so that information related to exchanged energy is completely separated from information about the desired behavior. Furthermore, the proposed implementation does not depend on any type of assumption about the time delay in the communication channel. By complete separation of the properties of passivity and transparency, each layer can accommodate any number of different implementations that allow for almost independent optimization. Experimental results are presented, which highlight the benefit of the proposed framework.},
  eventtitle = {{{IEEE Transactions}} on {{Robotics}}},
  keywords = {Bilateral control,Communication channels,Control systems,Delay effects,Energy exchange,Force,Friction,passivity,Robots,stability,STABILITY,telemanipulation,time delay,transparency},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/franken_et_al_2011_bilateral_telemanipulation_with_time_delays.pdf}
}

@inproceedings{franzeseILoSAInteractiveLearning2021,
  title = {{{ILoSA}}: {{Interactive Learning}} of {{Stiffness}} and {{Attractors}}},
  shorttitle = {{{ILoSA}}},
  booktitle = {2021 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Franzese, Giovanni and Mészáros, Anna and Peternel, Luka and Kober, Jens},
  date = {2021-09},
  pages = {7778--7785},
  issn = {2153-0866},
  doi = {10.1109/IROS51168.2021.9636710},
  abstract = {Teaching robots how to apply forces according to our preferences is still an open challenge that has to be tackled from multiple engineering perspectives. This paper studies how to learn variable impedance policies where both the Cartesian stiffness and the attractor can be learned from human demonstrations and corrections with a user-friendly interface. The presented framework, named ILoSA, uses Gaussian Processes for policy learning, identifying regions of uncertainty and allowing interactive corrections, stiffness modulation and active disturbance rejection. The experimental evaluation of the framework is carried out on a Franka-Emika Panda in four separate cases with unique force interaction properties: 1) pulling a plug wherein a sudden force discontinuity occurs upon successful removal of the plug, 2) pushing a box where a sustained force is required to keep the robot in motion, 3) wiping a whiteboard in which the force is applied perpendicular to the direction of movement, and 4) inserting a plug to verify the usability for precision-critical tasks in an experimental validation performed with non-expert users.},
  eventtitle = {2021 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  keywords = {Education,Force,Gaussian processes,Impedance,Modulation,STABILITY,Uncertainty,Usability},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/franzese_et_al_2021_ilosa.pdf}
}

@thesis{friedrichLearningFeedbackRobotics2021,
  title = {Learning and {{Feedback}} in {{Robotics}} with {{Stabilizing Controller Parameterizations}}},
  author = {Friedrich, Stefan Roland},
  date = {2021},
  institution = {{Technische Universität München}},
  url = {https://mediatum.ub.tum.de/1543212},
  urldate = {2023-04-25},
  abstract = {Increasing autonomy of intelligent robots demands for control technologies to leverage machine learning in the closed feedback loop while preserving stability. To this end, the parameterization of stabilizing controllers is investigated and combined with reinforcement learning, with a particular focus on robotics. The proposed control architectures and learning algorithms are illustrated by means of simulations and two case studies including laboratory experiments on robotic hardware.},
  keywords = {REVIEW},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/friedrich_2021_learning_and_feedback_in_robotics_with_stabilizing_controller_parameterizations.pdf}
}

@article{fuchsSuperHumanPerformanceGran2021,
  title = {Super-{{Human Performance}} in {{Gran Turismo Sport Using Deep Reinforcement Learning}}},
  author = {Fuchs, Florian and Song, Yunlong and Kaufmann, Elia and Scaramuzza, Davide and Durr, Peter},
  date = {2021-07},
  journaltitle = {IEEE Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {6},
  number = {3},
  pages = {4257--4264},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2021.3064284},
  url = {https://ieeexplore.ieee.org/document/9372847/},
  urldate = {2021-04-18},
  abstract = {Autonomous car racing is a major challenge in robotics. It raises fundamental problems for classical approaches such as planning minimum-time trajectories under uncertain dynamics and controlling the car at its limits of handling. Besides, the requirement of minimizing the lap time, which is a sparse objective, and the difficulty of collecting training data from human experts have also hindered researchers from directly applying learning-based approaches to solve the problem. In the present work, we propose a learning-based system for autonomous car racing by leveraging high-fidelity physical car simulation, a course-progress-proxy reward, and deep reinforcement learning. We deploy our system in Gran Turismo Sport, a world-leading car simulator known for its realistic physics simulation of different race cars and tracks, which is even used to recruit human race car drivers. Our trained policy achieves autonomous racing performance that goes beyond what had been achieved so far by the built-in AI, and, at the same time, outperforms the fastest driver in a dataset of over 50,000 human players.},
  langid = {english},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics}
}

@inproceedings{fuDirectDatadrivenStabilization2022,
  title = {Direct Data-Driven Stabilization of Nonlinear Affine Systems via the {{Koopman}} Operator},
  booktitle = {2022 {{IEEE}} 61st {{Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Fu, Xingyun and You, Keyou},
  date = {2022-12},
  pages = {2668--2673},
  issn = {2576-2370},
  doi = {10.1109/CDC51059.2022.9993302},
  abstract = {In this work, we are concerned with the design of direct data-driven controllers for nonlinear affine systems without explicit dynamical models. To this end, we adopt the Koopman operator to approximately reformulate the nonlinear systems into bilinear forms, based on which we propose a "simple" static controller in the linear form of the preset function dictionary. Then, we show how to obtain the feedback gain matrix and establish the stability condition of the closed-loop system using only the off-line collected data. Finally, numerical results demonstrate the effectiveness and robustness of this data-driven controller.},
  eventtitle = {2022 {{IEEE}} 61st {{Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  keywords = {Closed loop systems,Data models,Dictionaries,Nonlinear dynamical systems,Numerical stability,READ,Robustness,Stability analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/fu_you_2022_direct_data-driven_stabilization_of_nonlinear_affine_systems_via_the_koopman.pdf}
}

@unpublished{fuEX2ExplorationExemplar2017,
  title = {{{EX2}}: {{Exploration}} with {{Exemplar Models}} for {{Deep Reinforcement Learning}}},
  shorttitle = {{{EX2}}},
  author = {Fu, Justin and Co-Reyes, John D. and Levine, Sergey},
  date = {2017-05-27},
  eprint = {1703.01260},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1703.01260},
  urldate = {2020-07-02},
  abstract = {Deep reinforcement learning algorithms have been shown to learn complex tasks using highly general policy classes. However, sparse reward problems remain a significant challenge. Exploration methods based on novelty detection have been particularly successful in such settings but typically require generative or predictive models of the observations, which can be difficult to train when the observations are very high-dimensional and complex, as in the case of raw images. We propose a novelty detection algorithm for exploration that is based entirely on discriminatively trained exemplar models, where classifiers are trained to discriminate each visited state against all others. Intuitively, novel states are easier to distinguish against other states seen during training. We show that this kind of discriminative modeling corresponds to implicit density estimation, and that it can be combined with count-based exploration to produce competitive results on a range of popular benchmark tasks, including state-of-the-art results on challenging egocentric observations in the vizDoom benchmark.},
  keywords = {#nosource,Computer Science - Machine Learning,exploration,intrinsic motivation,reinforcement learning},
  annotation = {00068}
}

@unpublished{fujimotoAddressingFunctionApproximation2018,
  title = {Addressing {{Function Approximation Error}} in {{Actor-Critic Methods}}},
  author = {Fujimoto, Scott and family=Hoof, given=Herke, prefix=van, useprefix=true and Meger, David},
  date = {2018-10-22},
  eprint = {1802.09477},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1802.09477},
  urldate = {2020-07-02},
  abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,deterministic policy gradients,model-free,policy optimization,q-learning,reinforcement learning,Statistics - Machine Learning}
}

@online{gadginmathDataDrivenFeedbackLinearization2022,
  title = {Data-{{Driven Feedback Linearization}} Using the {{Koopman Generator}}},
  author = {Gadginmath, Darshan and Krishnan, Vishaal and Pasqualetti, Fabio},
  date = {2022-10-10},
  eprint = {2210.05046},
  eprinttype = {arxiv},
  eprintclass = {cs, eess, math},
  doi = {10.48550/arXiv.2210.05046},
  url = {http://arxiv.org/abs/2210.05046},
  urldate = {2023-01-19},
  abstract = {This paper contributes a theoretical framework for data-driven feedback linearization of nonlinear control-affine systems. We unify the traditional geometric perspective on feedback linearization with an operator-theoretic perspective involving the Koopman operator. We first show that if the distribution of the control vector field and its repeated Lie brackets with the drift vector field is involutive, then there exists an output and a feedback control law for which the Koopman generator is finite-dimensional and locally nilpotent. We use this connection to propose a data-driven algorithm for feedback linearization. Particularly, we use experimental data to identify the state transformation and control feedback from a dictionary of functions for which feedback linearization is achieved in a least-squares sense. Finally, we provide numerical examples for the data-driven algorithm and compare it with model-based feedback linearization. We also numerically study the effect of the richness of the dictionary and the size of the data set on the effectiveness of feedback linearization.},
  pubstate = {preprint},
  keywords = {Electrical Engineering and Systems Science - Systems and Control,Mathematics - Optimization and Control,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/gadginmath_et_al_2022_data-driven_feedback_linearization_using_the_koopman_generator.pdf}
}

@unpublished{galDropoutBayesianApproximation2015,
  title = {Dropout as a {{Bayesian Approximation}}: {{Representing Model Uncertainty}} in {{Deep Learning}}},
  shorttitle = {Dropout as a {{Bayesian Approximation}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  date = {2015-06-06},
  eprint = {1506.02142},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1506.02142},
  urldate = {2019-04-18},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs –extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and nonlinearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout’s uncertainty in deep reinforcement learning.},
  langid = {english},
  keywords = {#nosource,Computer Science - Machine Learning,machine learning control,safe-rl,Statistics - Machine Learning}
}

@inproceedings{galImprovingPILCOBayesian2016,
  title = {Improving {{PILCO}} with {{Bayesian}} Neural Network Dynamics Models},
  booktitle = {Data-{{Efficient Machine Learning}} Workshop, {{ICML}}},
  author = {Gal, Yarin and McAllister, Rowan and Rasmussen, Carl Edward},
  date = {2016},
  volume = {4},
  keywords = {#nosource,machine learning control,safe-rl}
}

@article{gamsCouplingMovementPrimitives2014,
  title = {Coupling {{Movement Primitives}}: {{Interaction With}} the {{Environment}} and {{Bimanual Tasks}}},
  shorttitle = {Coupling {{Movement Primitives}}},
  author = {Gams, Andrej and Nemec, Bojan and Ijspeert, Auke Jan and Ude, Ales},
  date = {2014-08},
  journaltitle = {Ieee Transactions on Robotics},
  shortjournal = {IEEE Trans. Robot.},
  volume = {30},
  number = {4},
  pages = {816--830},
  publisher = {{Ieee-Inst Electrical Electronics Engineers Inc}},
  location = {{Piscataway}},
  issn = {1552-3098},
  doi = {10.1109/TRO.2014.2304775},
  url = {https://ieeexplore.ieee.org/document/6748918},
  urldate = {2022-12-15},
  abstract = {The framework of dynamic movement primitives (DMPs) contains many favorable properties for the execution of robotic trajectories, such as indirect dependence on time, response to perturbations, and the ability to easily modulate the given trajectories, but the framework in its original form remains constrained to the kinematic aspect of the movement. In this paper, we bridge the gap to dynamic behavior by extending the framework with force/torque feedback. We propose and evaluate a modulation approach that allows interaction with objects and the environment. Through the proposed coupling of originally independent robotic trajectories, the approach also enables the execution of bimanual and tightly coupled cooperative tasks. We apply an iterative learning control algorithm to learn a coupling term, which is applied to the original trajectory in a feed-forward fashion and, thus, modifies the trajectory in accordance to the desired positions or external forces. A stability analysis and results of simulated and real-world experiments using two KUKA LWR arms for bimanual tasks and interaction with the environment are presented. By expanding on the framework of DMPs, we keep all the favorable properties, which is demonstrated with temporal modulation and in a two-agent obstacle avoidance task.},
  langid = {english},
  keywords = {adaptation,Bimanual operation,cooperative task,dynamic movement primitives   (DMPs),dynamics,imitation,impedance control,interaction with environment,iterative learning control,motion,READ,robots,STABILITY},
  annotation = {WOS:000340451800004},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/gams_et_al_2014_coupling_movement_primitives.pdf}
}

@inproceedings{ganeshVersatileBiomimeticController2012,
  title = {A Versatile Biomimetic Controller for Contact Tooling and Haptic Exploration},
  booktitle = {2012 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Ganesh, Gowrishankar and Jarrassé, Nathanael and Haddadin, Sami and Albu-Schaeffer, Alin and Burdet, Etienne},
  date = {2012},
  pages = {3329--3334},
  publisher = {{IEEE}},
  keywords = {READ,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ganesh_et_al_2012_a_versatile_biomimetic_controller_for_contact_tooling_and_haptic_exploration.pdf}
}

@online{gaoLearningDynamicalSystem2021,
  title = {Learning {{Dynamical System}} for {{Grasping Motion}}},
  author = {Gao, Xiao and Li, Miao and Xiao, Xiaohui},
  date = {2021-08-15},
  eprint = {2108.06728},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2108.06728},
  urldate = {2023-01-18},
  abstract = {Dynamical System has been widely used for encoding trajectories from human demonstration, which has the inherent adaptability to dynamically changing environments and robustness to perturbations. In this paper we propose a framework to learn a dynamical system that couples position and orientation based on a diffeomorphism. Different from other methods, it can realise the synchronization between positon and orientation during the whole trajectory. Online grasping experiments are carried out to prove its effectiveness and online adaptability.},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/gao_et_al_2021_learning_dynamical_system_for_grasping_motion.pdf}
}

@article{garciaSafeExplorationState2012,
  title = {Safe {{Exploration}} of {{State}} and {{Action Spaces}} in {{Reinforcement Learning}}},
  author = {Garcia, Javier and Fernandez, Fernando},
  date = {2012-12-19},
  journaltitle = {Journal of Artificial Intelligence Research},
  volume = {45},
  eprint = {1402.0560},
  eprinttype = {arxiv},
  pages = {515--564},
  issn = {1076-9757},
  doi = {10.1613/jair.3761},
  url = {http://arxiv.org/abs/1402.0560},
  urldate = {2019-04-23},
  abstract = {In this paper, we consider the important problem of safe exploration in reinforcement learning. While reinforcement learning is well-suited to domains with complex transition dynamics and high-dimensional state-action spaces, an additional challenge is posed by the need for safe and efficient exploration. Traditional exploration techniques are not particularly useful for solving dangerous tasks, where the trial and error process may lead to the selection of actions whose execution in some states may result in damage to the learning system (or any other system). Consequently, when an agent begins an interaction with a dangerous and high-dimensional state-action space, an important question arises; namely, that of how to avoid (or at least minimize) damage caused by the exploration of the state-action space. We introduce the PI-SRL algorithm which safely improves suboptimal albeit robust behaviors for continuous state and action control tasks and which efficiently learns from the experience gained from the environment. We evaluate the proposed method in four complex tasks: automatic car parking, pole-balancing, helicopter hovering, and business management.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,machine learning control,safe-rl}
}

@inproceedings{gatrellCADbasedGraspSynthesis1989,
  title = {{{CAD-based}} Grasp Synthesis Utilizing Polygons, Edges and Vertexes},
  booktitle = {1989 {{International Conference}} on {{Robotics}} and {{Automation Proceedings}}},
  author = {Gatrell, L. B.},
  date = {1989-05},
  pages = {184-189 vol.1},
  doi = {10.1109/ROBOT.1989.99987},
  abstract = {A CAD-based grasp-synthesis system has been developed that precomputes valid, stable grasp sites on CAD models for use by a robot system. Extended Gaussian images, which are efficient data structures for mapping surfaces into Gaussian space based on surface normals, are shown. A modified version is introduced which allows the mapping of polygon, edge, and vertex normals. Using the modified extended Gaussian image, pairs of parallel surfaces are found in linear time. Pairs of graspable surfaces include polygon-polygon, polygon-edge, and polygon-point. A method of computing the rotational stability of edge contacts is given. The stability of each grasp point and its orientation are computed, and all grasps are ranked in descending order of stability. The grasp-synthesis program is demonstrated on three models, one concave and two convex; all three types of grasps are shown.{$<<$}ETX{$>>$}},
  eventtitle = {1989 {{International Conference}} on {{Robotics}} and {{Automation Proceedings}}},
  keywords = {#nosource,CAD-based grasp-synthesis system,CAD/CAM,computational geometry,Computer science,Computer vision,data structures,Data structures,Design automation,edges,extended Gaussian image,graspable surfaces,Grippers,machine learning,mechanical stability,object grasping,Orbital robotics,Packaging,polygons,robot,Robot vision systems,robotic grasping,robots,rotational stability,Solid modeling,Stability,stable grasp sites,surface normals,tno internship,vertexes},
  annotation = {00010}
}

@unpublished{gauciHorizonFacebookOpen2019,
  title = {Horizon: {{Facebook}}'s {{Open Source Applied Reinforcement Learning Platform}}},
  shorttitle = {Horizon},
  author = {Gauci, Jason and Conti, Edoardo and Liang, Yitao and Virochsiri, Kittipat and He, Yuchen and Kaden, Zachary and Narayanan, Vivek and Ye, Xiaohui and Chen, Zhengxing and Fujimoto, Scott},
  date = {2019-09-04},
  eprint = {1811.00260},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1811.00260},
  urldate = {2020-07-02},
  abstract = {In this paper we present Horizon, Facebook's open source applied reinforcement learning (RL) platform. Horizon is an end-to-end platform designed to solve industry applied RL problems where datasets are large (millions to billions of observations), the feedback loop is slow (vs. a simulator), and experiments must be done with care because they don't run in a simulator. Unlike other RL platforms, which are often designed for fast prototyping and experimentation, Horizon is designed with production use cases as top of mind. The platform contains workflows to train popular deep RL algorithms and includes data preprocessing, feature transformation, distributed training, counterfactual policy evaluation, optimized serving, and a model-based data understanding tool. We also showcase and describe real examples where reinforcement learning models trained with Horizon significantly outperformed and replaced supervised learning systems at Facebook.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,real-world,reinforcement learning,Statistics - Machine Learning}
}

@inproceedings{gerlaghEnergyawareAdaptiveImpedance2021,
  title = {Energy-Aware Adaptive Impedance Control Using Offline Task-Based Optimization},
  booktitle = {2021 20th {{International Conference}} on {{Advanced Robotics}} ({{ICAR}})},
  author = {Gerlagh, Bart and Califano, Federico and Stramigioli, Stefano and Roozing, Wesley},
  date = {2021-12},
  pages = {187--194},
  doi = {10.1109/ICAR53236.2021.9659443},
  abstract = {Robotic tasks involving interaction with an environment are commonly addressed by means of impedance control. Under the framework of energy-aware robotics, and motivated by evidence that many tasks benefit from variable impedance, we leverage a geometric formulation of an impedance controlled manipulator with time-varying impedance, to design an optimisation problem able to produce an open-loop, task-based control action. A feedback controller accounts for model variations and disturbances from the nominal task and completes the control strategy. Beyond achieving passivity by means of energy tanks, a novel energy budgeting protocol ensures safety along task execution. Simulations on a 5-DoF robot executing a peg-in-hole task validate the approach.},
  eventtitle = {2021 20th {{International Conference}} on {{Advanced Robotics}} ({{ICAR}})},
  keywords = {Adaptation models,Impedance,Measurement,Protocols,READ,Robustness,Safety,Shock absorbers,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/gerlagh_et_al_2021_energy-aware_adaptive_impedance_control_using_offline_task-based_optimization.pdf}
}

@article{geSafeQLearningMethod2019,
  title = {Safe {{Q-Learning Method Based}} on {{Constrained Markov Decision Processes}}},
  author = {Ge, Y. and Zhu, F. and Ling, X. and Liu, Q.},
  date = {2019},
  journaltitle = {IEEE Access},
  volume = {7},
  pages = {165007--165017},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2952651},
  abstract = {The application of reinforcement learning in industrial fields makes the safety problem of the agent a research hotspot. Traditional methods mainly alter the objective function and the exploration process of the agent to address the safety problem. Those methods, however, can hardly prevent the agent from falling into dangerous states because most of the methods ignore the damage caused by unsafe states. As a result, most solutions are not satisfactory. In order to solve the aforementioned problem, we come forward with a safe Q-learning method that is based on constrained Markov decision processes, adding safety constraints as prerequisites to the model, which improves standard Q-learning algorithm so that the proposed algorithm seeks for the optimal solution ensuring that the safety premise is satisfied. During the process of finding the solution in form of the optimal state-action value, the feasible space of the agent is limited to the safe space that guarantees the safety via the feasible space being filtered by constraints added to the action space. Because the traditional solution methods are not applicable to the safe Q-learning model as they tend to obtain local optimal solution, we take advantage of the Lagrange multiplier method to solve the optimal action that can be performed in the current state based on the premise of linearizing constraint functions, which not only improves the efficiency and accuracy of the algorithm, but also guarantees to obtain the global optimal solution. The experiments verify the effectiveness of the algorithm.},
  eventtitle = {{{IEEE Access}}},
  keywords = {#nosource,Constrained Markov decision processes,constraint,Heuristic algorithms,Lagrange multiplier,Linear programming,lyapunov,Markov processes,Optimization,Q-learning,Reinforcement learning,safe reinforcement learning,Safety,Task analysis}
}

@unpublished{ghazaeiDealingAmbiguityRobotic2018,
  title = {Dealing with Ambiguity in Robotic Grasping via Multiple Predictions},
  author = {Ghazaei, Ghazal and Laina, Iro and Rupprecht, Christian and Tombari, Federico and Navab, Nassir and Nazarpour, Kianoush},
  date = {2018},
  eprint = {1811.00793},
  eprinttype = {arxiv},
  keywords = {#nosource,robotic grasping,tno internship}
}

@article{gieslReviewComputationalMethods2015,
  title = {Review on Computational Methods for {{Lyapunov}} Functions},
  author = {Giesl, Peter and Hafstein, Sigurdur},
  date = {2015},
  journaltitle = {Discrete and Continuous Dynamical Systems - B},
  volume = {20},
  number = {8},
  pages = {2291},
  publisher = {{American Institute of Mathematical Sciences}},
  doi = {10.3934/dcdsb.2015.20.2291},
  url = {https://www.aimsciences.org/article/doi/10.3934/dcdsb.2015.20.2291},
  urldate = {2022-06-11},
  abstract = {Lyapunov functions are an essential tool in the stability analysis of dynamical systems, both in theory and applications. They provide sufficient conditions for the stability of equilibria or more general invariant sets, as well as for their basin of attraction. The necessity, i.e. the existence of Lyapunov functions, has been studied in converse theorems, however, they do not provide a general method to compute them. {$<$}br{$>~$} ~ Because of their importance in stability analysis, numerous computational construction methods have been developed  within the Engineering, Informatics, and Mathematics community. They cover different types of systems such as ordinary differential equations, switched systems, non-smooth systems, discrete-time systems etc., and employ different methods such as series expansion, linear programming, linear matrix inequalities, collocation methods, algebraic methods, set-theoretic methods, and many others. This review brings these different methods together. First, the different types of systems, where Lyapunov functions are used, are briefly discussed. In the main part, the computational methods are presented, ordered by the type of method used to construct a Lyapunov function.},
  langid = {english},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/giesl_hafstein_2015_review_on_computational_methods_for_lyapunov_functions.pdf}
}

@inproceedings{ginesiDynamicMovementPrimitives2019,
  title = {Dynamic {{Movement Primitives}}: {{Volumetric Obstacle Avoidance}}},
  shorttitle = {Dynamic {{Movement Primitives}}},
  booktitle = {2019 19th {{International Conference}} on {{Advanced Robotics}} ({{ICAR}})},
  author = {Ginesi, Michele and Meli, Daniele and Calanca, Andrea and Dall'Alba, Diego and Sansonetto, Nicola and Fiorini, Paolo},
  date = {2019-12},
  pages = {234--239},
  doi = {10.1109/ICAR46387.2019.8981552},
  abstract = {Dynamic Movement Primitives (DMPs) are a framework for learning a trajectory from a demonstration. The trajectory can be learned efficiently after only one demonstration, and it is immediate to adapt it to new goal positions and time duration. Moreover, the trajectory is also robust against perturbations. However, obstacle avoidance for DMPs is still an open problem. In this work, we propose an extension of DMPs to support volumetric obstacle avoidance based on the use of superquadric potentials. We show the advantages of this approach when obstacles have known shape, and we extend it to unknown objects using minimal enclosing ellipsoids. A simulation and experiments with a real robot validate the framework, and we make freely available our implementation.},
  eventtitle = {2019 19th {{International Conference}} on {{Advanced Robotics}} ({{ICAR}})},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ginesi_et_al_2019_dynamic_movement_primitives.pdf}
}

@article{ginesiDynamicMovementPrimitives2021,
  title = {Dynamic Movement Primitives: {{Volumetric}} Obstacle Avoidance Using Dynamic Potential Functions},
  shorttitle = {Dynamic Movement Primitives},
  author = {Ginesi, Michele and Meli, Daniele and Roberti, Andrea and Sansonetto, Nicola and Fiorini, Paolo},
  date = {2021},
  journaltitle = {Journal of Intelligent \& Robotic Systems},
  volume = {101},
  number = {4},
  pages = {1--20},
  publisher = {{Springer}},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ginesi_et_al_2021_dynamic_movement_primitives.pdf}
}

@article{ginesiOvercomingDrawbacksDynamic2021,
  title = {Overcoming Some Drawbacks of {{Dynamic Movement Primitives}}},
  author = {Ginesi, Michele and Sansonetto, Nicola and Fiorini, Paolo},
  date = {2021-10-01},
  journaltitle = {Robotics and Autonomous Systems},
  shortjournal = {Robotics and Autonomous Systems},
  volume = {144},
  pages = {103844},
  issn = {0921-8890},
  doi = {10.1016/j.robot.2021.103844},
  url = {https://www.sciencedirect.com/science/article/pii/S0921889021001299},
  urldate = {2022-10-30},
  abstract = {Dynamic Movement Primitives (DMPs) is a framework for learning a point-to-point trajectory from a demonstration. Despite being widely used, DMPs still present some shortcomings that may limit their usage in real robotic applications. Firstly, at the state of the art, mainly Gaussian basis functions have been used to perform function approximation. Secondly, the adaptation of the trajectory generated by the DMP heavily depends on the choice of hyperparameters and the new desired goal position. Lastly, DMPs are a framework for ‘one-shot learning’, meaning that they are constrained to learn from a unique demonstration. In this work, we present and motivate a new set of basis functions to be used in the learning process, showing their ability to accurately approximate functions while having both analytical and numerical advantages w.r.t. Gaussian basis functions. Then, we show how to use the invariance of DMPs w.r.t. affine transformations to make the generalization of the trajectory robust against both the choice of hyperparameters and new goal position, performing both synthetic tests and experiments with real robots to show this increased robustness. Finally, we propose an algorithm to extract a common behavior from multiple observations, validating it both on a synthetic dataset and on a dataset obtained by performing a task on a real robot.},
  langid = {english},
  keywords = {Dynamic Movement Primitives,IMPORTANT,Kinematics,Learning from demonstrations,Motion and path planning,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ginesi_et_al_2021_overcoming_some_drawbacks_of_dynamic_movement_primitives.pdf}
}

@article{greeffExploitingDifferentialFlatness2021,
  title = {Exploiting {{Differential Flatness}} for {{Robust Learning-Based Tracking Control Using Gaussian Processes}}},
  author = {Greeff, Melissa and Schoellig, Angela P.},
  date = {2021-10},
  journaltitle = {IEEE Control Systems Letters},
  volume = {5},
  number = {4},
  pages = {1121--1126},
  issn = {2475-1456},
  doi = {10.1109/LCSYS.2020.3009177},
  abstract = {Learning-based control has shown to outperform conventional model-based techniques in the presence of model uncertainties and systematic disturbances. However, most state-of-the-art learning-based nonlinear trajectory tracking controllers still lack any formal guarantees. In this letter, we exploit the property of differential flatness to design an online, robust learning-based controller to achieve both high tracking performance and probabilistically guarantee a uniform ultimate bound on the tracking error. A common control approach for differentially flat systems is to try to linearize the system by using a feedback (FB) linearization controller designed based on a nominal system model. Performance and safety are limited by the mismatch between the nominal model and the actual system. Our proposed approach uses a nonparametric Gaussian Process (GP) to both improve FB linearization and quantify, probabilistically, the uncertainty in our FB linearization. We use this probabilistic bound in a robust linear quadratic regulator (LQR) framework. Through simulation, we highlight that our proposed approach significantly outperforms alternative learning-based strategies that use differential flatness.},
  eventtitle = {{{IEEE Control Systems Letters}}},
  keywords = {Aerodynamics,Control systems,Feedback linearization,Gaussian processes,machine learning,Nonlinear dynamical systems,Probabilistic logic,READ,robust control,Robustness,Uncertainty},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/greeff_schoellig_2021_exploiting_differential_flatness_for_robust_learning-based_tracking_control.pdf}
}

@unpublished{gregorVariationalIntrinsicControl2016,
  title = {Variational {{Intrinsic Control}}},
  author = {Gregor, Karol and Rezende, Danilo Jimenez and Wierstra, Daan},
  date = {2016-11-22},
  eprint = {1611.07507},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1611.07507},
  urldate = {2020-07-02},
  abstract = {In this paper we introduce a new unsupervised reinforcement learning method for discovering the set of intrinsic options available to an agent. This set is learned by maximizing the number of different states an agent can reliably reach, as measured by the mutual information between the set of options and option termination states. To this end, we instantiate two policy gradient based algorithms, one that creates an explicit embedding space of options and one that represents options implicitly. The algorithms also provide an explicit measure of empowerment in a given state that can be used by an empowerment maximizing agent. The algorithm scales well with function approximation and we demonstrate the applicability of the algorithm on a range of tasks.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,reinforcement learning,unsupervised RL},
  annotation = {00088}
}

@article{gribovskayaLearningNonlinearMultivariate2011,
  title = {Learning Non-Linear Multivariate Dynamics of Motion in Robotic Manipulators},
  author = {Gribovskaya, Elena and Khansari-Zadeh, Seyed Mohammad and Billard, Aude},
  date = {2011},
  journaltitle = {The International Journal of Robotics Research},
  volume = {30},
  number = {1},
  pages = {80--117},
  publisher = {{SAGE Publications Sage UK: London, England}},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/gribovskaya_et_al_2011_learning_non-linear_multivariate_dynamics_of_motion_in_robotic_manipulators.pdf}
}

@unpublished{gruslysReactorFastSampleefficient2018,
  title = {The {{Reactor}}: {{A}} Fast and Sample-Efficient {{Actor-Critic}} Agent for {{Reinforcement Learning}}},
  shorttitle = {The {{Reactor}}},
  author = {Gruslys, Audrunas and Dabney, Will and Azar, Mohammad Gheshlaghi and Piot, Bilal and Bellemare, Marc and Munos, Remi},
  date = {2018-06-19},
  eprint = {1704.04651},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1704.04651},
  urldate = {2020-07-02},
  abstract = {In this work we present a new agent architecture, called Reactor, which combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN (Wang et al., 2016) and Categorical DQN (Bellemare et al., 2017), while giving better run-time performance than A3C (Mnih et al., 2016). Our first contribution is a new policy evaluation algorithm called Distributional Retrace, which brings multi-step off-policy updates to the distributional reinforcement learning setting. The same approach can be used to convert several classes of multi-step policy evaluation algorithms designed for expected value evaluation into distributional ones. Next, we introduce the \textbackslash b\{eta\}-leave-one-out policy gradient algorithm which improves the trade-off between variance and bias by using action values as a baseline. Our final algorithmic contribution is a new prioritized replay algorithm for sequences, which exploits the temporal locality of neighboring observations for more efficient replay prioritization. Using the Atari 2600 benchmarks, we show that each of these innovations contribute to both the sample efficiency and final agent performance. Finally, we demonstrate that Reactor reaches state-of-the-art performance after 200 million frames and less than a day of training.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,model-free,policy gradients,QL,reinforcement learning}
}

@unpublished{gualtieriHighPrecisionGrasp2016,
  title = {High Precision Grasp Pose Detection in Dense Clutter},
  author = {Gualtieri, Marcus and family=Pas, given=Andreas, prefix=ten, useprefix=false and Saenko, Kate and Platt, Robert},
  date = {2016-03-04},
  eprint = {1603.01564},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1603.01564},
  urldate = {2019-04-17},
  abstract = {This paper considers the problem of grasp pose detection in point clouds. We follow a general algorithmic structure that first generates a large set of 6-DOF grasp candidates and then classifies each of them as a good or a bad grasp. Our focus in this paper is on improving the second step by using depth sensor scans from large online datasets to train a convolutional neural network. We propose two new representations of grasp candidates, and we quantify the effect of using prior knowledge of two forms: instance or category knowledge of the object to be grasped, and pretraining the network on simulated depth data obtained from idealized CAD models. Our analysis shows that a more informative grasp candidate representation as well as pretraining and prior knowledge significantly improve grasp detection. We evaluate our approach on a Baxter Research Robot and demonstrate an average grasp success rate of 93\% in dense clutter. This is a 20\% improvement compared to our prior work.},
  keywords = {#nosource,Computer Science - Robotics,object grasping,robotic grasping,tno internship}
}

@inproceedings{guDeepReinforcementLearning2017,
  title = {Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates},
  booktitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Gu, S. and Holly, E. and Lillicrap, T. and Levine, S.},
  date = {2017-05},
  pages = {3389--3396},
  doi = {10.1109/ICRA.2017.7989385},
  abstract = {Reinforcement learning holds the promise of enabling autonomous robots to learn large repertoires of behavioral skills with minimal human intervention. However, robotic applications of reinforcement learning often compromise the autonomy of the learning process in favor of achieving training times that are practical for real physical systems. This typically involves introducing hand-engineered policy representations and human-supplied demonstrations. Deep reinforcement learning alleviates this limitation by training general-purpose neural network policies, but applications of direct deep reinforcement learning algorithms have so far been restricted to simulated settings and relatively simple tasks, due to their apparent high sample complexity. In this paper, we demonstrate that a recent deep reinforcement learning algorithm based on off-policy training of deep Q-functions can scale to complex 3D manipulation tasks and can learn deep neural network policies efficiently enough to train on real physical robots. We demonstrate that the training times can be further reduced by parallelizing the algorithm across multiple robots which pool their policy updates asynchronously. Our experimental evaluation shows that our method can learn a variety of 3D manipulation skills in simulation and a complex door opening skill on real robots without any prior demonstrations or manually designed representations.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  keywords = {#nosource,3D manipulation skills,asynchronous off-policy updates,autonomous robots,deep Q-functions,deep reinforcement learning,hand-engineered policy representations,Heuristic algorithms,human-supplied demonstration,Instruction sets,learning (artificial intelligence),Learning (artificial intelligence),manipulators,Neural networks,object grasping,reinforcement learning,robotic grasping,robotic manipulation,Robots,Safety,sample complexity,tno internship,Training},
  annotation = {00270}
}

@article{guoRoboticGraspingUsing2017,
  title = {Robotic Grasping Using Visual and Tactile Sensing},
  author = {Guo, Di and Sun, Fuchun and Fang, Bin and Yang, Chao and Xi, Ning},
  date = {2017-11-01},
  journaltitle = {Information Sciences},
  shortjournal = {Information Sciences},
  volume = {417},
  pages = {274--286},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2017.07.017},
  url = {http://www.sciencedirect.com/science/article/pii/S002002551730837X},
  urldate = {2019-05-09},
  abstract = {Visual and tactile sensing are complementary factors in the task of robotic grasping. In this paper, a grasp detection deep network is first proposed to detect the grasp rectangle from the visual image， then a new metric using tactile sensing is designed to assess the stability of the grasp. By means of this scheme, a THU grasp dataset， which includes the visual information, corresponding tactile and grasp configurations， is collected to train the proposed deep network. Experiments results have demonstrated that the proposed grasp detection deep networks outperform other mainstream approaches in a public grasp dataset. Furthermore， the grasp success rate can be improved significantly in real world scenarios. The trained model has also been successfully implemented in a new robotic platform to perform the robotic grasping task in a cluttered scenario.},
  keywords = {#nosource,Grasp detection deep networks,grasp pose detection,Grasp stability,neural network,object grasping,robotic grasping,Tactile sensing,tno internship,Visual sensing},
  annotation = {00007}
}

@online{guptaLearningHighDimensional2022,
  title = {Learning {{High Dimensional Demonstrations Using Laplacian Eigenmaps}}},
  author = {Gupta, Sthithpragya and Nayak, Aradhana and Billard, Aude},
  date = {2022-07-18},
  eprint = {2207.08714},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2207.08714},
  urldate = {2022-12-09},
  abstract = {This article proposes a novel methodology to learn a stable robot control law driven by dynamical systems. The methodology requires a single demonstration and can deduce a stable dynamics in arbitrary high dimensions. The method relies on the idea that there exists a latent space in which the nonlinear dynamics appears quasi linear. The original nonlinear dynamics is mapped into a stable linear DS, by leveraging on the properties of graph embeddings. We show that the eigendecomposition of the Graph Laplacian results in linear embeddings in two dimensions and quasi-linear in higher dimensions. The nonlinear terms vanish, exponentially as the number of datapoints increase, and for large density of points, the embedding appears linear. We show that this new embedding enables to model highly nonlinear dynamics in high dimension and overcomes alternative techniques in both precision of reconstruction and number of parameters required for the embedding. We demonstrate its applicability to control real robot tasked to perform complex free motion in space.},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/gupta_et_al_2022_learning_high_dimensional_demonstrations_using_laplacian_eigenmaps.pdf}
}

@unpublished{guQPropSampleEfficientPolicy2017,
  title = {Q-{{Prop}}: {{Sample-Efficient Policy Gradient}} with {{An Off-Policy Critic}}},
  shorttitle = {Q-{{Prop}}},
  author = {Gu, Shixiang and Lillicrap, Timothy and Ghahramani, Zoubin and Turner, Richard E. and Levine, Sergey},
  date = {2017-02-27},
  eprint = {1611.02247},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1611.02247},
  urldate = {2020-07-02},
  abstract = {Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.},
  keywords = {#nosource,action-dependent baseline,Computer Science - Machine Learning,model-free,policy gradients,reinforcement learning}
}

@online{haarnojaSoftActorCriticAlgorithms2019,
  title = {Soft {{Actor-Critic Algorithms}} and {{Applications}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
  date = {2019-01-29},
  eprint = {1812.05905},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1812.05905},
  url = {http://arxiv.org/abs/1812.05905},
  urldate = {2023-07-05},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework. In this framework, the actor aims to simultaneously maximize expected return and entropy. That is, to succeed at the task while acting as randomly as possible. We extend SAC to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate SAC on a range of benchmark tasks, as well as real-world challenging tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that SAC is a promising candidate for learning in real-world robotics tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/Haarnoja et al_2019_Soft Actor-Critic Algorithms and Applications.pdf;/home/ricks/Zotero/storage/QQTLTIXE/1812.html}
}

@unpublished{haarnojaSoftActorCriticOffPolicy2018,
  title = {Soft {{Actor-Critic}}: {{Off-Policy Maximum Entropy Deep Reinforcement Learning}} with a {{Stochastic Actor}}},
  shorttitle = {Soft {{Actor-Critic}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  date = {2018-08-08},
  eprint = {1801.01290},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1801.01290},
  urldate = {2020-07-02},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,model-free,policy gradients,policy optimization,q-learning,reinforcement learning,Statistics - Machine Learning},
  annotation = {00000},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/Haarnoja et al_2018_Soft Actor-Critic.pdf}
}

@article{haddadinRobotCollisionsSurvey2017,
  title = {Robot {{Collisions}}: {{A Survey}} on {{Detection}}, {{Isolation}}, and {{Identification}}},
  shorttitle = {Robot {{Collisions}}},
  author = {Haddadin, Sami and De Luca, Alessandro and Albu-Schäffer, Alin},
  date = {2017-12},
  journaltitle = {IEEE Transactions on Robotics},
  volume = {33},
  number = {6},
  pages = {1292--1312},
  issn = {1941-0468},
  doi = {10.1109/TRO.2017.2723903},
  abstract = {Robot assistants and professional coworkers are becoming a commodity in domestic and industrial settings. In order to enable robots to share their workspace with humans and physically interact with them, fast and reliable handling of possible collisions on the entire robot structure is needed, along with control strategies for safe robot reaction. The primary motivation is the prevention or limitation of possible human injury due to physical contacts. In this survey paper, based on our early work on the subject, we review, extend, compare, and evaluate experimentally model-based algorithms for real-time collision detection, isolation, and identification that use only proprioceptive sensors. This covers the context-independent phases of the collision event pipeline for robots interacting with the environment, as in physical human-robot interaction or manipulation tasks. The problem is addressed for rigid robots first and then extended to the presence of joint/transmission flexibility. The basic physically motivated solution has already been applied to numerous robotic systems worldwide, ranging from manipulators and humanoids to flying robots, and even to commercial products.},
  eventtitle = {{{IEEE Transactions}} on {{Robotics}}},
  keywords = {Algorithm design and analysis,Collision avoidance,Collision detection,collision identification,collision isolation,flexible joint manipulators,human-friendly robotics,Human-robot interaction,physical human–robot interaction (pHRI),Real-time systems,Robot sensing systems,safe robotics,Service robots},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/haddadin_et_al_2017_robot_collisions.pdf}
}

@book{haddadinSafeRobotsApproaching2013,
  title = {Towards Safe Robots: Approaching {{Asimov}}’s 1st Law},
  shorttitle = {Towards Safe Robots},
  author = {Haddadin, Sami},
  date = {2013},
  volume = {90},
  publisher = {{Springer}},
  keywords = {REVIEW},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/haddadin_2013_towards_safe_robots.pdf}
}

@incollection{haddadNonlinearDynamicalSystems2011,
  title = {Nonlinear Dynamical Systems and Control},
  booktitle = {Nonlinear {{Dynamical Systems}} and {{Control}}},
  author = {Haddad, Wassim M. and Chellaboina, VijaySekhar},
  date = {2011},
  publisher = {{Princeton university press}},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/haddad_chellaboina_2011_nonlinear_dynamical_systems_and_control2.pdf}
}

@article{hafnerLearningLatentDynamics,
  title = {Learning {{Latent Dynamics}} for {{Planning}} from {{Pixels}}},
  author = {Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
  pages = {20},
  abstract = {Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains. We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. We approach this using a latent dynamics model with both deterministic and stochastic transition components. Moreover, we propose a multi-step variational inference objective that we name latent overshooting. Using only pixel observations, our agent solves continuous control tasks with contact dynamics, partial observability, and sparse rewards, which exceed the difficulty of tasks that were previously solved by planning with learned models. PlaNet uses substantially fewer episodes and reaches final performance close to and sometimes higher than strong model-free algorithms.},
  langid = {english},
  keywords = {#nosource}
}

@article{hamedaniIntelligentImpedanceControl2021,
  title = {Intelligent {{Impedance Control}} Using {{Wavelet Neural Network}} for Dynamic Contact Force Tracking in Unknown Varying Environments},
  author = {Hamedani, Mohammad Hossein and Sadeghian, Hamid and Zekri, Maryam and Sheikholeslam, Farid and Keshmiri, Mehdi},
  date = {2021-08-01},
  journaltitle = {Control Engineering Practice},
  shortjournal = {Control Engineering Practice},
  volume = {113},
  pages = {104840},
  issn = {0967-0661},
  doi = {10.1016/j.conengprac.2021.104840},
  url = {https://www.sciencedirect.com/science/article/pii/S0967066121001179},
  urldate = {2022-05-02},
  abstract = {In this paper, the Intelligent Impedance Control based Wavelet Neural Network (IIC-WNN) is introduced as a noble adaptive variable impedance approach to enhance the efficiency of tracking the desired force and interaction with varying unknown (in terms of unknown stiffness and unknown geometric) environment. In the proposed method, a systematic online adaptation mechanism using the wavelet neural network is presented to adapt the impedance parameter according to a variable environment. Using the introduced adaptive law, the robot would be able to track the desired force on the moving environment with the unknown stiffness. Unlike the general impedance control which the position and stiffness of the environment need to be available and known for choosing the impedance parameters, the proposed structure for the impedance equation leads to adapt the impedance parameters according to the interaction environment. In addition, the stability conditions and adaptation laws using Lyapunov’s method for the variable impedance are given to guarantee the force tracking and stability of the closed-loop system. Finally, various numerical and experimental results verify the performance of the proposed adaptive approach. The experimental results strongly prove that the presented method has a better force tracking performance than the general impedance with constant parameters.},
  langid = {english},
  keywords = {Adaptive wavelet neural network,Dynamic contact force tracking,Force tracking,Intelligent impedance control,KUKA iiwa manipulator,READ,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/hamedani_et_al_2021_intelligent_impedance_control_using_wavelet_neural_network_for_dynamic_contact.pdf}
}

@article{hamedaniRecurrentFuzzyWavelet2021,
  title = {Recurrent Fuzzy Wavelet Neural Network Variable Impedance Control of Robotic Manipulators with Fuzzy Gain Dynamic Surface in an Unknown Varied Environment},
  author = {Hamedani, Mohammad Hossein and Zekri, Maryam and Sheikholeslam, Farid and Selvaggio, Mario and Ficuciello, Fanny and Siciliano, Bruno},
  date = {2021-07-30},
  journaltitle = {Fuzzy Sets and Systems},
  shortjournal = {Fuzzy Sets and Systems},
  series = {Systems {{Engineering}}},
  volume = {416},
  pages = {1--26},
  issn = {0165-0114},
  doi = {10.1016/j.fss.2020.05.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0165011420301780},
  urldate = {2022-04-28},
  abstract = {In this paper, an intelligent variable impedance control combined with a fuzzy gain dynamic surface is proposed to improve the interaction of the robot manipulator with an unknown varied environment. The parameters of the proposed variable impedance are adapted by optimization an introduced cost function using a recurrent fuzzy wavelet network. The stability conditions for the varying inertial, stiffness and damping are presented to guarantee the stability of the variable impedance. Additionally, a fuzzy dynamic surface method is developed to tune the gains of the dynamic surface as a robust controller. The proposed fuzzy gain dynamic surface is used to force the end-effector of the manipulator to track the desired impedance profile in the presence of large disturbances. Using Lyapunov's method, the stability of the mentioned closed-loop system is proved. Finally, by using a designed simulator for IRB120 (ABB) robot, several simulations are carried out to verify the performance of the proposed method for the execution of various tasks in an unknown varied environment in the presence of large disturbances.},
  langid = {english},
  keywords = {Fuzzy dynamic surface control,Intelligent impedance control,READ,Recurrent fuzzy wavelet neural networks,Robotic manipulator,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/hamedani_et_al_2021_recurrent_fuzzy_wavelet_neural_network_variable_impedance_control_of_robotic.pdf}
}

@article{hanActorCriticReinforcementLearning2020,
  title = {Actor-{{Critic Reinforcement Learning}} for {{Control With Stability Guarantee}}},
  author = {Han, Minghao and Zhang, Lixian and Wang, Jun and Pan, Wei},
  date = {2020-10},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {5},
  number = {4},
  pages = {6217--6224},
  issn = {2377-3766},
  doi = {10.1109/LRA.2020.3011351},
  abstract = {Reinforcement Learning (RL) and its integration with deep learning have achieved impressive performance in various robotic control tasks, ranging from motion planning and navigation to end-to-end visual manipulation. However, stability is not guaranteed in model-free RL by solely using data. From a control-theoretic perspective, stability is the most important property for any control system, since it is closely related to safety, robustness, and reliability of robotic systems. In this letter, we propose an actor-critic RL framework for control which can guarantee closed-loop stability by employing the classic Lyapunov's method in control theory. First of all, a data-based stability theorem is proposed for stochastic nonlinear systems modeled by Markov decision process. Then we show that the stability condition could be exploited as the critic in the actor-critic RL to learn a controller/policy. At last, the effectiveness of our approach is evaluated on several well-known 3-dimensional robot control tasks and a synthetic biology gene network tracking task in three different popular physics simulation platforms. As an empirical evaluation on the advantage of stability, we show that the learned policies can enable the systems to recover to the equilibrium or way-points when interfered by uncertainties such as system parametric variations and external disturbances to a certain extent.},
  eventtitle = {{{IEEE Robotics}} and {{Automation Letters}}},
  keywords = {Asymptotic stability,Control systems,IMPORTANT,Lyapunov methods,lyapunov's method,Reinforcement learning,Robots,stability,STABILITY,Stability criteria,Task analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/Han et al_2020_Actor-Critic Reinforcement Learning for Control With Stability Guarantee.pdf;/home/ricks/Zotero/storage/5UKYERX4/9146733.html}
}

@book{hangosAnalysisControlNonlinear2004,
  title = {Analysis and Control of Nonlinear Process Systems},
  author = {Hangos, Katalin M. and Bokor, József and Szederkényi, Gábor},
  date = {2004},
  volume = {13},
  publisher = {{Springer}},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/hangos_et_al_2004_analysis_and_control_of_nonlinear_process_systems.pdf}
}

@article{hanLyapunovActorCriticReinforcement2019,
  title = {Lyapunov {{Actor-Critic Reinforcement Learning}} with {{Uniformly Ultimate Boundedness Stability Guarantee}}},
  author = {Han, Minghao},
  date = {2019},
  url = {https://arxiv.org/abs/2004.14288},
  keywords = {#nosource,LAC,reinforcement learning}
}

@thesis{hanModelFreeReinforcement2019,
  title = {Model {{Free Reinforcement Learning}} with {{Stability Guarantee}}},
  author = {Han, Minghao},
  date = {2019-12-08},
  institution = {{Tu Delft}},
  keywords = {#nosource,LAC,reinforcement learning,thesis},
  file = {C:\Users\ricks\OneDrive\Education\Master\Graduation\Thesis\Resources\thesis_Han.pdf}
}

@article{hanModifiedDynamicMovement2022,
  title = {Modified {{Dynamic Movement Primitives}}: {{Robot Trajectory Planning}} and {{Force Control Under Curved Surface Constraints}}},
  shorttitle = {Modified {{Dynamic Movement Primitives}}},
  author = {Han, Liang and Yuan, Han and Xu, Wenfu and Huang, Yunzhi},
  date = {2022},
  journaltitle = {IEEE Transactions on Cybernetics},
  pages = {1--14},
  issn = {2168-2275},
  doi = {10.1109/TCYB.2022.3158029},
  abstract = {Dynamic movement primitives (DMPs) have been widely applied in robot motion planning and control. However, in some special cases, original discrete DMP fails to generalize proper trajectories. Moreover, it is difficult to produce trajectories on the curved surface. To solve the above problems, a modified DMP method is proposed for robot control by adding the scaling factor and force coupling term. First, the adjusted cosine similarity is defined to assess the similarity of the generalized trajectory with respect to the demonstrated trajectory. By optimizing the similarity, the trajectories can be generated in all situations. Next, by adding the force coupling term derived from adaptive admittance control to the transformation system of the original DMP, the controller achieves the force control ability. Then, the modified DMP-based robot control system is developed. The stability and convergence of the system are proved. Finally, the high precisions of the proposed method are verified by simulations and experiments. The method is significant for trajectory learning and generalization on the curved surface.},
  eventtitle = {{{IEEE Transactions}} on {{Cybernetics}}},
  keywords = {Adaptive force control,curved surface,Force,Force control,Impedance,modified dynamic movement primitive (DMP),READ,redundant robot,Robots,Task analysis,Trajectory,trajectory planning,Trajectory planning},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/han_et_al_2022_modified_dynamic_movement_primitives.pdf}
}

@inproceedings{hannafordTimeDomainPassivity2001,
  title = {Time Domain Passivity Control of Haptic Interfaces},
  booktitle = {Proceedings 2001 {{ICRA}}. {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{Cat}}. {{No}}.{{01CH37164}})},
  author = {Hannaford, B. and Ryu, Jee-Hwan},
  date = {2001-05},
  volume = {2},
  pages = {1863-1869 vol.2},
  issn = {1050-4729},
  doi = {10.1109/ROBOT.2001.932880},
  abstract = {An energy-based method is presented for controlling a haptic interface system to ensure stable contact under a wide variety of operating conditions. System stability is analyzed in terms of the time-domain definition of passivity. We define a "passivity observer" (PO) which measures energy flow in and out of one or more subsystems in real-time software. Active behavior is indicated by a negative value of the PO at any time. We also define the "passivity controller" (PC), an adaptive dissipative element which, at each time sample, absorbs exactly the net energy output (if any) measured by the PO. The method is tested with simulation and implementation in the "Excalibur" haptic interface system. Totally stable operation was achieved under conditions such as stiffness {$>$}100 N/mm or time delays of 15 ms. The PO/PC method requires very little additional computation and does not require a dynamical model to be identified.},
  eventtitle = {Proceedings 2001 {{ICRA}}. {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{Cat}}. {{No}}.{{01CH37164}})},
  keywords = {Adaptive control,Control systems,Energy measurement,Fluid flow measurement,Haptic interfaces,Programmable control,Software measurement,Stability analysis,Time domain analysis,Time measurement},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/hannaford_ryu_2001_time_domain_passivity_control_of_haptic_interfaces.pdf}
}

@article{hannafordTimedomainPassivityControl2002,
  title = {Time-Domain Passivity Control of Haptic Interfaces},
  author = {Hannaford, B. and Ryu, Jee-Hwan},
  date = {2002-02},
  journaltitle = {IEEE Transactions on Robotics and Automation},
  volume = {18},
  number = {1},
  pages = {1--10},
  issn = {2374-958X},
  doi = {10.1109/70.988969},
  abstract = {A patent-pending, energy-based method is presented for controlling a haptic interface system to ensure stable contact under a wide variety of operating conditions. System stability is analyzed in terms of the time-domain definition of passivity. We define a "passivity observer" (PO) which measures energy flow in and out of one or more subsystems in real-time software. Active behavior is indicated by a negative value of the PO at any time. We also define the "passivity controller" (PC), an adaptive dissipative element which, at each time sample, absorbs exactly the net energy output (if any) measured by the PO. The method is tested with simulation and implementation in the Excalibur haptic interface system. Totally stable operation was achieved under conditions such as stiffness {$>$}100 N/mm or time delays of 15 ms. The PO/PC method requires very little additional computation and does not require a dynamical model to be identified.},
  eventtitle = {{{IEEE Transactions}} on {{Robotics}} and {{Automation}}},
  keywords = {Adaptive control,Control systems,Energy measurement,Fluid flow measurement,Haptic interfaces,Programmable control,Software measurement,Stability analysis,Time domain analysis,Time measurement},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/hannaford_ryu_2002_time-domain_passivity_control_of_haptic_interfaces.pdf}
}

@unpublished{haRecurrentWorldModels2018,
  title = {Recurrent {{World Models Facilitate Policy Evolution}}},
  author = {Ha, David and Schmidhuber, Jürgen},
  date = {2018-09-04},
  eprint = {1809.01999},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1809.01999},
  urldate = {2020-07-02},
  abstract = {A generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments through compressed spatio-temporal representations. The world model's extracted features are fed into compact and simple policies trained by evolution, achieving state of the art results in various environments. We also train our agent entirely inside of an environment generated by its own internal world model, and transfer this policy back into the actual environment. Interactive version of paper at https://worldmodels.github.io},
  keywords = {#nosource,Computer Science - Machine Learning,learned model,model-based,reinforcement learning,Statistics - Machine Learning}
}

@article{haseliTemporalForwardBackward2023,
  title = {Temporal {{Forward}}–{{Backward Consistency}}, {{Not Residual Error}}, {{Measures}} the {{Prediction Accuracy}} of {{Extended Dynamic Mode Decomposition}}},
  author = {Haseli, Masih and Cortes, Jorge},
  date = {2023},
  journaltitle = {IEEE Control Systems Letters},
  shortjournal = {IEEE Control Syst. Lett.},
  volume = {7},
  pages = {649--654},
  issn = {2475-1456},
  doi = {10.1109/LCSYS.2022.3214476},
  url = {https://ieeexplore.ieee.org/document/9917469/},
  urldate = {2023-01-19},
  abstract = {Extended Dynamic Mode Decomposition (EDMD) is a popular data-driven method to approximate the action of the Koopman operator on a linear function space spanned by a dictionary of functions. The accuracy of EDMD model critically depends on the quality of the particular dictionary span1, specifically on how close it is to being invariant under the Koopman operator. Motivated by the observation that the residual error of EDMD, typically used for dictionary learning, does not encode the quality of the function space and is sensitive to the choice of basis, we introduce the novel concept of consistency index. We show that this measure, based on using EDMD forward and backward in time, enjoys a number of desirable qualities that make it suitable for data-driven modeling of dynamical systems: it measures the quality of the function space, it is invariant under the choice of basis, can be computed in closed form from the data, and provides a tight upper-bound for the relative root mean square error of all function predictions on the entire span of the dictionary.},
  langid = {english},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/haseli_cortes_2023_temporal_forward–backward_consistency,_not_residual_error,_measures_the.pdf}
}

@unpublished{hausknechtDeepRecurrentQLearning2017,
  title = {Deep {{Recurrent Q-Learning}} for {{Partially Observable MDPs}}},
  author = {Hausknecht, Matthew and Stone, Peter},
  date = {2017-01-11},
  eprint = {1507.06527},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1507.06527},
  urldate = {2020-07-02},
  abstract = {Deep Reinforcement Learning has yielded proficient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM. The resulting \textbackslash textit\{Deep Recurrent Q-Network\} (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens. Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN's performance scales as a function of observability. Conversely, when trained with full observations and evaluated with partial observations, DRQN's performance degrades less than DQN's. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.},
  keywords = {#nosource,Computer Science - Machine Learning,DQL,model-free,reinforcement learning},
  annotation = {00644}
}

@inproceedings{hausmanLearningEmbeddingSpace2018,
  title = {Learning an {{Embedding Space}} for {{Transferable Robot Skills}}},
  author = {Hausman, Karol and Springenberg, Jost Tobias and Wang, Ziyu and Heess, Nicolas and Riedmiller, Martin},
  date = {2018-02-15},
  url = {https://openreview.net/forum?id=rk07ZXZRb&noteId=rk07ZXZRb},
  urldate = {2020-07-02},
  abstract = {We present a method for reinforcement learning of closely related skills that are parameterized via a skill embedding space. We learn such skills by taking advantage of latent variables and...},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  keywords = {#nosource,multitask RL,reinforcement learning,transfer learning},
  annotation = {00087}
}

@inproceedings{havensImitationLearningLinear2021,
  title = {On Imitation Learning of Linear Control Policies: {{Enforcing}} Stability and Robustness Constraints via Lmi Conditions},
  shorttitle = {On Imitation Learning of Linear Control Policies},
  booktitle = {2021 {{American Control Conference}} ({{ACC}})},
  author = {Havens, Aaron and Hu, Bin},
  date = {2021},
  pages = {882--887},
  publisher = {{IEEE}}
}

@inproceedings{hazaraReinforcementLearningImproving2016,
  title = {Reinforcement Learning for Improving Imitated In-Contact Skills},
  booktitle = {2016 {{IEEE-RAS}} 16th {{International Conference}} on {{Humanoid Robots}} ({{Humanoids}})},
  author = {Hazara, Murtaza and Kyrki, Ville},
  date = {2016},
  pages = {194--201},
  publisher = {{IEEE}},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/hazara_kyrki_2016_reinforcement_learning_for_improving_imitated_in-contact_skills.pdf}
}

@article{heAdaptiveFuzzyNeural2018,
  title = {Adaptive {{Fuzzy Neural Network Control}} for a {{Constrained Robot Using Impedance Learning}}},
  author = {He, Wei and Dong, Yiting},
  date = {2018-04},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {29},
  number = {4},
  pages = {1174--1186},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2017.2665581},
  url = {http://ieeexplore.ieee.org/document/7865986/},
  urldate = {2019-04-18},
  abstract = {This paper investigates adaptive fuzzy neural network (NN) control using impedance learning for a constrained robot, subject to unknown system dynamics, the effect of state constraints, and the uncertain compliant environment with which the robot comes into contact. A fuzzy NN learning algorithm is developed to identify the uncertain plant model. The prominent feature of the fuzzy NN is that there is no need to get the prior knowledge about the uncertainty and a sufficient amount of observed data. Also, impedance learning is introduced to tackle the interaction between the robot and its environment, so that the robot follows a desired destination generated by impedance learning. A barrier Lyapunov function is used to address the effect of state constraints. With the proposed control, the stability of the closed-loop system is achieved via Lyapunov’s stability theory, and the tracking performance is guaranteed under the condition of state constraints and uncertainty. Some simulation studies are carried out to illustrate the effectiveness of the proposed scheme.},
  langid = {english},
  keywords = {#nosource,Adaptive control,Adaptive systems,Artificial neural networks,constraint,Force,fuzzy logic control,Impedance,impedance learning,machine learning control,neural networks (NN),robot,Robots,safe-rl,Stability analysis,Uncertainty},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/he_dong_2018_adaptive_fuzzy_neural_network_control_for_a_constrained_robot_using_impedance.pdf}
}

@article{heAdaptiveNeuralImpedance2015,
  title = {Adaptive Neural Impedance Control of a Robotic Manipulator with Input Saturation},
  author = {He, Wei and Dong, Yiting and Sun, Changyin},
  date = {2015},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  volume = {46},
  number = {3},
  pages = {334--344},
  publisher = {{IEEE}},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/he_et_al_2015_adaptive_neural_impedance_control_of_a_robotic_manipulator_with_input_saturation.pdf}
}

@article{heAdmittanceBasedControllerDesign2020,
  title = {Admittance-{{Based Controller Design}} for {{Physical Human}}–{{Robot Interaction}} in the {{Constrained Task Space}}},
  author = {He, Wei and Xue, Chengqian and Yu, Xinbo and Li, Zhijun and Yang, Chenguang},
  date = {2020-10},
  journaltitle = {IEEE Transactions on Automation Science and Engineering},
  volume = {17},
  number = {4},
  pages = {1937--1949},
  issn = {1558-3783},
  doi = {10.1109/TASE.2020.2983225},
  abstract = {In this article, an admittance-based controller for physical human-robot interaction (pHRI) is presented to perform the coordinated operation in the constrained task space. An admittance model and a soft saturation function are employed to generate a differentiable reference trajectory to ensure that the end-effector motion of the manipulator complies with the human operation and avoids collision with surroundings. Then, an adaptive neural network (NN) controller involving integral barrier Lyapunov function (IBLF) is designed to deal with tracking issues. Meanwhile, the controller can guarantee the end-effector of the manipulator limited in the constrained task space. A learning method based on the radial basis function NN (RBFNN) is involved in controller design to compensate for the dynamic uncertainties and improve tracking performance. The IBLF method is provided to prevent violations of the constrained task space. We prove that all states of the closed-loop system are semiglobally uniformly ultimately bounded (SGUUB) by utilizing the Lyapunov stability principles. At last, the effectiveness of the proposed algorithm is verified on a Baxter robot experiment platform. Note to Practitioners-This work is motivated by the neglect of safety in existing controller design in physical human-robot interaction (pHRI), which exists in industry and services, such as assembly and medical care. It is considerably required in the controller design for rigorously handling constraints. Therefore, in this article, we propose a novel admittance-based human-robot interaction controller. The developed controller has the following functionalities: 1) ensuring reference trajectory remaining in the constrained task space: a differentiable reference trajectory is shaped by the desired admittance model and a soft saturation function; 2) solving uncertainties of robotic dynamics: a learning approach based on radial basis function neural network (RBFNN) is involved in controller design; and 3) ensuring the end-effector of the manipulator remaining in the constrained task space: different from other barrier Lyapunov function (BLF), integral BLF (IBLF) is proposed to constrain system output directly rather than tracking error, which may be more convenient for controller designers. The controller can be potentially applied in many areas. First, it can be used in the rehabilitation robot to avoid injuring the patient by limiting the motion. Second, it can ensure the end-effector of the industrial manipulator in a prescribed task region. In some industrial tasks, dangerous or damageable tools are mounted on the end-effector, and it will hurt humans and bring damage to the robot when the end-effector is out of the prescribed task region. Third, it may bring a new idea to the designed controller for avoiding collisions in pHRI when collisions occur in the prescribed trajectory of end-effector.},
  eventtitle = {{{IEEE Transactions}} on {{Automation Science}} and {{Engineering}}},
  keywords = {Adaptive neural network (NN) control,Admittance,admittance control,Human-robot interaction,integral barrier Lyapunov function (IBLF),Lyapunov methods,Manipulator dynamics,motion constraint,Neural networks,physical human–robot interaction (pHRI),Service robots},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/he_et_al_2020_admittance-based_controller_design_for_physical_human–robot_interaction_in_the.pdf}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep Residual Learning for Image Recognition},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2016},
  pages = {770--778},
  keywords = {#nosource,robotic grasping,tno internship},
  annotation = {20589}
}

@unpublished{heessEmergenceLocomotionBehaviours2017,
  title = {Emergence of {{Locomotion Behaviours}} in {{Rich Environments}}},
  author = {Heess, Nicolas and TB, Dhruva and Sriram, Srinivasan and Lemmon, Jay and Merel, Josh and Wayne, Greg and Tassa, Yuval and Erez, Tom and Wang, Ziyu and Eslami, S. M. Ali and Riedmiller, Martin and Silver, David},
  date = {2017-07-10},
  eprint = {1707.02286},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1707.02286},
  urldate = {2020-07-02},
  abstract = {The reinforcement learning paradigm allows, in principle, for complex behaviours to be learned directly from simple reward signals. In practice, however, it is common to carefully hand-design the reward function to encourage a particular solution, or to derive it from demonstration data. In this paper explore how a rich environment can help to promote the learning of complex behavior. Specifically, we train agents in diverse environmental contexts, and find that this encourages the emergence of robust behaviours that perform well across a suite of tasks. We demonstrate this principle for locomotion -- behaviours that are known for their sensitivity to the choice of reward. We train several simulated bodies on a diverse set of challenging terrains and obstacles, using a simple reward function based on forward progress. Using a novel scalable variant of policy gradient reinforcement learning, our agents learn to run, jump, crouch and turn as required by the environment without explicit reward-based guidance. A visual depiction of highlights of the learned behavior can be viewed following https://youtu.be/hx\_bgoTF7bs .},
  keywords = {#nosource,Computer Science - Artificial Intelligence,model-free,policy gradients,reinforcement learning},
  annotation = {00000}
}

@unpublished{hendersonDeepReinforcementLearning2019,
  title = {Deep {{Reinforcement Learning}} That {{Matters}}},
  author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  date = {2019-01-29},
  eprint = {1709.06560},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1709.06560},
  urldate = {2020-07-02},
  abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
  keywords = {#nosource,benchmarks,Computer Science - Machine Learning,reinforcement learning,reproducibility,Statistics - Machine Learning},
  annotation = {00571}
}

@online{hendersonDeepReinforcementLearning2019a,
  title = {Deep {{Reinforcement Learning}} That {{Matters}}},
  author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  date = {2019-01-29},
  eprint = {1709.06560},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1709.06560},
  urldate = {2024-03-01},
  abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/Henderson et al_2019_Deep Reinforcement Learning that Matters.pdf}
}

@unpublished{hendersonWhereDidMy2018,
  title = {Where {{Did My Optimum Go}}?: {{An Empirical Analysis}} of {{Gradient Descent Optimization}} in {{Policy Gradient Methods}}},
  shorttitle = {Where {{Did My Optimum Go}}?},
  author = {Henderson, Peter and Romoff, Joshua and Pineau, Joelle},
  date = {2018-10-05},
  eprint = {1810.02525},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1810.02525},
  urldate = {2020-07-02},
  abstract = {Recent analyses of certain gradient descent optimization methods have shown that performance can degrade in some settings - such as with stochasticity or implicit momentum. In deep reinforcement learning (Deep RL), such optimization methods are often used for training neural networks via the temporal difference error or policy gradient. As an agent improves over time, the optimization target changes and thus the loss landscape (and local optima) change. Due to the failure modes of those methods, the ideal choice of optimizer for Deep RL remains unclear. As such, we provide an empirical analysis of the effects that a wide range of gradient descent optimizers and their hyperparameters have on policy gradient methods, a subset of Deep RL algorithms, for benchmark continuous control tasks. We find that adaptive optimizers have a narrow window of effective learning rates, diverging in other cases, and that the effectiveness of momentum varies depending on the properties of the environment. Our analysis suggests that there is significant interplay between the dynamics of the environment and Deep RL algorithm properties which aren't necessarily accounted for by traditional adaptive gradient methods. We provide suggestions for optimal settings of current methods and further lines of research based on our findings.},
  keywords = {#nosource,analysis,benchmarks,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,reinforcement learning,Statistics - Machine Learning},
  annotation = {00006}
}

@unpublished{hesselRainbowCombiningImprovements2017,
  title = {Rainbow: {{Combining Improvements}} in {{Deep Reinforcement Learning}}},
  shorttitle = {Rainbow},
  author = {Hessel, Matteo and Modayil, Joseph and family=Hasselt, given=Hado, prefix=van, useprefix=true and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  date = {2017-10-06},
  eprint = {1710.02298},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1710.02298},
  urldate = {2020-07-02},
  abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,DQL,model-free,reinforcement learning},
  annotation = {00000}
}

@article{heVariableImpedanceControl2020,
  title = {Variable {{Impedance Control}} of {{Cable Actuated Continuum Manipulators}}},
  author = {He, Guangping and Fan, Yanan and Su, Tingting and Zhao, Lei and Zhao, Quanliang},
  date = {2020-07-01},
  journaltitle = {International Journal of Control, Automation and Systems},
  shortjournal = {Int. J. Control Autom. Syst.},
  volume = {18},
  number = {7},
  pages = {1839--1852},
  issn = {2005-4092},
  doi = {10.1007/s12555-019-0449-y},
  url = {https://doi.org/10.1007/s12555-019-0449-y},
  urldate = {2022-08-04},
  abstract = {Continuum manipulators are a class of special compliant robots that have important potential applications in the field of human-machine interactive operations, or work in cluttered and constrained environments. In these application scenarios, the most popular operation tasks are those with coupling force-position constraints. To simultaneously stabilize the desired operation force and the position of the manipulator, variable impedance control issues of the cable driven continuum manipulators are investigated in this paper. On the basis of constructing a novel Lyapunov function, a variable impedance control law is presented and the stability of the closed-loop system has also been analyzed. Then the operation space variable impedance control for a single segment cable driven continuum manipulator is realized by the aid of a pseudo-rigid-body model. Some numerical simulations also demonstrate the stability of the variable impedance control system.},
  langid = {english},
  keywords = {Continuum manipulators,control,READ,robots,stability,variable impedance},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/he_et_al_2020_variable_impedance_control_of_cable_actuated_continuum_manipulators.pdf}
}

@article{HighorderControlBarrier2022,
  title = {High-Order Control Barrier Functions-Based Impedance Control of a Robotic Manipulator with Time-Varying Output Constraints},
  date = {2022-02-15},
  journaltitle = {ISA Transactions},
  publisher = {{Elsevier}},
  issn = {0019-0578},
  doi = {10.1016/j.isatra.2022.02.013},
  url = {http://www.sciencedirect.com/science/article/pii/S0019057822000726},
  urldate = {2022-08-13},
  abstract = {This paper focuses on the impedance control for robotic manipulators with time-varying output constraints. High-order control barrier functions (HoCBF…},
  langid = {english},
  keywords = {High-order control barrier function,Impedance control,Quadratic program,Robotic manipulator,Time-varying output constraints},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/wang_et_al_2022_high-order_control_barrier_functions-based_impedance_control_of_a_robotic.pdf}
}

@incollection{hillierIdealMHDInstabilities2020,
  title = {Ideal {{MHD Instabilities}}, with a {{Focus}} on the {{Rayleigh}}–{{Taylor}} and {{Kelvin}}–{{Helmholtz Instabilities}}},
  booktitle = {Topics in {{Magnetohydrodynamic Topology}}, {{Reconnection}} and {{Stability Theory}}},
  author = {Hillier, Andrew},
  editor = {MacTaggart, David and Hillier, Andrew},
  date = {2020},
  series = {{{CISM International Centre}} for {{Mechanical Sciences}}},
  pages = {1--36},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-16343-3_1},
  url = {https://doi.org/10.1007/978-3-030-16343-3_1},
  urldate = {2022-09-14},
  abstract = {In this chapter we focus on the magnetohydrodynamic (MHD) versions of the Rayleigh–Taylor and Kelvin–Helmholtz instabilities, taking the reader beyond the commonly presented situations to include how extra physics influences the stability of the models. After a discussion of the physical processes behind each instability we look at the general framework behind the study of ideal MHD instabilities, providing a detailed look at the derivation of the dispersion relation for a simple model. Extensions to this model are presented, including an investigation into how stability changes in the presence of a time-varying flow. Finally, we take a look at how nonlinearities develop and the role of the MHD in terms of the development of these nonlinearities.},
  isbn = {978-3-030-16343-3},
  langid = {english},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/hillier_2020_ideal_mhd_instabilities,_with_a_focus_on_the_rayleigh–taylor_and.pdf}
}

@article{hoganContactPhysicalInteraction2022,
  title = {Contact and {{Physical Interaction}}},
  author = {Hogan, Neville},
  date = {2022},
  journaltitle = {Annual Review of Control, Robotics, and Autonomous Systems},
  volume = {5},
  pages = {179--203},
  publisher = {{Annual Reviews}},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/hogan_2022_contact_and_physical_interaction.pdf}
}

@article{hoganImpedanceControlApproach1985,
  title = {Impedance Control: {{An}} Approach to Manipulation: {{Part II}}—{{Implementation}}},
  shorttitle = {Impedance Control},
  author = {Hogan, Neville},
  date = {1985},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/hogan_1985_impedance_control.pdf}
}

@article{hoganStabilityManipulatorsPerforming1988,
  title = {On the Stability of Manipulators Performing Contact Tasks},
  author = {Hogan, N.},
  date = {1988-12},
  journaltitle = {IEEE Journal on Robotics and Automation},
  volume = {4},
  number = {6},
  pages = {677--686},
  issn = {2374-8710},
  doi = {10.1109/56.9305},
  abstract = {Manipulation requires contact with the object being manipulated, and the full potential of robots can only be realized when they are applied to contact tasks. One of the difficulties engendered by contact tasks is that they require intimate dynamic interaction between the robot and its environment. That interaction changes the performance of the robot and can jeopardize the stability of its control system. A discussion is presented of the problem of preserving the stability of a manipulator's control system during contact tasks. It will be shown that contact stability may be guaranteed if the control system provides the manipulator with an appropriately structured dynamic response to environmental inputs. Two aspects of one implementation of such a controller will be considered. Robustness to large errors in the manipulator kinematic equations and to unmodeled interface dynamics is shown.{$<>$}},
  eventtitle = {{{IEEE Journal}} on {{Robotics}} and {{Automation}}},
  keywords = {Automatic control,Control systems,Frequency,Kinetic energy,Laplace equations,Manipulator dynamics,Robot kinematics,Robust stability,Robustness,STABILITY,Tensile stress},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/hogan_1988_on_the_stability_of_manipulators_performing_contact_tasks.pdf}
}

@inproceedings{hoganStableExecutionContact1987,
  title = {Stable Execution of Contact Tasks Using Impedance Control},
  booktitle = {1987 {{IEEE International Conference}} on {{Robotics}} and {{Automation Proceedings}}},
  author = {Hogan, N.},
  date = {1987-03},
  volume = {4},
  pages = {1047--1054},
  doi = {10.1109/ROBOT.1987.1087854},
  abstract = {This paper presents an experimental evaluation of the performance of a nonlinear robot control algorithm on a contact task involving free motion, constrained motion and transitions between the two. The algorithm is an implementation of impedance control which uses end-point force feedback. Stable control of the force exerted on a rigid surface is achieved without recourse to a soft sensor. Motion control is achieved without inverse kinematic computations. It is unnecessary to switch between different modes of control at the moment of contact as the impedance controller is competent in all phases of the task.},
  eventtitle = {1987 {{IEEE International Conference}} on {{Robotics}} and {{Automation Proceedings}}},
  keywords = {Contacts,Force control,Force feedback,Force sensors,Motion control,Orbital robotics,Robot control,Robot sensing systems,Surface impedance,Switches},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/hogan_1987_stable_execution_of_contact_tasks_using_impedance_control.pdf}
}

@unpublished{hoGenerativeAdversarialImitation2016,
  title = {Generative {{Adversarial Imitation Learning}}},
  author = {Ho, Jonathan and Ermon, Stefano},
  date = {2016-06-10},
  eprint = {1606.03476},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1606.03476},
  urldate = {2020-07-02},
  abstract = {Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,imitation learning,inverse RL learning,reinforcement learning},
  annotation = {00790}
}

@article{houFuzzyLogicDrivenVariable2022,
  title = {Fuzzy {{Logic-Driven Variable Time-Scale Prediction-Based Reinforcement Learning}} for {{Robotic Multiple Peg-in-Hole Assembly}}},
  author = {Hou, Zhimin and Li, Zhihu and Hsu, Chenwei and Zhang, Kuangen and Xu, Jing},
  date = {2022-01},
  journaltitle = {IEEE Transactions on Automation Science and Engineering},
  volume = {19},
  number = {1},
  pages = {218--229},
  issn = {1558-3783},
  doi = {10.1109/TASE.2020.3024725},
  abstract = {Reinforcement learning (RL) has been increasingly used for single peg-in-hole assembly, where assembly skill is learned through interaction with the assembly environment in a manner similar to skills employed by human beings. However, the existing RL algorithms are difficult to apply to the multiple peg-in-hole assembly because the much more complicated assembly environment requires sufficient exploration, resulting in a long training time and less data efficiency. To this end, this article focuses on how to predict the assembly environment and how to use the predicted environment in assembly action control to improve the data efficiency of the RL algorithm. Specifically, first, the assembly environment is exactly predicted by a variable time-scale prediction (VTSP) defined as general value functions (GVFs), reducing the unnecessary exploration. Second, we propose a fuzzy logic-driven variable time-scale prediction-based reinforcement learning (FLDVTSP-RL) for assembly action control to improve the efficiency of the RL algorithm, in which the predicted environment is mapped to the impedance parameter in the proposed impedance action space by a fuzzy logic system (FLS) as the action baseline. To demonstrate the effectiveness of VTSP and the data efficiency of the FLDVTSP-RL methods, a dual peg-in-hole assembly experiment is set up; the results show that FLDVTSP-deep Q-learning (DQN) decreases the assembly time about 44\% compared with DQN and FLDVTSP-deep deterministic policy gradient (DDPG) decreases the assembly time about 24\% compared with DDPG. Note to Practitioners—The complicated assembly environment of the multiple peg-in-hole assembly results in a contact state that cannot be recognized exactly from the force sensor. Therefore, contact-model-based methods that require tuning of the control parameters based on the contact state recognition cannot be applied directly in this complicated environment. Recently, reinforcement learning (RL) methods without contact state recognition have recently attracted scientific interest. However, the existing RL methods still rely on numerous explorations and a long training time, which cannot be directly applied to real-world tasks. This article takes inspiration from the manner in which human beings can learn assembly skills with a few trials, which relies on the variable time-scale predictions (VTSPs) of the environment and the optimized assembly action control strategy. Our proposed fuzzy logic-driven variable time-scale prediction-based reinforcement learning (FLDVTSP-RL) can be implemented in two steps. First, the assembly environment is predicted by the VTSP defined as general value functions (GVFs). Second, assembly action control is realized in an impedance action space with a baseline defined by the impedance parameter mapped from the predicted environment by the fuzzy logic system (FLS). Finally, a dual peg-in-hole assembly experiment is conducted; compared with deep Q-learning (DQN), FLDVTSP-DQN can decrease the assembly time about 44\%; compared with deep deterministic policy gradient (DDPG), FLDVTSP-DDPG can decrease the assembly time about 24\%.},
  eventtitle = {{{IEEE Transactions}} on {{Automation Science}} and {{Engineering}}},
  keywords = {Force,Fuzzy logic,Fuzzy logic system (FLS),Impedance,multiple peg-in-hole,prediction learning,READ,reinforcement learning (RL),Robot sensing systems,robotic assembly,Robotic assembly,Task analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/hou_et_al_2022_fuzzy_logic-driven_variable_time-scale_prediction-based_reinforcement_learning.pdf}
}

@unpublished{houthooftVIMEVariationalInformation2017,
  title = {{{VIME}}: {{Variational Information Maximizing Exploration}}},
  shorttitle = {{{VIME}}},
  author = {Houthooft, Rein and Chen, Xi and Duan, Yan and Schulman, John and De Turck, Filip and Abbeel, Pieter},
  date = {2017-01-27},
  eprint = {1605.09674},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1605.09674},
  urldate = {2020-07-02},
  abstract = {Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as epsilon-greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,exploration,intrinsic motivation,reinforcement learning,Statistics - Machine Learning}
}

@inproceedings{houVariableImpedanceControl2020,
  title = {Variable {{Impedance Control}} of {{Manipulator Based}} on {{DQN}}},
  booktitle = {Intelligent {{Robotics}} and {{Applications}}},
  author = {Hou, Yongjin and Xu, Hao and Luo, Jiawei and Lei, Yanpu and Xu, Jinyu and Zhang, Hai-Tao},
  editor = {Chan, Chee Seng and Liu, Hong and Zhu, Xiangyang and Lim, Chern Hong and Liu, Xinjun and Liu, Lianqing and Goh, Kam Meng},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {296--307},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-66645-3_25},
  abstract = {For traditional constant impedance control, the robot suffers from constant stiffness, poor flexibility, large wear and high energy consumption in the process of movement. To address these problems, a variable impedance control method based on reinforcement learning (RL) algorithm Deep Q Network (DQN) is proposed in this paper. Our method can optimize the reference trajectory and gain schedule simultaneously according to the completion of task and the complexity of surroundings. Simulation experiments show that, compared with the constant impedance control, the proposed algorithm can adjust impedance in real time while manipulator is executing the task, which implies a better compliance, less wear and less control energy.},
  isbn = {978-3-030-66645-3},
  langid = {english},
  keywords = {Compliance,Control energy,Manipulator,Reinforcement learning,Variable impedance control},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/hou_et_al_2020_variable_impedance_control_of_manipulator_based_on_dqn.pdf}
}

@article{huaLearningRobotDeep2021,
  title = {Learning for a {{Robot}}: {{Deep Reinforcement Learning}}, {{Imitation Learning}}, {{Transfer Learning}}},
  shorttitle = {Learning for a {{Robot}}},
  author = {Hua, Jiang and Zeng, Liangcai and Li, Gongfa and Ju, Zhaojie},
  date = {2021-01},
  journaltitle = {Sensors},
  volume = {21},
  number = {4},
  pages = {1278},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1424-8220},
  doi = {10.3390/s21041278},
  url = {https://www.mdpi.com/1424-8220/21/4/1278},
  urldate = {2022-09-09},
  abstract = {Dexterous manipulation of the robot is an important part of realizing intelligence, but manipulators can only perform simple tasks such as sorting and packing in a structured environment. In view of the existing problem, this paper presents a state-of-the-art survey on an intelligent robot with the capability of autonomous deciding and learning. The paper first reviews the main achievements and research of the robot, which were mainly based on the breakthrough of automatic control and hardware in mechanics. With the evolution of artificial intelligence, many pieces of research have made further progresses in adaptive and robust control. The survey reveals that the latest research in deep learning and reinforcement learning has paved the way for highly complex tasks to be performed by robots. Furthermore, deep reinforcement learning, imitation learning, and transfer learning in robot control are discussed in detail. Finally, major achievements based on these methods are summarized and analyzed thoroughly, and future research challenges are proposed.},
  issue = {4},
  langid = {english},
  keywords = {adaptive and robust control,deep reinforcement learning,dexterous manipulation,imitation learning,REVIEW,transfer learning},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/hua_et_al_2021_learning_for_a_robot.pdf}
}

@inproceedings{huangBroadFuzzyNeural2019,
  title = {Broad {{Fuzzy Neural Control Using Impedance Learning}}},
  booktitle = {2019 {{IEEE}} 4th {{International Conference}} on {{Advanced Robotics}} and {{Mechatronics}} ({{ICARM}})},
  author = {Huang, Haohui and Yang, Chenguang and Ju, Zhaojie and Yuan, Yuxia and Li, Zhijun},
  date = {2019-07},
  pages = {173--178},
  doi = {10.1109/ICARM.2019.8833648},
  abstract = {This work proposes a novel control strategy based on broad fuzzy neural network (BFNN) by using impedance learning, which is subjected to contact with the unknown dynamic environment. Compared with the original fuzzy neural network, this framework is provided the prominent feature by taking the advantage of broad learning system (BLS) to approximate the unknown dynamic model. Aiming at offering a compliance contact scheme, this paper introduce the impedance learning to establish the robot-environment interaction model. Also, a stable controller, which is able to tackle the problems related to the state constrain, is designed through Barrier Lyapunov Function (BLF). The proposed method can achieve the favourable tracking action while guaranteeing the stability of closed-loop system. In the end, simulation study is performed to verify the effectiveness of BFNN with a two-DOF manipulator.},
  eventtitle = {2019 {{IEEE}} 4th {{International Conference}} on {{Advanced Robotics}} and {{Mechatronics}} ({{ICARM}})},
  keywords = {Adaptation models,Force,Fuzzy control,Fuzzy neural networks,Impedance,Manipulators},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/huang_et_al_2019_broad_fuzzy_neural_control_using_impedance_learning.pdf}
}

@inproceedings{huangCompliantMotionAdaptation2020,
  title = {Compliant {{Motion Adaptation}} with {{Dynamical System}} during {{Robot-Environment Interaction}}},
  booktitle = {2020 {{IEEE}}/{{ASME International Conference}} on {{Advanced Intelligent Mechatronics}} ({{AIM}})},
  author = {Huang, Haohui and Yang, Chenguang and Su, Chun-Yi},
  date = {2020-07},
  pages = {2033--2038},
  issn = {2159-6255},
  doi = {10.1109/AIM43001.2020.9158894},
  abstract = {Compliant control ability is essential for physical robot-environment interaction. This paper presents a novel framework for robots to achieve active compliant behaviour through Dynamical System (DS). In contrast to the existing research works that aim to improve the compliant performance by introducing an appropriate impedance model or force control scheme, in our framework, no prerequisite for predefined robot impedance parameters or environmental model is required. The proposed method can generate a continuous and adaptive trajectory in terms of the contacting force by designing a globally asymptotically stable DS to guarantee a safe interaction with unknown environment. The method is not only strictly derived from theoretical study, but also verified by extensive simulations and experiments on different shapes of contacting object and different types of DS model.},
  eventtitle = {2020 {{IEEE}}/{{ASME International Conference}} on {{Advanced Intelligent Mechatronics}} ({{AIM}})},
  keywords = {Adaptation models,Force,Impedance,READ,Robots,Task analysis,Tracking,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/huang_et_al_2020_compliant_motion_adaptation_with_dynamical_system_during_robot-environment.pdf}
}

@online{huangFeedbackStabilizationUsing2018,
  title = {Feedback {{Stabilization Using Koopman Operator}}},
  author = {Huang, Bowen and Ma, Xu and Vaidya, Umesh},
  date = {2018-09-28},
  eprint = {1810.00089},
  eprinttype = {arxiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.1810.00089},
  url = {http://arxiv.org/abs/1810.00089},
  urldate = {2023-01-27},
  abstract = {In this paper, we provide a systematic approach for the design of stabilizing feedback controllers for nonlinear control systems using the Koopman operator framework. The Koopman operator approach provides a linear representation for a nonlinear dynamical system and a bilinear representation for a nonlinear control system. The problem of feedback stabilization of a nonlinear control system is then transformed to the stabilization of a bilinear control system. We propose a control Lyapunov function (CLF)-based approach for the design of stabilizing feedback controllers for the bilinear system. The search for finding a CLF for the bilinear control system is formulated as a convex optimization problem. This leads to a schematic procedure for designing CLF-based stabilizing feedback controllers for the bilinear system and hence the original nonlinear system. Another advantage of the proposed controller design approach outlined in this paper is that it does not require explicit knowledge of system dynamics. In particular, the bilinear representation of a nonlinear control system in the Koopman eigenfunction space can be obtained from time-series data. Simulation results are presented to verify the main results on the design of stabilizing feedback controllers and the data-driven aspect of the proposed approach.},
  pubstate = {preprint},
  keywords = {Mathematics - Optimization and Control,READ,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/huang_et_al_2018_feedback_stabilization_using_koopman_operator.pdf}
}

@article{huangRobustPassivityBasedDynamical2022,
  title = {Robust {{Passivity-Based Dynamical Systems}} for {{Compliant Motion Adaptation}}},
  author = {Huang, Haohui and Guo, Yi and Yang, Genke and Chu, Jian and Chen, Xinwei and Li, Zhibin and Yang, Chenguang},
  date = {2022},
  journaltitle = {IEEE/ASME Transactions on Mechatronics},
  pages = {1--10},
  issn = {1941-014X},
  doi = {10.1109/TMECH.2022.3166204},
  abstract = {Motivated by human compliant behaviors during interacting with unknown environments and how motions and impedance to are adapted skilfully complete a task, this article develops a motion planning scheme that is capable of generating a compliant trajectory online such that tracking desired contacting forces under a predefined motion task. First, an improved dynamical system (DS) is designed to generate an adaptive compliant scanning trajectory online from the original DS in terms of the contact forces and the desired scanning forces. Inspired by passivity analysis for the robot control system, a robust term is formulated to guarantee stability by considering the balance between environmental and robotic energy. Furthermore, we develop a state-constrained controller based on barrier Lyapunov function to track the compliant DS motion and to ensure safety during scanning for the patient. Finally, comparative simulations are conducted to validate the general compliant capability of the proposed framework. We also instantiate our methodology through a use case of liver ultrasound scanning to demonstrate the stable and dynamic force tracking performance.},
  eventtitle = {{{IEEE}}/{{ASME Transactions}} on {{Mechatronics}}},
  keywords = {Adaptation models,dynamical system,Force,laser tracker,motion adaptation,Motion control,passivity analysis,Physical robot-environment interaction,Planning,READ,Robots,Robustness,Task analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/huang_et_al_2022_robust_passivity-based_dynamical_systems_for_compliant_motion_adaptation.pdf}
}

@article{huFuzzyAdaptivePassive2023,
  title = {Fuzzy {{Adaptive Passive Control Strategy Design}} for {{Upper-Limb End-Effector Rehabilitation Robot}}},
  author = {Hu, Yang and Meng, Jingyan and Li, Guoning and Zhao, Dazheng and Feng, Guang and Zuo, Guokun and Liu, Yunfeng and Zhang, Jiaji and Shi, Changcheng},
  date = {2023-01},
  journaltitle = {Sensors},
  volume = {23},
  number = {8},
  pages = {4042},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1424-8220},
  doi = {10.3390/s23084042},
  url = {https://www.mdpi.com/1424-8220/23/8/4042},
  urldate = {2023-04-29},
  abstract = {Robot-assisted rehabilitation therapy has been proven to effectively improve upper-limb motor function in stroke patients. However, most current rehabilitation robotic controllers will provide too much assistance force and focus only on the patient’s position tracking performance while ignoring the patient’s interactive force situation, resulting in the inability to accurately assess the patient’s true motor intention and difficulty stimulating the patient’s initiative, thus negatively affecting the patient’s rehabilitation outcome. Therefore, this paper proposes a fuzzy adaptive passive (FAP) control strategy based on subjects’ task performance and impulse. To ensure the safety of subjects, a passive controller based on the potential field is designed to guide and assist patients in their movements, and the stability of the controller is demonstrated in a passive formalism. Then, using the subject’s task performance and impulse as evaluation indicators, fuzzy logic rules were designed and used as an evaluation algorithm to quantitively assess the subject’s motor ability and to adaptively modify the stiffness coefficient of the potential field and thus change the magnitude of the assistance force to stimulate the subject’s initiative. Through experiments, this control strategy has been shown to not only improve the subject’s initiative during the training process and ensure their safety during training but also enhance the subject’s motor learning ability.},
  issue = {8},
  langid = {english},
  keywords = {assist-as-needed,end-effector rehabilitation robot,fuzzy logic,human–robot interaction,potential field,READ,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/hu_et_al_2023_fuzzy_adaptive_passive_control_strategy_design_for_upper-limb_end-effector.pdf}
}

@article{hughesSoftManipulatorsGrippers2016,
  title = {Soft {{Manipulators}} and {{Grippers}}: {{A Review}}},
  shorttitle = {Soft {{Manipulators}} and {{Grippers}}},
  author = {Hughes, Josie and Culha, Utku and Giardina, Fabio and Guenther, Fabian and Rosendo, Andre and Iida, Fumiya},
  date = {2016},
  journaltitle = {Frontiers in Robotics and AI},
  volume = {3},
  issn = {2296-9144},
  url = {https://www.frontiersin.org/articles/10.3389/frobt.2016.00069},
  urldate = {2022-07-22},
  abstract = {Soft robotics is a growing area of research which utilizes the compliance and adaptability of soft structures to develop highly adaptive robotics for soft interactions. One area in which soft robotics has the ability to make significant impact is in the development of soft grippers and manipulators. With an increased requirement for automation, robotics systems are required to perform task in unstructured and not well defined environments; conditions which conventional rigid robotics are not best suited. This requires a paradigm shift in the methods and materials used to develop robots such that they can adapt to and work safely in human environments. One solution to this is soft robotics, which enables soft interactions with the surroundings while maintaining the ability to apply significant force. This review paper assesses the current materials and methods, actuation methods and sensors which are used in the development of soft manipulators. The achievements and shortcomings of recent technology in these key areas are evaluated, and this paper concludes with a discussion on the potential impacts of soft manipulators on industry and society.},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/hughes_et_al_2016_soft_manipulators_and_grippers.pdf}
}

@article{huImpedanceSlidingMode2020,
  title = {Impedance {{Sliding Mode Control With Adaptive Fuzzy Compensation}} for {{Robot-Environment Interacting}}},
  author = {Hu, Heyu and Wang, Xiaoqi and Chen, Lerui},
  date = {2020},
  journaltitle = {IEEE Access},
  volume = {8},
  pages = {19880--19889},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.2968954},
  abstract = {In the field of robot research and application, improving the interaction performance between robots and the environment is the basic requirement of robot control. Hence, the position/force control problem needs to be solved. However, in practice, the model of the robot is usually inaccurate, and the working environment is usually uncertain. To solve the position/force control problem of the robot when the model and position are uncertain, a new method of impedance sliding mode control with adaptive fuzzy compensation (ISMCAF) is proposed. The dynamics of the robot are governed to follow a target impedance model and the interaction control objective is achieved. According to Lyapulov's theory, sliding mode control law and adaptive control law are designed to ensure the stability of the closed-loop system. The proposed method is further verified by simulation.},
  eventtitle = {{{IEEE Access}}},
  keywords = {Adaptation models,Force,force/position control,Impedance,Impedance control,Mathematical model,robot–environment interacting,Robots,Sliding mode control,Trajectory,uncertainty},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/hu_et_al_2020_impedance_sliding_mode_control_with_adaptive_fuzzy_compensation_for.pdf}
}

@inproceedings{huNeuralLearningStable2015,
  title = {Neural Learning of Stable Dynamical Systems Based on Extreme Learning Machine},
  booktitle = {2015 {{IEEE International Conference}} on {{Information}} and {{Automation}}},
  author = {Hu, Jianbing and Yang, Zining and Wang, Zhiyang and Wu, Xinyu and Ou, Yongsheng},
  date = {2015-08},
  pages = {306--311},
  doi = {10.1109/ICInfA.2015.7279303},
  abstract = {This paper presents a method based on extreme learning machine to learn motions from human demonstrations. We model a motion as an autonomous dynamical system and define sufficient conditions to ensure the global stability at the target. A detailed theoretic analysis is proposed on the constraints regarding to input and output weights which yields a globally stable reproduction of demonstrations. We solve the corresponding optimization problem using nonlinear programming and evaluate it on an available data set and a real robot. Combined with the generalization capacities of extreme learning machine, the results show that the human movement strategies within demonstrations can be generalized well.},
  eventtitle = {2015 {{IEEE International Conference}} on {{Information}} and {{Automation}}},
  keywords = {Asymptotic stability,Extreme learning machine,Mathematical model,Nonlinear dynamical system,Optimization,READ,Robot kinematics,Stability analysis,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/hu_et_al_2015_neural_learning_of_stable_dynamical_systems_based_on_extreme_learning_machine.pdf}
}

@article{huRobotPolicyImprovement2022,
  title = {Robot {{Policy Improvement With Natural Evolution Strategies}} for {{Stable Nonlinear Dynamical System}}},
  author = {Hu, Yingbai and Chen, Guang and Li, Zhijun and Knoll, Alois},
  date = {2022},
  journaltitle = {IEEE Transactions on Cybernetics},
  pages = {1--13},
  issn = {2168-2275},
  doi = {10.1109/TCYB.2022.3192049},
  abstract = {Robot learning through kinesthetic teaching is a promising way of cloning human behaviors, but it has its limits in the performance of complex tasks with small amounts of data, due to compounding errors. In order to improve the robustness and adaptability of imitation learning, a hierarchical learning strategy is proposed: low-level learning comprises only behavioral cloning with supervised learning, and high-level learning constitutes policy improvement. First, the Gaussian mixture model (GMM)-based dynamical system is formulated to encode a motion from the demonstration. We then derive the sufficient conditions of the GMM parameters that guarantee the global stability of the dynamical system from any initial state, using the Lyapunov stability theorem. Generally, imitation learning should reason about the motion well into the future for a wide range of tasks; it is significant to improve the adaptability of the learning method by policy improvement. Finally, a method based on exponential natural evolution strategies is proposed to optimize the parameters of the dynamical system associated with the stiffness of variable impedance control, in which the exploration noise is subject to stability conditions of the dynamical system in the exploration space, thus guaranteeing the global stability. Empirical evaluations are conducted on manipulators for different scenarios, including motion planning with obstacle avoidance and stiffness learning.},
  eventtitle = {{{IEEE Transactions}} on {{Cybernetics}}},
  keywords = {Cloning,Dynamical system,exponential natural evolution strategies (NESs),Heuristic algorithms,imitation learning,Impedance,policy improvement of robustness and adaptability,READ,Robots,Robustness,Stability criteria,Task analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/hu_et_al_2022_robot_policy_improvement_with_natural_evolution_strategies_for_stable_nonlinear.pdf}
}

@article{husseinImitationLearningSurvey2017,
  title = {Imitation {{Learning}}: {{A Survey}} of {{Learning Methods}}},
  shorttitle = {Imitation {{Learning}}},
  author = {Hussein, Ahmed and Gaber, Mohamed Medhat and Elyan, Eyad and Jayne, Chrisina},
  date = {2017-04-06},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {50},
  number = {2},
  pages = {21:1--21:35},
  issn = {0360-0300},
  doi = {10.1145/3054912},
  url = {http://doi.org/10.1145/3054912},
  urldate = {2022-10-24},
  abstract = {Imitation learning techniques aim to mimic human behavior in a given task. An agent (a learning machine) is trained to perform a task from demonstrations by learning a mapping between observations and actions. The idea of teaching by imitation has been around for many years; however, the field is gaining attention recently due to advances in computing and sensing as well as rising demand for intelligent applications. The paradigm of learning by imitation is gaining popularity because it facilitates teaching complex tasks with minimal expert knowledge of the tasks. Generic imitation learning methods could potentially reduce the problem of teaching a task to that of providing demonstrations, without the need for explicit programming or designing reward functions specific to the task. Modern sensors are able to collect and transmit high volumes of data rapidly, and processors with high computational power allow fast processing that maps the sensory data to actions in a timely manner. This opens the door for many potential AI applications that require real-time perception and reaction such as humanoid robots, self-driving vehicles, human computer interaction, and computer games, to name a few. However, specialized algorithms are needed to effectively and robustly learn models as learning by imitation poses its own set of challenges. In this article, we survey imitation learning methods and present design options in different steps of the learning process. We introduce a background and motivation for the field as well as highlight challenges specific to the imitation problem. Methods for designing and evaluating imitation learning tasks are categorized and reviewed. Special attention is given to learning methods in robotics and games as these domains are the most popular in the literature and provide a wide array of problems and methodologies. We extensively discuss combining imitation learning approaches using different sources and methods, as well as incorporating other motion learning methods to enhance imitation. We also discuss the potential impact on industry, present major applications, and highlight current and future research directions.},
  keywords = {deep learning,feature representations,Imitation learning,intelligent agents,learning from demonstrations,learning from experience,READ,reinforcement learning,REVIEW,robotics,self-improvement},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/hussein_et_al_2017_imitation_learning.pdf}
}

@online{iacobKoopmanFormNonlinear2022,
  title = {Koopman {{Form}} of {{Nonlinear Systems}} with {{Inputs}}},
  author = {Iacob, Lucian Cristian and Tóth, Roland and Schoukens, Maarten},
  date = {2022-07-25},
  eprint = {2207.12132},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  doi = {10.48550/arXiv.2207.12132},
  url = {http://arxiv.org/abs/2207.12132},
  urldate = {2023-01-19},
  abstract = {The Koopman framework proposes a linear representation of finite dimensional nonlinear systems through a generally infinite dimensional globally linear representation. Originally, the Koopman formalism has been described for autonomous systems and the extension for actuated continuous-time systems with a linear input or a control affine form has only recently been addressed. However, such a derivation for discrete-time systems has not yet been developed. Thus, a particular Koopman form is generally assumed, predominantly a linear time invariant (LTI) model, as it facilitates the use of control techniques such as linear quadratic regulation and model predictive control. However, we show that this assumption is insufficient to capture the dynamics of the underlying nonlinear system. In the present paper, we systematically investigate and analytically derive lifted forms under inputs for a general class of nonlinear systems in both continuous and discrete time, using the fundamental theorem of calculus. We prove that the resulting lifted representations give linear state-space Koopman models where the input matrix becomes state (and input, for the discrete-time case)-dependent, hence it can be seen as a specially structured linear parameter-varying (LPV) description of the underlying system. We also provide error bounds on how much the parameter variation contributes and how well the system behaviour can be approximated by an LTI Koopman representation. The introduced theoretical insight greatly helps for performing proper model structure selection in system identification with Koopman models as well as making a proper choice for LTI or LPV techniques for the control of nonlinear systems through the Koopman approach.},
  pubstate = {preprint},
  keywords = {Electrical Engineering and Systems Science - Systems and Control,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/iacob_et_al_2022_koopman_form_of_nonlinear_systems_with_inputs.pdf}
}

@article{ijspeertDynamicalMovementPrimitives2013,
  title = {Dynamical {{Movement Primitives}}: {{Learning Attractor Models}} for {{Motor Behaviors}}},
  shorttitle = {Dynamical {{Movement Primitives}}},
  author = {Ijspeert, Auke Jan and Nakanishi, Jun and Hoffmann, Heiko and Pastor, Peter and Schaal, Stefan},
  date = {2013-02},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {25},
  number = {2},
  pages = {328--373},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00393},
  url = {https://direct.mit.edu/neco/article/25/2/328-373/7850},
  urldate = {2022-05-09},
  abstract = {Nonlinear dynamical systems have been used in many disciplines to model complex behaviors, including biological motor control, robotics, perception, economics, traffic prediction, and neuroscience. While often the unexpected emergent behavior of nonlinear systems is the focus of investigations, it is of equal importance to create goal-directed behavior (e.g., stable locomotion from a system of coupled oscillators under perceptual guidance). Modeling goal-directed behavior with nonlinear systems is, however, rather difficult due to the parameter sensitivity of these systems, their complex phase transitions in response to subtle parameter changes, and the difficulty of analyzing and predicting their long-term behavior; intuition and time-consuming parameter tuning play a major role. This letter presents and reviews dynamical movement primitives, a line of research for modeling attractor behaviors of autonomous nonlinear dynamical systems with the help of statistical learning techniques. The essence of our approach is to start with a simple dynamical system, such as a set of linear differential equations, and transform those into a weakly nonlinear system with prescribed attractor dynamics by means of a learnable autonomous forcing term. Both point attractors and limit cycle attractors of almost arbitrary complexity can be generated. We explain the design principle of our approach and evaluate its properties in several example applications in motor control and robotics.},
  langid = {english},
  keywords = {DMP,dynamic movement primitives,IMPORTANT,READ,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ijspeert_et_al_2013_dynamical_movement_primitives.pdf}
}

@article{ijspeertLearningAttractorLandscapes2002,
  title = {Learning Attractor Landscapes for Learning Motor Primitives},
  author = {Ijspeert, Auke and Nakanishi, Jun and Schaal, Stefan},
  date = {2002},
  journaltitle = {Advances in neural information processing systems},
  volume = {15},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ijspeert_et_al_2002_learning_attractor_landscapes_for_learning_motor_primitives.pdf}
}

@inproceedings{ijspeertLearningRhythmicMovements2002,
  title = {Learning Rhythmic Movements by Demonstration Using Nonlinear Oscillators},
  booktitle = {Proceedings of the Ieee/Rsj Int. Conference on Intelligent Robots and Systems (Iros2002)},
  author = {Ijspeert, Auke Jan and Nakanishi, Jun and Schaal, Stefan},
  date = {2002},
  pages = {958--963},
  issue = {CONF},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ijspeert_et_al_2002_learning_rhythmic_movements_by_demonstration_using_nonlinear_oscillators2.pdf}
}

@inproceedings{ijspeertMovementImitationNonlinear2002,
  title = {Movement Imitation with Nonlinear Dynamical Systems in Humanoid Robots},
  booktitle = {Proceedings 2002 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{Cat}}. {{No}}.{{02CH37292}})},
  author = {Ijspeert, A.J. and Nakanishi, J. and Schaal, S.},
  date = {2002-05},
  volume = {2},
  pages = {1398-1403 vol.2},
  doi = {10.1109/ROBOT.2002.1014739},
  abstract = {Presents an approach to movement planning, on-line trajectory modification, and imitation learning by representing movement plans based on a set of nonlinear differential equations with well-defined attractor dynamics. The resultant movement plan remains an autonomous set of nonlinear differential equations that forms a control policy (CP) which is robust to strong external perturbations and that can be modified on-line by additional perceptual variables. We evaluate the system with a humanoid robot simulation and an actual humanoid robot. Experiments are presented for the imitation of three types of movements: reaching movements with one arm, drawing movements of 2-D patterns, and tennis swings. Our results demonstrate (a) that multi-joint human movements can be encoded successfully by the CPs, (b) that a learned movement policy can readily be reused to produce robust trajectories towards different targets, (c) that a policy fitted for one particular target provides a good predictor of human reaching movements towards neighboring targets, and (d) that the parameter space which encodes a policy is suitable for measuring to which extent two trajectories are qualitatively similar.},
  eventtitle = {Proceedings 2002 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{Cat}}. {{No}}.{{02CH37292}})},
  keywords = {Biological system modeling,Control systems,Convergence,Encoding,Humanoid robots,Humans,Laboratories,Nonlinear dynamical systems,READ,Robustness,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ijspeert_et_al_2002_movement_imitation_with_nonlinear_dynamical_systems_in_humanoid_robots.pdf}
}

@inproceedings{ikeuraOptimalVariableImpedance2002,
  title = {Optimal Variable Impedance Control for a Robot and Its Application to Lifting an Object with a Human},
  booktitle = {Proceedings. 11th {{IEEE International Workshop}} on {{Robot}} and {{Human Interactive Communication}}},
  author = {Ikeura, Ryojun and Moriguchi, Tomoki and Mizutani, Kazuki},
  date = {2002},
  pages = {500--505},
  publisher = {{IEEE}},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ikeura_et_al_2002_optimal_variable_impedance_control_for_a_robot_and_its_application_to_lifting.pdf;/home/ricks/Zotero/storage/QXIDS3LJ/link.html}
}

@unpublished{ilyasCloserLookDeep2020,
  title = {A {{Closer Look}} at {{Deep Policy Gradients}}},
  author = {Ilyas, Andrew and Engstrom, Logan and Santurkar, Shibani and Tsipras, Dimitris and Janoos, Firdaus and Rudolph, Larry and Madry, Aleksander},
  date = {2020-05-25},
  eprint = {1811.02553},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1811.02553},
  urldate = {2020-07-02},
  abstract = {We study how the behavior of deep policy gradient algorithms reflects the conceptual framework motivating their development. To this end, we propose a fine-grained analysis of state-of-the-art methods based on key elements of this framework: gradient estimation, value prediction, and optimization landscapes. Our results show that the behavior of deep policy gradient algorithms often deviates from what their motivating framework would predict: the surrogate objective does not match the true reward landscape, learned value estimators fail to fit the true value function, and gradient estimates poorly correlate with the "true" gradient. The mismatch between predicted and empirical behavior we uncover highlights our poor understanding of current methods, and indicates the need to move beyond current benchmark-centric evaluation methods.},
  keywords = {#nosource,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics,critique,reinforcement learning,Statistics - Machine Learning},
  annotation = {00000}
}

@online{InterpolatedPolicyGradient,
  title = {Interpolated {{Policy Gradient}}: {{Merging On-Policy}} and {{Off-Policy Gradient Estimation}} for {{Deep Reinforcement Learning}}},
  url = {https://papers.nips.cc/paper/6974-interpolated-policy-gradient-merging-on-policy-and-off-policy-gradient-estimation-for-deep-reinforcement-learning},
  urldate = {2020-07-02},
  keywords = {#nosource,model-free,reinforcement learning}
}

@online{IntroductionDeepGaussian2019,
  title = {Introduction to {{Deep Gaussian Processes}}},
  date = {2019-09-10T00:00:00+00:00},
  url = {http://inverseprobability.com/talks/notes/introduction-to-deep-gps.html},
  urldate = {2021-05-07},
  abstract = {\$\$\textbackslash newcommand\{\textbackslash tk\}[1]\{\} \textbackslash newcommand\{\textbackslash Amatrix\}\{\textbackslash mathbf\{A\}\} \textbackslash newcommand\{\textbackslash KL\}[2]\{\textbackslash text\{KL\}\textbackslash left( \#1\textbackslash,\textbackslash |\textbackslash,\#2 \textbackslash right)\} \textbackslash newcommand\{\textbackslash Kaast\}\{\textbackslash kernelMatrix\_\{\textbackslash mathbf\{ \textbackslash ast\}\textbackslash mathbf\{ \textbackslash ast\}\}\} \textbackslash newcommand\{\textbackslash Kastu\}\{\textbackslash kernelMatrix\_\{\textbackslash mathbf\{ \textbackslash ast\} \textbackslash inducingVector\}\} \textbackslash newcommand\{\textbackslash Kff\}\{\textbackslash kernelMatrix\_\{\textbackslash mappingFunctionVector \textbackslash mappingFunctionVector\}\} \textbackslash newcommand\{\textbackslash Kfu\}\{\textbackslash kernelMatrix\_\{\textbackslash mappingFunctionVector \textbackslash inducingVector\}\} \textbackslash newcommand\{\textbackslash Kuast\}\{\textbackslash kernelMatrix\_\{\textbackslash inducingVector \textbackslash bf\textbackslash ast\}\} \textbackslash newcommand\{\textbackslash Kuf\}\{\textbackslash kernelMatrix\_\{\textbackslash inducingVector \textbackslash mappingFunctionVector\}\} \textbackslash newcommand\{\textbackslash Kuu\}\{\textbackslash kernelMatrix\_\{\textbackslash inducingVector \textbackslash inducingVector\}\} \textbackslash newcommand\{\textbackslash Kuui\}\{\textbackslash Kuu\^{}\{-1\}\} \textbackslash newcommand\{\textbackslash Qaast\}\{\textbackslash mathbf\{Q\}\_\{\textbackslash bf \textbackslash ast \textbackslash ast\}\} \textbackslash newcommand\{\textbackslash Qastf\}\{\textbackslash mathbf\{Q\}\_\{\textbackslash ast \textbackslash mappingFunction\}\} \textbackslash newcommand\{\textbackslash Qfast\}\{\textbackslash mathbf\{Q\}\_\{\textbackslash mappingFunctionVector \textbackslash bf \textbackslash ast\}\} \textbackslash newcommand\{\textbackslash Qff\}\{\textbackslash mathbf\{Q\}\_\{\textbackslash mappingFunctionVector \textbackslash mappingFunctionVector\}\} \textbackslash newcommand\{\textbackslash aMatrix\}\{\textbackslash mathbf\{A\}\} \textbackslash newcommand\{\textbackslash aScalar\}\{a\} \textbackslash newcommand\{\textbackslash aVector\}\{\textbackslash mathbf\{a\}\} \textbackslash newcommand\{\textbackslash acceleration\}\{a\} \textbackslash newcommand\{\textbackslash bMatrix\}\{\textbackslash mathbf\{B\}\} \textbackslash newcommand\{\textbackslash bScalar\}\{b\} \textbackslash newcommand\{\textbackslash bVector\}\{\textbackslash mathbf\{b\}\} \textbackslash newcommand\{\textbackslash basisFunc\}\{\textbackslash phi\} \textbackslash newcommand\{\textbackslash basisFuncVector\}\{\textbackslash boldsymbol\{ \textbackslash basisFunc\}\} \textbackslash newcommand\{\textbackslash basisFunction\}\{\textbackslash phi\} \textbackslash newcommand\{\textbackslash basisLocation\}\{\textbackslash mu\} \textbackslash newcommand\{\textbackslash basisMatrix\}\{\textbackslash boldsymbol\{ \textbackslash Phi\}\} \textbackslash newcommand\{\textbackslash basisScalar\}\{\textbackslash basisFunction\} \textbackslash newcommand\{\textbackslash basisVector\}\{\textbackslash boldsymbol\{ \textbackslash basisFunction\}\} \textbackslash newcommand\{\textbackslash activationFunction\}\{\textbackslash phi\} \textbackslash newcommand\{\textbackslash activationMatrix\}\{\textbackslash boldsymbol\{ \textbackslash Phi\}\} \textbackslash newcommand\{\textbackslash activationScalar\}\{\textbackslash basisFunction\} \textbackslash newcommand\{\textbackslash activationVector\}\{\textbackslash boldsymbol\{ \textbackslash basisFunction\}\} \textbackslash newcommand\{\textbackslash bigO\}\{\textbackslash mathcal\{O\}\} \textbackslash newcommand\{\textbackslash binomProb\}\{\textbackslash pi\} \textbackslash newcommand\{\textbackslash cMatrix\}\{\textbackslash mathbf\{C\}\} \textbackslash newcommand\{\textbackslash cbasisMatrix\}\{\textbackslash hat\{\textbackslash boldsymbol\{ \textbackslash Phi\}\}\} \textbackslash newcommand\{\textbackslash cdataMatrix\}\{\textbackslash hat\{\textbackslash dataMatrix\}\} \textbackslash newcommand\{\textbackslash cdataScalar\}\{\textbackslash hat\{\textbackslash dataScalar\}\} \textbackslash newcommand\{\textbackslash cdataVector\}\{\textbackslash hat\{\textbackslash dataVector\}\} \textbackslash newcommand\{\textbackslash centeredKernelMatrix\}\{\textbackslash mathbf\{ \textbackslash MakeUppercase\{\textbackslash centeredKernelScalar\}\}\} \textbackslash newcommand\{\textbackslash centeredKernelScalar\}\{b\} \textbackslash newcommand\{\textbackslash centeredKernelVector\}\{\textbackslash centeredKernelScalar\} \textbackslash newcommand\{\textbackslash centeringMatrix\}\{\textbackslash mathbf\{H\}\} \textbackslash newcommand\{\textbackslash chiSquaredDist\}[2]\{\textbackslash chi\_\{\#1\}\^{}\{2\}\textbackslash left(\#2\textbackslash right)\} \textbackslash newcommand\{\textbackslash chiSquaredSamp\}[1]\{\textbackslash chi\_\{\#1\}\^{}\{2\}\} \textbackslash newcommand\{\textbackslash conditionalCovariance\}\{\textbackslash boldsymbol\{ \textbackslash Sigma\}\} \textbackslash newcommand\{\textbackslash coregionalizationMatrix\}\{\textbackslash mathbf\{B\}\} \textbackslash newcommand\{\textbackslash coregionalizationScalar\}\{b\} \textbackslash newcommand\{\textbackslash coregionalizationVector\}\{\textbackslash mathbf\{ \textbackslash coregionalizationScalar\}\} \textbackslash newcommand\{\textbackslash covDist\}[2]\{\textbackslash text\{cov\}\_\{\#2\}\textbackslash left(\#1\textbackslash right)\} \textbackslash newcommand\{\textbackslash covSamp\}[1]\{\textbackslash text\{cov\}\textbackslash left(\#1\textbackslash right)\} \textbackslash newcommand\{\textbackslash covarianceScalar\}\{c\} \textbackslash newcommand\{\textbackslash covarianceVector\}\{\textbackslash mathbf\{ \textbackslash covarianceScalar\}\} \textbackslash newcommand\{\textbackslash covarianceMatrix\}\{\textbackslash mathbf\{C\}\} \textbackslash newcommand\{\textbackslash covarianceMatrixTwo\}\{\textbackslash boldsymbol\{ \textbackslash Sigma\}\} \textbackslash newcommand\{\textbackslash croupierScalar\}\{s\} \textbackslash newcommand\{\textbackslash croupierVector\}\{\textbackslash mathbf\{ \textbackslash croupierScalar\}\} \textbackslash newcommand\{\textbackslash croupierMatrix\}\{\textbackslash mathbf\{ \textbackslash MakeUppercase\{\textbackslash croupierScalar\}\}\} \textbackslash newcommand\{\textbackslash dataDim\}\{p\} \textbackslash newcommand\{\textbackslash dataIndex\}\{i\} \textbackslash newcommand\{\textbackslash dataIndexTwo\}\{j\} \textbackslash newcommand\{\textbackslash dataMatrix\}\{\textbackslash mathbf\{Y\}\} \textbackslash newcommand\{\textbackslash dataScalar\}\{y\} \textbackslash newcommand\{\textbackslash dataSet\}\{\textbackslash mathcal\{D\}\} \textbackslash newcommand\{\textbackslash dataStd\}\{\textbackslash sigma\} \textbackslash newcommand\{\textbackslash dataVector\}\{\textbackslash mathbf\{ \textbackslash dataScalar\}\} \textbackslash newcommand\{\textbackslash decayRate\}\{d\} \textbackslash newcommand\{\textbackslash degreeMatrix\}\{\textbackslash mathbf\{ \textbackslash MakeUppercase\{\textbackslash degreeScalar\}\}\} \textbackslash newcommand\{\textbackslash degreeScalar\}\{d\} \textbackslash newcommand\{\textbackslash degreeVector\}\{\textbackslash mathbf\{ \textbackslash degreeScalar\}\} \textbackslash newcommand\{\textbackslash diag\}[1]\{\textbackslash text\{diag\}\textbackslash left(\#1\textbackslash right)\} \textbackslash newcommand\{\textbackslash diagonalMatrix\}\{\textbackslash mathbf\{D\}\} \textbackslash newcommand\{\textbackslash diff\}[2]\{\textbackslash frac\{\textbackslash text\{d\}\#1\}\{\textbackslash text\{d\}\#2\}\} \textbackslash newcommand\{\textbackslash diffTwo\}[2]\{\textbackslash frac\{\textbackslash text\{d\}\^{}2\#1\}\{\textbackslash text\{d\}\#2\^{}2\}\} \textbackslash newcommand\{\textbackslash displacement\}\{x\} \textbackslash newcommand\{\textbackslash displacementVector\}\{\textbackslash textbf\{\textbackslash displacement\}\} \textbackslash newcommand\{\textbackslash distanceMatrix\}\{\textbackslash mathbf\{ \textbackslash MakeUppercase\{\textbackslash distanceScalar\}\}\} \textbackslash newcommand\{\textbackslash distanceScalar\}\{d\} \textbackslash newcommand\{\textbackslash distanceVector\}\{\textbackslash mathbf\{ \textbackslash distanceScalar\}\} \textbackslash newcommand\{\textbackslash eigenvaltwo\}\{\textbackslash ell\} \textbackslash newcommand\{\textbackslash eigenvaltwoMatrix\}\{\textbackslash mathbf\{L\}\} \textbackslash newcommand\{\textbackslash eigenvaltwoVector\}\{\textbackslash mathbf\{l\}\} \textbackslash newcommand\{\textbackslash eigenvalue\}\{\textbackslash lambda\} \textbackslash newcommand\{\textbackslash eigenvalueMatrix\}\{\textbackslash boldsymbol\{ \textbackslash Lambda\}\} \textbackslash newcommand\{\textbackslash eigenvalueVector\}\{\textbackslash boldsymbol\{ \textbackslash lambda\}\} \textbackslash newcommand\{\textbackslash eigenvector\}\{\textbackslash mathbf\{ \textbackslash eigenvectorScalar\}\} \textbackslash newcommand\{\textbackslash eigenvectorMatrix\}\{\textbackslash mathbf\{U\}\} \textbackslash newcommand\{\textbackslash eigenvectorScalar\}\{u\} \textbackslash newcommand\{\textbackslash eigenvectwo\}\{\textbackslash mathbf\{v\}\} \textbackslash newcommand\{\textbackslash eigenvectwoMatrix\}\{\textbackslash mathbf\{V\}\} \textbackslash newcommand\{\textbackslash eigenvectwoScalar\}\{v\} \textbackslash newcommand\{\textbackslash entropy\}[1]\{\textbackslash mathcal\{H\}\textbackslash left(\#1\textbackslash right)\} \textbackslash newcommand\{\textbackslash errorFunction\}\{E\} \textbackslash newcommand\{\textbackslash expDist\}[2]\{\textbackslash left{$<\#$}1\textbackslash right{$>\_$}\{\#2\}\} \textbackslash newcommand\{\textbackslash expSamp\}[1]\{\textbackslash left{$<\#$}1\textbackslash right{$>$}\} \textbackslash newcommand\{\textbackslash expectation\}[1]\{\textbackslash left\textbackslash langle \#1 \textbackslash right\textbackslash rangle \} \textbackslash newcommand\{\textbackslash expectationDist\}[2]\{\textbackslash left\textbackslash langle \#1 \textbackslash right\textbackslash rangle \_\{\#2\}\} \textbackslash newcommand\{\textbackslash expectedDistanceMatrix\}\{\textbackslash mathcal\{D\}\} \textbackslash newcommand\{\textbackslash eye\}\{\textbackslash mathbf\{I\}\} \textbackslash newcommand\{\textbackslash fantasyDim\}\{r\} \textbackslash newcommand\{\textbackslash fantasyMatrix\}\{\textbackslash mathbf\{ \textbackslash MakeUppercase\{\textbackslash fantasyScalar\}\}\} \textbackslash newcommand\{\textbackslash fantasyScalar\}\{z\} \textbackslash newcommand\{\textbackslash fantasyVector\}\{\textbackslash mathbf\{ \textbackslash fantasyScalar\}\} \textbackslash newcommand\{\textbackslash featureStd\}\{\textbackslash varsigma\} \textbackslash newcommand\{\textbackslash gammaCdf\}[3]\{\textbackslash mathcal\{GAMMA CDF\}\textbackslash left(\#1|\#2,\#3\textbackslash right)\} \textbackslash newcommand\{\textbackslash gammaDist\}[3]\{\textbackslash mathcal\{G\}\textbackslash left(\#1|\#2,\#3\textbackslash right)\} \textbackslash newcommand\{\textbackslash gammaSamp\}[2]\{\textbackslash mathcal\{G\}\textbackslash left(\#1,\#2\textbackslash right)\} \textbackslash newcommand\{\textbackslash gaussianDist\}[3]\{\textbackslash mathcal\{N\}\textbackslash left(\#1|\#2,\#3\textbackslash right)\} \textbackslash newcommand\{\textbackslash gaussianSamp\}[2]\{\textbackslash mathcal\{N\}\textbackslash left(\#1,\#2\textbackslash right)\} \textbackslash newcommand\{\textbackslash uniformDist\}[3]\{\textbackslash mathcal\{U\}\textbackslash left(\#1|\#2,\#3\textbackslash right)\} \textbackslash newcommand\{\textbackslash uniformSamp\}[2]\{\textbackslash mathcal\{U\}\textbackslash left(\#1,\#2\textbackslash right)\} \textbackslash newcommand\{\textbackslash given\}\{|\} \textbackslash newcommand\{\textbackslash half\}\{\textbackslash frac\{1\}\{2\}\} \textbackslash newcommand\{\textbackslash heaviside\}\{H\} \textbackslash newcommand\{\textbackslash hiddenMatrix\}\{\textbackslash mathbf\{ \textbackslash MakeUppercase\{\textbackslash hiddenScalar\}\}\} \textbackslash newcommand\{\textbackslash hiddenScalar\}\{h\} \textbackslash newcommand\{\textbackslash hiddenVector\}\{\textbackslash mathbf\{ \textbackslash hiddenScalar\}\} \textbackslash newcommand\{\textbackslash identityMatrix\}\{\textbackslash eye\} \textbackslash newcommand\{\textbackslash inducingInputScalar\}\{z\} \textbackslash newcommand\{\textbackslash inducingInputVector\}\{\textbackslash mathbf\{ \textbackslash inducingInputScalar\}\} \textbackslash newcommand\{\textbackslash inducingInputMatrix\}\{\textbackslash mathbf\{Z\}\} \textbackslash newcommand\{\textbackslash inducingScalar\}\{u\} \textbackslash newcommand\{\textbackslash inducingVector\}\{\textbackslash mathbf\{ \textbackslash inducingScalar\}\} \textbackslash newcommand\{\textbackslash inducingMatrix\}\{\textbackslash mathbf\{U\}\} \textbackslash newcommand\{\textbackslash inlineDiff\}[2]\{\textbackslash text\{d\}\#1/\textbackslash text\{d\}\#2\} \textbackslash newcommand\{\textbackslash inputDim\}\{q\} \textbackslash newcommand\{\textbackslash inputMatrix\}\{\textbackslash mathbf\{X\}\} \textbackslash newcommand\{\textbackslash inputScalar\}\{x\} \textbackslash newcommand\{\textbackslash inputSpace\}\{\textbackslash mathcal\{X\}\} \textbackslash newcommand\{\textbackslash inputVals\}\{\textbackslash inputVector\} \textbackslash newcommand\{\textbackslash inputVector\}\{\textbackslash mathbf\{ \textbackslash inputScalar\}\} \textbackslash newcommand\{\textbackslash iterNum\}\{k\} \textbackslash newcommand\{\textbackslash kernel\}\{\textbackslash kernelScalar\} \textbackslash newcommand\{\textbackslash kernelMatrix\}\{\textbackslash mathbf\{K\}\} \textbackslash newcommand\{\textbackslash kernelScalar\}\{k\} \textbackslash newcommand\{\textbackslash kernelVector\}\{\textbackslash mathbf\{ \textbackslash kernelScalar\}\} \textbackslash newcommand\{\textbackslash kff\}\{\textbackslash kernelScalar\_\{\textbackslash mappingFunction \textbackslash mappingFunction\}\} \textbackslash newcommand\{\textbackslash kfu\}\{\textbackslash kernelVector\_\{\textbackslash mappingFunction \textbackslash inducingScalar\}\} \textbackslash newcommand\{\textbackslash kuf\}\{\textbackslash kernelVector\_\{\textbackslash inducingScalar \textbackslash mappingFunction\}\} \textbackslash newcommand\{\textbackslash kuu\}\{\textbackslash kernelVector\_\{\textbackslash inducingScalar \textbackslash inducingScalar\}\} \textbackslash newcommand\{\textbackslash lagrangeMultiplier\}\{\textbackslash lambda\} \textbackslash newcommand\{\textbackslash lagrangeMultiplierMatrix\}\{\textbackslash boldsymbol\{ \textbackslash Lambda\}\} \textbackslash newcommand\{\textbackslash lagrangian\}\{L\} \textbackslash newcommand\{\textbackslash laplacianFactor\}\{\textbackslash mathbf\{ \textbackslash MakeUppercase\{\textbackslash laplacianFactorScalar\}\}\} \textbackslash newcommand\{\textbackslash laplacianFactorScalar\}\{m\} \textbackslash newcommand\{\textbackslash laplacianFactorVector\}\{\textbackslash mathbf\{ \textbackslash laplacianFactorScalar\}\} \textbackslash newcommand\{\textbackslash laplacianMatrix\}\{\textbackslash mathbf\{L\}\} \textbackslash newcommand\{\textbackslash laplacianScalar\}\{\textbackslash ell\} \textbackslash newcommand\{\textbackslash laplacianVector\}\{\textbackslash mathbf\{ \textbackslash ell\}\} \textbackslash newcommand\{\textbackslash latentDim\}\{q\} \textbackslash newcommand\{\textbackslash latentDistanceMatrix\}\{\textbackslash boldsymbol\{ \textbackslash Delta\}\} \textbackslash newcommand\{\textbackslash latentDistanceScalar\}\{\textbackslash delta\} \textbackslash newcommand\{\textbackslash latentDistanceVector\}\{\textbackslash boldsymbol\{ \textbackslash delta\}\} \textbackslash newcommand\{\textbackslash latentForce\}\{f\} \textbackslash newcommand\{\textbackslash latentFunction\}\{u\} \textbackslash newcommand\{\textbackslash latentFunctionVector\}\{\textbackslash mathbf\{ \textbackslash latentFunction\}\} \textbackslash newcommand\{\textbackslash latentFunctionMatrix\}\{\textbackslash mathbf\{ \textbackslash MakeUppercase\{\textbackslash latentFunction\}\}\} \textbackslash newcommand\{\textbackslash latentIndex\}\{j\} \textbackslash newcommand\{\textbackslash latentScalar\}\{z\} \textbackslash newcommand\{\textbackslash latentVector\}\{\textbackslash mathbf\{ \textbackslash latentScalar\}\} \textbackslash newcommand\{\textbackslash latentMatrix\}\{\textbackslash mathbf\{Z\}\} \textbackslash newcommand\{\textbackslash learnRate\}\{\textbackslash eta\} \textbackslash newcommand\{\textbackslash lengthScale\}\{\textbackslash ell\} \textbackslash newcommand\{\textbackslash rbfWidth\}\{\textbackslash ell\} \textbackslash newcommand\{\textbackslash likelihoodBound\}\{\textbackslash mathcal\{L\}\} \textbackslash newcommand\{\textbackslash likelihoodFunction\}\{L\} \textbackslash newcommand\{\textbackslash locationScalar\}\{\textbackslash mu\} \textbackslash newcommand\{\textbackslash locationVector\}\{\textbackslash boldsymbol\{ \textbackslash locationScalar\}\} \textbackslash newcommand\{\textbackslash locationMatrix\}\{\textbackslash mathbf\{M\}\} \textbackslash newcommand\{\textbackslash variance\}[1]\{\textbackslash text\{var\}\textbackslash left( \#1 \textbackslash right)\} \textbackslash newcommand\{\textbackslash mappingFunction\}\{f\} \textbackslash newcommand\{\textbackslash mappingFunctionMatrix\}\{\textbackslash mathbf\{F\}\} \textbackslash newcommand\{\textbackslash mappingFunctionTwo\}\{g\} \textbackslash newcommand\{\textbackslash mappingFunctionTwoMatrix\}\{\textbackslash mathbf\{G\}\} \textbackslash newcommand\{\textbackslash mappingFunctionTwoVector\}\{\textbackslash mathbf\{ \textbackslash mappingFunctionTwo\}\} \textbackslash newcommand\{\textbackslash mappingFunctionVector\}\{\textbackslash mathbf\{ \textbackslash mappingFunction\}\} \textbackslash newcommand\{\textbackslash scaleScalar\}\{s\} \textbackslash newcommand\{\textbackslash mappingScalar\}\{w\} \textbackslash newcommand\{\textbackslash mappingVector\}\{\textbackslash mathbf\{ \textbackslash mappingScalar\}\} \textbackslash newcommand\{\textbackslash mappingMatrix\}\{\textbackslash mathbf\{W\}\} \textbackslash newcommand\{\textbackslash mappingScalarTwo\}\{v\} \textbackslash newcommand\{\textbackslash mappingVectorTwo\}\{\textbackslash mathbf\{ \textbackslash mappingScalarTwo\}\} \textbackslash newcommand\{\textbackslash mappingMatrixTwo\}\{\textbackslash mathbf\{V\}\} \textbackslash newcommand\{\textbackslash maxIters\}\{K\} \textbackslash newcommand\{\textbackslash meanMatrix\}\{\textbackslash mathbf\{M\}\} \textbackslash newcommand\{\textbackslash meanScalar\}\{\textbackslash mu\} \textbackslash newcommand\{\textbackslash meanTwoMatrix\}\{\textbackslash mathbf\{M\}\} \textbackslash newcommand\{\textbackslash meanTwoScalar\}\{m\} \textbackslash newcommand\{\textbackslash meanTwoVector\}\{\textbackslash mathbf\{ \textbackslash meanTwoScalar\}\} \textbackslash newcommand\{\textbackslash meanVector\}\{\textbackslash boldsymbol\{ \textbackslash meanScalar\}\} \textbackslash newcommand\{\textbackslash mrnaConcentration\}\{m\} \textbackslash newcommand\{\textbackslash naturalFrequency\}\{\textbackslash omega\} \textbackslash newcommand\{\textbackslash neighborhood\}[1]\{\textbackslash mathcal\{N\}\textbackslash left( \#1 \textbackslash right)\} \textbackslash newcommand\{\textbackslash neilurl\}\{http://inverseprobability.com/\} \textbackslash newcommand\{\textbackslash noiseMatrix\}\{\textbackslash boldsymbol\{ E\}\} \textbackslash newcommand\{\textbackslash noiseScalar\}\{\textbackslash epsilon\} \textbackslash newcommand\{\textbackslash noiseVector\}\{\textbackslash boldsymbol\{ \textbackslash epsilon\}\} \textbackslash newcommand\{\textbackslash noiseStd\}\{\textbackslash sigma\} \textbackslash newcommand\{\textbackslash norm\}[1]\{\textbackslash left\textbackslash Vert \#1 \textbackslash right\textbackslash Vert\} \textbackslash newcommand\{\textbackslash normalizedLaplacianMatrix\}\{\textbackslash hat\{\textbackslash mathbf\{L\}\}\} \textbackslash newcommand\{\textbackslash normalizedLaplacianScalar\}\{\textbackslash hat\{\textbackslash ell\}\} \textbackslash newcommand\{\textbackslash normalizedLaplacianVector\}\{\textbackslash hat\{\textbackslash mathbf\{ \textbackslash ell\}\}\} \textbackslash newcommand\{\textbackslash numActive\}\{m\} \textbackslash newcommand\{\textbackslash numBasisFunc\}\{m\} \textbackslash newcommand\{\textbackslash numComponents\}\{m\} \textbackslash newcommand\{\textbackslash numComps\}\{K\} \textbackslash newcommand\{\textbackslash numData\}\{n\} \textbackslash newcommand\{\textbackslash numFeatures\}\{K\} \textbackslash newcommand\{\textbackslash numHidden\}\{h\} \textbackslash newcommand\{\textbackslash numInducing\}\{m\} \textbackslash newcommand\{\textbackslash numLayers\}\{\textbackslash ell\} \textbackslash newcommand\{\textbackslash numNeighbors\}\{K\} \textbackslash newcommand\{\textbackslash numSequences\}\{s\} \textbackslash newcommand\{\textbackslash numSuccess\}\{s\} \textbackslash newcommand\{\textbackslash numTasks\}\{m\} \textbackslash newcommand\{\textbackslash numTime\}\{T\} \textbackslash newcommand\{\textbackslash numTrials\}\{S\} \textbackslash newcommand\{\textbackslash outputIndex\}\{j\} \textbackslash newcommand\{\textbackslash paramVector\}\{\textbackslash boldsymbol\{ \textbackslash theta\}\} \textbackslash newcommand\{\textbackslash parameterMatrix\}\{\textbackslash boldsymbol\{ \textbackslash Theta\}\} \textbackslash newcommand\{\textbackslash parameterScalar\}\{\textbackslash theta\} \textbackslash newcommand\{\textbackslash parameterVector\}\{\textbackslash boldsymbol\{ \textbackslash parameterScalar\}\} \textbackslash newcommand\{\textbackslash partDiff\}[2]\{\textbackslash frac\{\textbackslash partial\#1\}\{\textbackslash partial\#2\}\} \textbackslash newcommand\{\textbackslash precisionScalar\}\{j\} \textbackslash newcommand\{\textbackslash precisionVector\}\{\textbackslash mathbf\{ \textbackslash precisionScalar\}\} \textbackslash newcommand\{\textbackslash precisionMatrix\}\{\textbackslash mathbf\{J\}\} \textbackslash newcommand\{\textbackslash pseudotargetScalar\}\{\textbackslash widetilde\{y\}\} \textbackslash newcommand\{\textbackslash pseudotargetVector\}\{\textbackslash mathbf\{ \textbackslash pseudotargetScalar\}\} \textbackslash newcommand\{\textbackslash pseudotargetMatrix\}\{\textbackslash mathbf\{ \textbackslash widetilde\{Y\}\}\} \textbackslash newcommand\{\textbackslash rank\}[1]\{\textbackslash text\{rank\}\textbackslash left(\#1\textbackslash right)\} \textbackslash newcommand\{\textbackslash rayleighDist\}[2]\{\textbackslash mathcal\{R\}\textbackslash left(\#1|\#2\textbackslash right)\} \textbackslash newcommand\{\textbackslash rayleighSamp\}[1]\{\textbackslash mathcal\{R\}\textbackslash left(\#1\textbackslash right)\} \textbackslash newcommand\{\textbackslash responsibility\}\{r\} \textbackslash newcommand\{\textbackslash rotationScalar\}\{r\} \textbackslash newcommand\{\textbackslash rotationVector\}\{\textbackslash mathbf\{ \textbackslash rotationScalar\}\} \textbackslash newcommand\{\textbackslash rotationMatrix\}\{\textbackslash mathbf\{R\}\} \textbackslash newcommand\{\textbackslash sampleCovScalar\}\{s\} \textbackslash newcommand\{\textbackslash sampleCovVector\}\{\textbackslash mathbf\{ \textbackslash sampleCovScalar\}\} \textbackslash newcommand\{\textbackslash sampleCovMatrix\}\{\textbackslash mathbf\{s\}\} \textbackslash newcommand\{\textbackslash scalarProduct\}[2]\{\textbackslash left\textbackslash langle\{\#1\},\{\#2\}\textbackslash right\textbackslash rangle\} \textbackslash newcommand\{\textbackslash sign\}[1]\{\textbackslash text\{sign\}\textbackslash left(\#1\textbackslash right)\} \textbackslash newcommand\{\textbackslash sigmoid\}[1]\{\textbackslash sigma\textbackslash left(\#1\textbackslash right)\} \textbackslash newcommand\{\textbackslash singularvalue\}\{\textbackslash ell\} \textbackslash newcommand\{\textbackslash singularvalueMatrix\}\{\textbackslash mathbf\{L\}\} \textbackslash newcommand\{\textbackslash singularvalueVector\}\{\textbackslash mathbf\{l\}\} \textbackslash newcommand\{\textbackslash sorth\}\{\textbackslash mathbf\{u\}\} \textbackslash newcommand\{\textbackslash spar\}\{\textbackslash lambda\} \textbackslash newcommand\{\textbackslash trace\}[1]\{\textbackslash text\{tr\}\textbackslash left(\#1\textbackslash right)\} \textbackslash newcommand\{\textbackslash BasalRate\}\{B\} \textbackslash newcommand\{\textbackslash DampingCoefficient\}\{C\} \textbackslash newcommand\{\textbackslash DecayRate\}\{D\} \textbackslash newcommand\{\textbackslash Displacement\}\{X\} \textbackslash newcommand\{\textbackslash LatentForce\}\{F\} \textbackslash newcommand\{\textbackslash Mass\}\{M\} \textbackslash newcommand\{\textbackslash Sensitivity\}\{S\} \textbackslash newcommand\{\textbackslash basalRate\}\{b\} \textbackslash newcommand\{\textbackslash dampingCoefficient\}\{c\} \textbackslash newcommand\{\textbackslash mass\}\{m\} \textbackslash newcommand\{\textbackslash sensitivity\}\{s\} \textbackslash newcommand\{\textbackslash springScalar\}\{\textbackslash kappa\} \textbackslash newcommand\{\textbackslash springVector\}\{\textbackslash boldsymbol\{ \textbackslash kappa\}\} \textbackslash newcommand\{\textbackslash springMatrix\}\{\textbackslash boldsymbol\{ \textbackslash mathcal\{K\}\}\} \textbackslash newcommand\{\textbackslash tfConcentration\}\{p\} \textbackslash newcommand\{\textbackslash tfDecayRate\}\{\textbackslash delta\} \textbackslash newcommand\{\textbackslash tfMrnaConcentration\}\{f\} \textbackslash newcommand\{\textbackslash tfVector\}\{\textbackslash mathbf\{ \textbackslash tfConcentration\}\} \textbackslash newcommand\{\textbackslash velocity\}\{v\} \textbackslash newcommand\{\textbackslash sufficientStatsScalar\}\{g\} \textbackslash newcommand\{\textbackslash sufficientStatsVector\}\{\textbackslash mathbf\{ \textbackslash sufficientStatsScalar\}\} \textbackslash newcommand\{\textbackslash sufficientStatsMatrix\}\{\textbackslash mathbf\{G\}\} \textbackslash newcommand\{\textbackslash switchScalar\}\{s\} \textbackslash newcommand\{\textbackslash switchVector\}\{\textbackslash mathbf\{ \textbackslash switchScalar\}\} \textbackslash newcommand\{\textbackslash switchMatrix\}\{\textbackslash mathbf\{S\}\} \textbackslash newcommand\{\textbackslash tr\}[1]\{\textbackslash text\{tr\}\textbackslash left(\#1\textbackslash right)\} \textbackslash newcommand\{\textbackslash loneNorm\}[1]\{\textbackslash left\textbackslash Vert \#1 \textbackslash right\textbackslash Vert\_1\} \textbackslash newcommand\{\textbackslash ltwoNorm\}[1]\{\textbackslash left\textbackslash Vert \#1 \textbackslash right\textbackslash Vert\_2\} \textbackslash newcommand\{\textbackslash onenorm\}[1]\{\textbackslash left\textbackslash vert\#1\textbackslash right\textbackslash vert\_1\} \textbackslash newcommand\{\textbackslash twonorm\}[1]\{\textbackslash left\textbackslash Vert \#1 \textbackslash right\textbackslash Vert\} \textbackslash newcommand\{\textbackslash vScalar\}\{v\} \textbackslash newcommand\{\textbackslash vVector\}\{\textbackslash mathbf\{v\}\} \textbackslash newcommand\{\textbackslash vMatrix\}\{\textbackslash mathbf\{V\}\} \textbackslash newcommand\{\textbackslash varianceDist\}[2]\{\textbackslash text\{var\}\_\{\#2\}\textbackslash left( \#1 \textbackslash right)\} \textbackslash newcommand\{\textbackslash vecb\}[1]\{\textbackslash left(\#1\textbackslash right):\} \textbackslash newcommand\{\textbackslash weightScalar\}\{w\} \textbackslash newcommand\{\textbackslash weightVector\}\{\textbackslash mathbf\{ \textbackslash weightScalar\}\} \textbackslash newcommand\{\textbackslash weightMatrix\}\{\textbackslash mathbf\{W\}\} \textbackslash newcommand\{\textbackslash weightedAdjacencyMatrix\}\{\textbackslash mathbf\{A\}\} \textbackslash newcommand\{\textbackslash weightedAdjacencyScalar\}\{a\} \textbackslash newcommand\{\textbackslash weightedAdjacencyVector\}\{\textbackslash mathbf\{ \textbackslash weightedAdjacencyScalar\}\} \textbackslash newcommand\{\textbackslash onesVector\}\{\textbackslash mathbf\{1\}\} \textbackslash newcommand\{\textbackslash zerosVector\}\{\textbackslash mathbf\{0\}\} \$\$},
  langid = {english},
  organization = {{Neil Lawrence’s Talks}},
  keywords = {#nosource,blog}
}

@unpublished{islamReproducibilityBenchmarkedDeep2017,
  title = {Reproducibility of {{Benchmarked Deep Reinforcement Learning Tasks}} for {{Continuous Control}}},
  author = {Islam, Riashat and Henderson, Peter and Gomrokchi, Maziar and Precup, Doina},
  date = {2017-08-10},
  eprint = {1708.04133},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1708.04133},
  urldate = {2020-07-02},
  abstract = {Policy gradient methods in reinforcement learning have become increasingly prevalent for state-of-the-art performance in continuous control tasks. Novel methods typically benchmark against a few key algorithms such as deep deterministic policy gradients and trust region policy optimization. As such, it is important to present and use consistent baselines experiments. However, this can be difficult due to general variance in the algorithms, hyper-parameter tuning, and environment stochasticity. We investigate and discuss: the significance of hyper-parameters in policy gradients for continuous control, general variance in the algorithms, and reproducibility of reported results. We provide guidelines on reporting novel results as comparisons against baseline methods such that future researchers can make informed decisions when investigating novel methods.},
  keywords = {#nosource,benchmarks,Computer Science - Machine Learning,reinforcement learning,reproducibility},
  annotation = {00104}
}

@unpublished{itkinaDynamicEnvironmentPrediction2019,
  title = {Dynamic {{Environment Prediction}} in {{Urban Scenes}} Using {{Recurrent Representation Learning}}},
  author = {Itkina, Masha and Driggs-Campbell, Katherine and Kochenderfer, Mykel J.},
  date = {2019-04-28},
  eprint = {1904.12374},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1904.12374},
  urldate = {2019-05-08},
  abstract = {A key challenge for autonomous driving is safe trajectory planning in cluttered, urban environments with dynamic obstacles, such as pedestrians, bicyclists, and other vehicles. A reliable prediction of the future environment state, including the behavior of dynamic agents, would allow planning algorithms to proactively generate a trajectory in response to a rapidly changing environment. We present a novel framework that predicts the future occupancy state of the local environment surrounding an autonomous agent by learning a motion model from occupancy grid data using a neural network. We take advantage of the temporal structure of the grid data by utilizing a convolutional long-short term memory network in the form of the PredNet architecture. This method is validated on the KITTI dataset and demonstrates higher accuracy and better predictive power than baseline methods.},
  keywords = {#nosource,Computer Science - Machine Learning,Computer Science - Robotics,path planning,robotic grasping,tno internship},
  annotation = {00000}
}

@unpublished{jaderbergReinforcementLearningUnsupervised2016,
  title = {Reinforcement {{Learning}} with {{Unsupervised Auxiliary Tasks}}},
  author = {Jaderberg, Max and Mnih, Volodymyr and Czarnecki, Wojciech Marian and Schaul, Tom and Leibo, Joel Z. and Silver, David and Kavukcuoglu, Koray},
  date = {2016-11-16},
  eprint = {1611.05397},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1611.05397},
  urldate = {2020-07-02},
  abstract = {Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the-art on Atari, averaging 880\textbackslash\% expert human performance, and a challenging suite of first-person, three-dimensional \textbackslash emph\{Labyrinth\} tasks leading to a mean speedup in learning of 10\$\textbackslash times\$ and averaging 87\textbackslash\% expert human performance on Labyrinth.},
  keywords = {#nosource,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,multitask RL,reinforcement learning,transfer learning},
  annotation = {00566}
}

@inproceedings{jagtapControlBarrierFunctions2020,
  title = {Control {{Barrier Functions}} for {{Unknown Nonlinear Systems}} Using {{Gaussian Processes}}},
  booktitle = {2020 59th {{Ieee Conference}} on {{Decision}} and {{Control}} (Cdc)},
  author = {Jagtap, Pushpak and Pappas, George J. and Zamani, Majid},
  date = {2020},
  pages = {3699--3704},
  publisher = {{Ieee}},
  location = {{New York}},
  issn = {0743-1546},
  url = {http://arxiv.org/pdf/2010.05818},
  urldate = {2022-11-22},
  abstract = {This paper focuses on the controller synthesis for unknown, nonlinear systems while ensuring safety constraints. Our approach consists of two steps, a learning step that uses Gaussian processes and a controller synthesis step that is based on control barrier functions. In the learning step, we use a data-driven approach utilizing Gaussian processes to learn the unknown control affine nonlinear dynamics together with a statistical bound on the accuracy of the learned model. In the second controller synthesis steps, we develop a systematic approach to compute control barrier functions that explicitly take into consideration the uncertainty of the learned model. The control barrier function not only results in a safe controller by construction but also provides a rigorous lower bound on the probability of satisfaction of the safety specification. Finally, we illustrate the effectiveness of the proposed results by synthesizing a safety controller for a jet engine example.},
  isbn = {978-1-72817-447-1},
  langid = {english},
  annotation = {WOS:000717663402154},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/jagtap_et_al_2020_control_barrier_functions_for_unknown_nonlinear_systems_using_gaussian_processes.pdf}
}

@unpublished{jamesSimtoRealSimtoSimDataefficient2018,
  title = {Sim-to-{{Real}} via {{Sim-to-Sim}}: {{Data-efficient Robotic Grasping}} via {{Randomized-to-Canonical Adaptation Networks}}},
  shorttitle = {Sim-to-{{Real}} via {{Sim-to-Sim}}},
  author = {James, Stephen and Wohlhart, Paul and Kalakrishnan, Mrinal and Kalashnikov, Dmitry and Irpan, Alex and Ibarz, Julian and Levine, Sergey and Hadsell, Raia and Bousmalis, Konstantinos},
  date = {2018-12-18},
  eprint = {1812.07252},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1812.07252},
  urldate = {2019-05-09},
  abstract = {Real world data, especially in the domain of robotics, is notoriously costly to collect. One way to circumvent this can be to leverage the power of simulation to produce large amounts of labelled data. However, training models on simulated images does not readily transfer to real-world ones. Using domain adaptation methods to cross this "reality gap" requires a large amount of unlabelled real-world data, whilst domain randomization alone can waste modeling power. In this paper, we present Randomized-to-Canonical Adaptation Networks (RCANs), a novel approach to crossing the visual reality gap that uses no real-world data. Our method learns to translate randomized rendered images into their equivalent non-randomized, canonical versions. This in turn allows for real images to also be translated into canonical sim images. We demonstrate the effectiveness of this sim-to-real approach by training a vision-based closed-loop grasping reinforcement learning agent in simulation, and then transferring it to the real world to attain 70\% zero-shot grasp success on unseen objects, a result that almost doubles the success of learning the same task directly on domain randomization alone. Additionally, by joint finetuning in the real-world with only 5,000 real-world grasps, our method achieves 91\%, attaining comparable performance to a state-of-the-art system trained with 580,000 real-world grasps, resulting in a reduction of real-world data by more than 99\%.},
  keywords = {#nosource,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,machine learning control,sim-to-real},
  annotation = {00006}
}

@article{jamwalImpedanceControlIntrinsically2016,
  title = {Impedance {{Control}} of an {{Intrinsically Compliant Parallel Ankle Rehabilitation Robot}}},
  author = {Jamwal, Prashant K. and Hussain, Shahid and Ghayesh, Mergen H. and Rogozina, Svetlana V.},
  date = {2016-06},
  journaltitle = {IEEE Transactions on Industrial Electronics},
  volume = {63},
  number = {6},
  pages = {3638--3647},
  issn = {1557-9948},
  doi = {10.1109/TIE.2016.2521600},
  abstract = {Robot-aided physical therapy should encourage subject's voluntary participation to achieve rapid motor function recovery. In order to enhance subject's cooperation during training sessions, the robot should allow deviation in the prescribed path depending on the subject's modified limb motions subsequent to the disability. In the present work, an interactive training paradigm based on the impedance control was developed for a lightweight intrinsically compliant parallel ankle rehabilitation robot. The parallel ankle robot is powered by pneumatic muscle actuators (PMAs). The proposed training paradigm allows the patients to modify the robot imposed motions according to their own level of disability. The parallel robot was operated in four training modes namely position control, zero-impedance control, nonzero-impedance control with high compliance, and nonzero-impedance control with low compliance to evaluate the performance of proposed control scheme. The impedance control scheme was evaluated on 10 neurologically intact subjects. The experimental results show that an increase in robotic compliance encouraged subjects to participate more actively in the training process. This work advances the current state of the art in the compliant actuation of parallel ankle rehabilitation robots in the context of interactive training.},
  eventtitle = {{{IEEE Transactions}} on {{Industrial Electronics}}},
  keywords = {Actuators,ankle rehabilitation,Ankle rehabilitation,Impedance,impedance control,Impedance control,parallel robot,Parallel robot,Parallel robots,pneumatic muscle actuators (PMAs),Pneumatic systems,Training,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/jamwal_et_al_2016_impedance_control_of_an_intrinsically_compliant_parallel_ankle_rehabilitation.pdf}
}

@unpublished{jangEndtoEndLearningSemantic2017,
  title = {End-to-{{End Learning}} of {{Semantic Grasping}}},
  author = {Jang, Eric and Vijayanarasimhan, Sudheendra and Pastor, Peter and Ibarz, Julian and Levine, Sergey},
  date = {2017-07-06},
  eprint = {1707.01932},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1707.01932},
  urldate = {2019-05-09},
  abstract = {We consider the task of semantic robotic grasping, in which a robot picks up an object of a user-specified class using only monocular images. Inspired by the two-stream hypothesis of visual reasoning, we present a semantic grasping framework that learns object detection, classification, and grasp planning in an end-to-end fashion. A "ventral stream" recognizes object class while a "dorsal stream" simultaneously interprets the geometric relationships necessary to execute successful grasps. We leverage the autonomous data collection capabilities of robots to obtain a large self-supervised dataset for training the dorsal stream, and use semi-supervised label propagation to train the ventral stream with only a modest amount of human supervision. We experimentally show that our approach improves upon grasping systems whose components are not learned end-to-end, including a baseline method that uses bounding box detection. Furthermore, we show that jointly training our model with auxiliary data consisting of non-semantic grasping data, as well as semantically labeled images without grasp actions, has the potential to substantially improve semantic grasping performance.},
  keywords = {#nosource,Computer Science - Machine Learning,Computer Science - Robotics,object grasping,robotic grasping,robotic reasoning,Statistics - Machine Learning,tno internship}
}

@unpublished{jangGrasp2VecLearningObject2018,
  title = {{{Grasp2Vec}}: {{Learning Object Representations}} from {{Self-Supervised Grasping}}},
  shorttitle = {{{Grasp2Vec}}},
  author = {Jang, Eric and Devin, Coline and Vanhoucke, Vincent and Levine, Sergey},
  date = {2018-11-16},
  eprint = {1811.06964},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1811.06964},
  urldate = {2019-05-09},
  abstract = {Well structured visual representations can make robot learning faster and can improve generalization. In this paper, we study how we can acquire effective object-centric representations for robotic manipulation tasks without human labeling by using autonomous robot interaction with the environment. Such representation learning methods can benefit from continuous refinement of the representation as the robot collects more experience, allowing them to scale effectively without human intervention. Our representation learning approach is based on object persistence: when a robot removes an object from a scene, the representation of that scene should change according to the features of the object that was removed. We formulate an arithmetic relationship between feature vectors from this observation, and use it to learn a representation of scenes and objects that can then be used to identify object instances, localize them in the scene, and perform goal-directed grasping tasks where the robot must retrieve commanded objects from a bin. The same grasping procedure can also be used to automatically collect training data for our method, by recording images of scenes, grasping and removing an object, and recording the outcome. Our experiments demonstrate that this self-supervised approach for tasked grasping substantially outperforms direct reinforcement learning from images and prior representation learning methods.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics,object manipulation,robotic grasping,tno internship},
  annotation = {00004}
}

@unpublished{jaskowskiImprovedGQCNNDeep2018,
  title = {Improved {{GQ-CNN}}: {{Deep Learning Model}} for {{Planning Robust Grasps}}},
  shorttitle = {Improved {{GQ-CNN}}},
  author = {Jaśkowski, Maciej and Świątkowski, Jakub and Zając, Michał and Klimek, Maciej and Potiuk, Jarek and Rybicki, Piotr and Polatowski, Piotr and Walczyk, Przemysław and Nowicki, Kacper and Cygan, Marek},
  date = {2018-02-16},
  eprint = {1802.05992},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1802.05992},
  urldate = {2019-05-09},
  abstract = {Recent developments in the field of robot grasping have shown great improvements in the grasp success rates when dealing with unknown objects. In this work we improve on one of the most promising approaches, the Grasp Quality Convolutional Neural Network (GQ-CNN) trained on the DexNet 2.0 dataset. We propose a new architecture for the GQ-CNN and describe practical improvements that increase the model validation accuracy from 92.2\% to 95.8\% and from 85.9\% to 88.0\% on respectively image-wise and object-wise training and validation splits.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,object grasping,robotic grasping,Statistics - Machine Learning,tno internship},
  annotation = {00002}
}

@inproceedings{jiangEfficientGraspingRgbd2011,
  title = {Efficient Grasping from Rgbd Images: {{Learning}} Using a New Rectangle Representation},
  shorttitle = {Efficient Grasping from Rgbd Images},
  booktitle = {2011 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Jiang, Yun and Moseson, Stephen and Saxena, Ashutosh},
  date = {2011},
  pages = {3304--3311},
  publisher = {{IEEE}},
  keywords = {#nosource,grasp pose detection,robotic grasping,tno internship},
  annotation = {00190}
}

@inproceedings{jinImprovedLearningAccuracy2019,
  title = {Improved {{Learning Accuracy}} for {{Learning Stable Control}} from {{Human Demonstrations}}},
  booktitle = {2019 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Jin, Shaokun and Wang, Zhiyang and Ou, Yongsheng and Zhou, Yimin},
  date = {2019-11},
  pages = {2679--2685},
  issn = {2153-0866},
  doi = {10.1109/IROS40897.2019.8968274},
  abstract = {Learning from Demonstration (LfD) has been identified as an effective method for making robots adapt to a similar kind of tasks. In this work, a framework of learning from demonstration has been proposed for modelling robot motions. We present an approach based on dimension ascending to learn a dynamical system, so that the reproduced motions can closely follow the demonstrations. In addition, the reproductions can ultimately reach and stop at the target, which reflects the robustness of the method. Therefore, the system accuracy and stability can be better guaranteed simultaneously. The effectiveness of the proposed approach is verified by performing handwriting experiments on the LASA data set.},
  eventtitle = {2019 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  keywords = {Analytical models,dynamical systems,Learning from demonstration,point-to-point motions,READ,Robot motion,Robustness,Stability analysis,stability analysis.,Task analysis,Trajectory,Transforms},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/jin_et_al_2019_improved_learning_accuracy_for_learning_stable_control_from_human_demonstrations.pdf}
}

@article{jinLearningAccurateStable2019,
  title = {Learning {{Accurate}} and {{Stable Dynamical System Under Manifold Immersion}} and {{Submersion}}},
  author = {Jin, Shaokun and Wang, Zhiyang and Ou, Yongsheng and Feng, Wei},
  date = {2019-12},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {30},
  number = {12},
  pages = {3598--3610},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2019.2892207},
  abstract = {Learning from demonstration (LfD) has been increasingly used to encode robot tasks such that robots can achieve reproduction more flexibly in unstructured environments (e.g., households or factories). It is an effective alternative to preprogramming methods owing to its capacity of enabling robots to generalize to different situations. In this paper, we focus on LfD in the point-to-point movement case, where the dilemma of stability and accuracy exists. To avoid such a dilemma, we propose a learning approach that guarantees accuracy and stability simultaneously by means of constructed manifold immersion and submersion. We evaluate the proposed approach on two libraries of human handwriting motions (the LASA data set and a self-made GREEK data set) and on a set of experiments on the Barrett WAM robot.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}} and {{Learning Systems}}},
  keywords = {Learning from demonstration (LfD),Learning systems,Lyapunov methods,nonlinear dynamical system (DS),Nonlinear dynamical systems,READ,robotics,Service robots,stability analysis,Stability analysis,stable estimator of DSs (SEDSs),Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/jin_et_al_2019_learning_accurate_and_stable_dynamical_system_under_manifold_immersion_and.pdf}
}

@article{jinLearningNeuralshapedQuadratic2023,
  title = {Learning Neural-Shaped Quadratic {{Lyapunov}} Function for Stable, Accurate and Generalizable Human–Robot Skills Transfer},
  author = {Jin, Zhehao and Qin, Dongdong and Liu, Andong and Zhang, Wen-An and Yu, Li},
  date = {2023-08-01},
  journaltitle = {Robotics and Computer-Integrated Manufacturing},
  shortjournal = {Robotics and Computer-Integrated Manufacturing},
  volume = {82},
  pages = {102526},
  issn = {0736-5845},
  doi = {10.1016/j.rcim.2023.102526},
  url = {https://www.sciencedirect.com/science/article/pii/S0736584523000029},
  urldate = {2023-04-18},
  abstract = {Learning a stable dynamic system (DS) encoding human motion rules has been shown as an efficient approach for transferring motion skills. However, contradictions always exist between the stability, accuracy and generalization of the learned DS. This paper presents an approach to enhance the accuracy and generalization by learning a neural-shaped quadratic Lyapunov function (NS-QLF). For the stability concern, the NS-QLF is designed to satisfy LF basic properties. Thanks to the flexibility of the neural network, the NS-QLF shape can capture motion rules in a broad area. The corresponding neural-learning problem is formulated as a convex optimization problem. We then learn an original DS (ODS) by using the Gaussian process regression (GPR) algorithm and stabilize the ODS by solving an NS-QLF-constrained convex optimization problem. The resulted stable DS (SDS) can not only accurately reproduce trajectories near the demonstration area, but also can utilize the NS-QLF shape information to enhance the generalization capacity in regions away from the demonstration area. Various comparative simulations and experiments are conducted to show the benefits of the presented approach.},
  langid = {english},
  keywords = {Convex optimization,Dynamic system,Learning from demonstration,Lyapunov function,Neural network,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/jin_et_al_2023_learning_neural-shaped_quadratic_lyapunov_function_for_stable,_accurate_and.pdf}
}

@article{jinModelPredictiveVariable2023,
  title = {Model {{Predictive Variable Impedance Control}} of {{Manipulators}} for {{Adaptive Precision-Compliance Tradeoff}}},
  author = {Jin, Zhehao and Qin, Dongdong and Liu, Andong and Zhang, Wen-an and Yu, Li},
  date = {2023-04},
  journaltitle = {IEEE/ASME Transactions on Mechatronics},
  volume = {28},
  number = {2},
  pages = {1174--1186},
  issn = {1941-014X},
  doi = {10.1109/TMECH.2022.3204350},
  abstract = {Impedance control (IC) is widely used in contact-rich manipulator tasks since it provides manipulators with both operation precision and contact compliance, and trades them off by impedance parameters. However, fixed impedance parameters limit the applicability of IC in complex tasks during which the focus on the operation precision or the contact compliance is variable, such as physical human–robot collaboration and complex assembly tasks. This article presents model predictive variable impedance control approaches for adaptive precision-compliance tradeoff to satisfy variable task requirements. Specifically, we establish a novel impedance model, which transforms the variable impedance law design problem into a control law design problem, and allows us to consider novel impedance constraints that can determine manipulators' extreme precision and compliance properties. According to whether state constraints are further considered, one-step (OS) and multistep (MS) model predictive control approaches are proposed to solve these transformed control problems, and the corresponding optimization problem of the OS MPC can be established as a QP problem due to the special form of the novel impedance model. A tank-based approach is further proposed to correct the variable impedance laws for the system passivity concern. Various comparative experiments are conducted to validate the effectiveness of the presented approaches.},
  eventtitle = {{{IEEE}}/{{ASME Transactions}} on {{Mechatronics}}},
  keywords = {Aerospace electronics,Cost function,Impedance,Model predictive control,optimization,precision-compliance tradeoff,Prediction algorithms,Predictive models,READ,Robots,Task analysis,variable impedance control (VIC)},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/jin_et_al_2023_model_predictive_variable_impedance_control_of_manipulators_for_adaptive.pdf}
}

@article{jinOptimalVariableImpedance2022,
  title = {An {{Optimal Variable Impedance Control With Consideration}} of the {{Stability}}},
  author = {Jin, Zhehao and Liu, Andong and Zhang, Wen-an and Yu, Li},
  date = {2022-04},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {2},
  pages = {1737--1744},
  issn = {2377-3766},
  doi = {10.1109/LRA.2022.3141759},
  abstract = {This letter presents an approach to develop a variable impedance controller with considerations of the optimality and stability. Firstly, an original optimal variable law is designed via demonstration learning, through which the Gaussian mixture model/Gaussian mixture regression (GMM/GMR) algorithm is employed to transfer the human impedance functions to the robot. By using the formulation of the GMR result, the regression task can be completed without using the ground-truth information of the human impedance parameters. To ensure the stability, a minimal complementary input is designed for the learned second-order impedance system. We transform the design problem to a constrained convex optimization problem, of which the constraints are related to a Lyapunov function. A criterion for choosing the Lyapunov functions is presented to ensure the feasibility of the problem, and an analytical solution is computed. The proposed approach is verified by the robotic-assisted rehabilitation and trajectory reproduction experiments conducted on a 7-DOF Franka Panda robot.},
  eventtitle = {{{IEEE Robotics}} and {{Automation Letters}}},
  keywords = {demonstration learning,gaussian mixture regression,GMR,Heuristic algorithms,Impedance,IMPORTANT,manipulation,Optimization,READ,robots,Robots,STABILITY,stability and   optimality,stability and optimality,Stability criteria,Task analysis,Trajectory,Variable impedance control},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/jin_et_al_2022_an_optimal_variable_impedance_control_with_consideration_of_the_stability.pdf}
}

@unpublished{julianScalingSimulationtorealTransfer2018,
  title = {Scaling Simulation-to-Real Transfer by Learning Composable Robot Skills},
  author = {Julian, Ryan and Heiden, Eric and He, Zhanpeng and Zhang, Hejia and Schaal, Stefan and Lim, Joseph J. and Sukhatme, Gaurav and Hausman, Karol},
  date = {2018-09-26},
  eprint = {1809.10253},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1809.10253},
  urldate = {2019-05-09},
  abstract = {We present a novel solution to the problem of simulation-to-real transfer, which builds on recent advances in robot skill decomposition. Rather than focusing on minimizing the simulation-reality gap, we learn a set of diverse policies that are parameterized in a way that makes them easily reusable. This diversity and parameterization of low-level skills allows us to find a transferable policy that is able to use combinations and variations of different skills to solve more complex, high-level tasks. In particular, we first use simulation to jointly learn a policy for a set of low-level skills, and a "skill embedding" parameterization which can be used to compose them. Later, we learn high-level policies which actuate the low-level policies via this skill embedding parameterization. The high-level policies encode how and when to reuse the low-level skills together to achieve specific high-level tasks. Importantly, our method learns to control a real robot in joint-space to achieve these high-level tasks with little or no on-robot time, despite the fact that the low-level policies may not be perfectly transferable from simulation to real, and that the low-level skills were not trained on any examples of high-level tasks. We illustrate the principles of our method using informative simulation experiments. We then verify its usefulness for real robotics problems by learning, transferring, and composing free-space and contact motion skills on a Sawyer robot using only joint-space control. We experiment with several techniques for composing pre-learned skills, and find that our method allows us to use both learning-based approaches and efficient search-based planning to achieve high-level tasks using only pre-learned skills.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,machine learning control,sim-to-real,Statistics - Machine Learning},
  annotation = {00002}
}

@article{jungForceTrackingImpedance2004,
  title = {Force Tracking Impedance Control of Robot Manipulators under Unknown Environment},
  author = {Jung, Seul and Hsia, T.C. and Bonitz, R.G.},
  date = {2004-05},
  journaltitle = {IEEE Transactions on Control Systems Technology},
  volume = {12},
  number = {3},
  pages = {474--483},
  issn = {1558-0865},
  doi = {10.1109/TCST.2004.824320},
  abstract = {In this paper, a new simple stable force tracking impedance control scheme that has the capability to track a specified desired force and to compensate for uncertainties in environment location and stiffness as well as in robot dynamic model is proposed. The uncertainties in robot dynamics are compensated by the robust position control algorithm. After contact, in force controllable direction the new impedance function is realized based on a desired force, environment stiffness and a position error. The new impedance function is simple and stable. The force error is minimized by using an adaptive technique. Stability and convergence of the adaptive technique are analyzed for a stable force tracking execution. Simulation studies with a three link rotary robot manipulator are shown to demonstrate the robustness of the proposed scheme under uncertainties in robot dynamics, and little knowledges of environment position and environment stiffness. Experimental results are carried out to confirm the proposed controller's performance.},
  eventtitle = {{{IEEE Transactions}} on {{Control Systems Technology}}},
  keywords = {Convergence,Error correction,Force control,Impedance,Manipulator dynamics,Position control,Robot control,Robust control,Stability analysis,Uncertainty},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/jung_et_al_2004_force_tracking_impedance_control_of_robot_manipulators_under_unknown_environment2.pdf}
}

@unpublished{kaiserModelBasedReinforcementLearning2019,
  title = {Model-{{Based Reinforcement Learning}} for {{Atari}}},
  author = {Kaiser, Lukasz and Babaeizadeh, Mohammad and Milos, Piotr and Osinski, Blazej and Campbell, Roy H. and Czechowski, Konrad and Erhan, Dumitru and Finn, Chelsea and Kozakowski, Piotr and Levine, Sergey and Sepassi, Ryan and Tucker, George and Michalewski, Henryk},
  date = {2019-03-01},
  eprint = {1903.00374},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1903.00374},
  urldate = {2019-04-18},
  abstract = {Model-free reinforcement learning (RL) can be used to learn effective policies for complex tasks, such as Atari games, even from image observations. However, this typically requires very large amounts of interaction – substantially more, in fact, than a human would need to learn the same games. How can people learn so quickly? Part of the answer may be that people can learn how the game works and predict which actions will lead to desirable outcomes. In this paper, we explore how video prediction models can similarly enable agents to solve Atari games with orders of magnitude fewer interactions than model-free methods. We describe Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models and present a comparison of several model architectures, including a novel architecture that yields the best results in our setting. Our experiments evaluate SimPLe on a range of Atari games and achieve competitive results with only 100K interactions between the agent and the environment (400K frames), which corresponds to about two hours of real-time play.},
  langid = {english},
  keywords = {#nosource,Computer Science - Machine Learning,machine learning control,Statistics - Machine Learning}
}

@inproceedings{kakadeApproximatelyOptimalApproximate2002,
  title = {Approximately {{Optimal Approximate Reinforcement Learning}}},
  booktitle = {In {{Proc}}. 19th {{International Conference}} on {{Machine Learning}}},
  author = {Kakade, Sham and Langford, John},
  date = {2002},
  pages = {267--274},
  abstract = {In order to solve realistic reinforcement  learning problems, it is critical  that approximate algorithms be used. In this paper,},
  keywords = {#nosource,classical RL,reinforcement learning},
  annotation = {00396}
}

@inproceedings{kakadeNaturalPolicyGradient2001,
  title = {A {{Natural Policy Gradient}}},
  author = {Kakade, Sham},
  date = {2001-01-01},
  volume = {14},
  pages = {1531--1538},
  abstract = {We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as dened by Sutton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris.},
  eventtitle = {Adv. {{Neural Inf}}. {{Process Syst}}.},
  keywords = {#nosource,classical RL,reinforcement learning},
  annotation = {00694}
}

@inproceedings{kalakrishnanLearningForceControl2011,
  title = {Learning Force Control Policies for Compliant Manipulation},
  booktitle = {2011 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Kalakrishnan, Mrinal and Righetti, Ludovic and Pastor, Peter and Schaal, Stefan},
  date = {2011},
  pages = {4639--4644},
  publisher = {{IEEE}},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/kalakrishnan_et_al_2011_learning_force_control_policies_for_compliant_manipulation.pdf}
}

@unpublished{kalashnikovQTOptScalableDeep2018,
  title = {{{QT-Opt}}: {{Scalable Deep Reinforcement Learning}} for {{Vision-Based Robotic Manipulation}}},
  shorttitle = {{{QT-Opt}}},
  author = {Kalashnikov, Dmitry and Irpan, Alex and Pastor, Peter and Ibarz, Julian and Herzog, Alexander and Jang, Eric and Quillen, Deirdre and Holly, Ethan and Kalakrishnan, Mrinal and Vanhoucke, Vincent and Levine, Sergey},
  date = {2018-06-27},
  eprint = {1806.10293},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1806.10293},
  urldate = {2019-05-09},
  abstract = {In this paper, we study the problem of learning vision-based dynamic manipulation skills using a scalable reinforcement learning approach. We study this problem in the context of grasping, a longstanding challenge in robotic manipulation. In contrast to static learning behaviors that choose a grasp point and then execute the desired grasp, our method enables closed-loop vision-based control, whereby the robot continuously updates its grasp strategy based on the most recent observations to optimize long-horizon grasp success. To that end, we introduce QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that can leverage over 580k real-world grasp attempts to train a deep neural network Q-function with over 1.2M parameters to perform closed-loop, real-world grasping that generalizes to 96\% grasp success on unseen objects. Aside from attaining a very high success rate, our method exhibits behaviors that are quite distinct from more standard grasping systems: using only RGB vision-based perception from an over-the-shoulder camera, our method automatically learns regrasping strategies, probes objects to find the most effective grasps, learns to reposition objects and perform other non-prehensile pre-grasp manipulations, and responds dynamically to disturbances and perturbations.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,object grasping,real-world,reinforcement learning,robotic grasping,Statistics - Machine Learning,tno internship},
  annotation = {00037}
}

@inproceedings{kambojDiscreteTimeLyapunovBased2020,
  title = {Discrete-{{Time Lyapunov}} Based {{Kinematic Control}} of {{Robot Manipulator}} Using {{Actor-Critic Framework}}},
  booktitle = {2020 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Kamboj, Ankur and Prakash, Ravi and Mohanta, Jayant Kumar and Behera, Laxmidhar},
  date = {2020-07},
  pages = {1--7},
  issn = {2161-4407},
  doi = {10.1109/IJCNN48605.2020.9207522},
  abstract = {Stability and optimality are the two foremost re-quirements for robotic systems that are deployed in critical operations and are to work for long hours or under limited energy resources. To address these, in this work we present a novel Lyapunov stability based discrete-time optimal kinematic control of a robot manipulator using actor-critic (AC) framework. The robot is actuated using optimal joint-space velocity control input to track a time-varying end-effector trajectory in its task space. In comparison to the existing near-optimal kinematic control solutions for robot manipulator under AC framework, proposed controller exhibits guaranteed analytical stability. We derive a novel critic weight update law based on Lyapunov stability, thus ensuring that the weights are updated along the negative gradient of Lyapunov function. This eventually ensures closed-loop system stability and convergence to the optimal control in discrete-time. Extensive simulations are performed on a 3D model of 6-DoF Universal Robot (UR) 10 in Gazebo, followed by implementation on real UR 10 robot manipulator to show the efficacy of the proposed scheme.},
  eventtitle = {2020 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  keywords = {Actor-Critic,Artificial neural networks,Discrete-Time Kinematic Control,Kinematics,Lyapunov Stability,Manipulators,Mathematical model,Optimal control,READ,Robot Manipulator,Stability analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/kamboj_et_al_2020_discrete-time_lyapunov_based_kinematic_control_of_robot_manipulator_using.pdf}
}

@unpublished{kamtheDataEfficientReinforcementLearning2017,
  title = {Data-{{Efficient Reinforcement Learning}} with {{Probabilistic Model Predictive Control}}},
  author = {Kamthe, Sanket and Deisenroth, Marc Peter},
  date = {2017-06-20},
  eprint = {1706.06491},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1706.06491},
  urldate = {2019-04-18},
  abstract = {Trial-and-error based reinforcement learning (RL) has seen rapid advancements in recent times, especially with the advent of deep neural networks. However, the majority of autonomous RL algorithms require a large number of interactions with the environment. A large number of interactions may be impractical in many real-world applications, such as robotics, and many practical systems have to obey limitations in the form of state space or control constraints. To reduce the number of system interactions while simultaneously handling constraints, we propose a modelbased RL framework based on probabilistic Model Predictive Control (MPC). In particular, we propose to learn a probabilistic transition model using Gaussian Processes (GPs) to incorporate model uncertainty into longterm predictions, thereby, reducing the impact of model errors. We then use MPC to find a control sequence that minimises the expected long-term cost. We provide theoretical guarantees for first-order optimality in the GP-based transition models with deterministic approximate inference for long-term planning. We demonstrate that our approach does not only achieve state-of-the-art data efficiency, but also is a principled way for RL in constrained environments.},
  langid = {english},
  keywords = {#nosource,Computer Science - Systems and Control,machine learning control,safe-rl,Statistics - Machine Learning}
}

@inproceedings{kapturowskiRecurrentExperienceReplay2018,
  title = {Recurrent {{Experience Replay}} in {{Distributed Reinforcement Learning}}},
  author = {Kapturowski, Steven and Ostrovski, Georg and Quan, John and Munos, Remi and Dabney, Will},
  date = {2018-09-27},
  url = {https://openreview.net/forum?id=r1lyTjAqYX},
  urldate = {2020-07-02},
  abstract = {Building on the recent successes of distributed training of RL agents, in this paper we investigate the training of RNN-based RL agents from distributed prioritized experience replay. We study the...},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  keywords = {#nosource,reinforcement learning,scaling RL}
}

@inproceedings{karacanPassivityBasedSkillMotion2022,
  title = {Passivity-{{Based Skill Motion Learning}} in {{Stiffness-Adaptive Unified Force-Impedance Control}}},
  booktitle = {2022 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Karacan, Kübra and Sadeghian, Hamid and Kirschner, Robin and Haddadin, Sami},
  date = {2022-10},
  pages = {9604--9611},
  issn = {2153-0866},
  doi = {10.1109/IROS47612.2022.9981728},
  abstract = {Tactile robots shall be deployed for dynamic task execution in production lines with small batch sizes. Therefore, these robots should have the ability to respond to changing conditions and be easy to (re-)program. Operating under uncertain environments requires unifying subsystems such as robot motion and force policy into one framework, referred to as tactile skills. In this paper, we propose the enhancement of these skills for passivity-based skill motion learning in stiffness-adaptive unified force-impedance control. To achieve the increased level of adaptability, we represent all tactile skills by three basic primitives: contact initiation, manipulation, and contact termination. To ensure passivity and stability, we develop an energy-based approach for unified force-impedance control that allows humans to teach the robot motion through physical interaction during the execution of a tactile task. We incorporate our proposed framework into a tactile robot to experimentally validate the motion adaptation by interaction performance and stability of the control. While the polishing task is presented as our use case through the paper, the experiments can also be carried out with various tactile skills. Finally, the results show the novel controller's stability and passivity to contact-loss and stiffness adaptation, leading to successful programming by interaction.},
  eventtitle = {2022 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  keywords = {Dynamics,Energy exchange,Force,Production,READ,Robot motion,Surface impedance,Task analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/karacan_et_al_2022_passivity-based_skill_motion_learning_in_stiffness-adaptive_unified.pdf}
}

@inproceedings{kastritsiPHRIFrameworkModifying2018,
  title = {A {{pHRI Framework}} for {{Modifying}} a {{Robot}}'s {{Kinematic Behaviour}} via {{Varying Stiffness}} and {{Dynamical System Synchronization}}},
  booktitle = {2018 26th {{Mediterranean Conference}} on {{Control}} and {{Automation}} ({{MED}})},
  author = {Kastritsi, Theodora and Sidiropoulos, Antonis and Doulgeri, Zoe},
  date = {2018-06},
  pages = {1--6},
  issn = {2473-3504},
  doi = {10.1109/MED.2018.8442577},
  abstract = {In this work, we propose a robot control framework for modifying a desired robot kinematic behavior encoded in dynamical movement primitives (DMP) by physically interacting with the robot during its autonomous operation. The proposed method is based on variable stiffness and DMP time synchronization with the user during the interaction. The overall controlled system is proved to be stable. After the user stops interacting with the robot, the robot motion continues according to the learned kinematic behavior until it reaches the final task goal. At the next execution cycle a new DMP can been learned to generate the modified trajectory. In this way, explicit robot programming and separation of learning and execution stages is eliminated. The proposed approach is implemented and evaluated on a 7-degree-of-freedom KUKA LWR4.},
  eventtitle = {2018 26th {{Mediterranean Conference}} on {{Control}} and {{Automation}} ({{MED}})},
  keywords = {Force,Kinematics,READ,Robot sensing systems,Synchronization,Task analysis,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/kastritsi_et_al_2018_a_phri_framework_for_modifying_a_robot's_kinematic_behaviour_via_varying.pdf}
}

@article{kastritsiProgressiveAutomationDMP2018,
  title = {Progressive {{Automation}} with {{DMP Synchronization}} and {{Variable Stiffness Control}}},
  author = {Kastritsi, Theodora and Dimeas, Fotios and Doulgeri, Zoe},
  date = {2018-10},
  journaltitle = {Ieee Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {3},
  number = {4},
  pages = {3789--3796},
  publisher = {{Ieee-Inst Electrical Electronics Engineers Inc}},
  location = {{Piscataway}},
  issn = {2377-3766},
  doi = {10.1109/LRA.2018.2856536},
  url = {https://ieeexplore.ieee.org/document/8411480},
  urldate = {2022-05-02},
  abstract = {Progressive automation is a method that allows the operator to demonstrate a task a few times until the robot gradually learns to execute it autonomously. In this letter, we combine a novel structure of dynamic movement primitives (DMP) with variable stiffness control, to allow synchronization with the demonstrated motion during the operator's intervention. This structure enables the DMP to speed up or slow down depending on the human demonstration. In addition, we present a variable stiffness controller to change the role of the robot between the follower and the leader based on the trucking error of the robot from the proposed DMP and on the guidance forces. The proposed variable stiffness controller is proved to be passive using energy tanks. The effectiveness of the proposed method is demonstrated experimentally.},
  langid = {english},
  keywords = {adaptation,Automation,compliance and impedance control,DMP,dynamic movement primitives,energy tanks,Learning from demonstration,physical human-robot interaction,Robot kinematics,STABILITY,Synchronization,Task analysis,Tracking,Trajectory},
  annotation = {WOS:000441444700015},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/kastritsi_et_al_2018_progressive_automation_with_dmp_synchronization_and_variable_stiffness_control.pdf}
}

@article{katsikopoulosEngelbrechtMarkovDecision2003,
  title = {Engelbrecht, {{S}}.{{E}}.: {{Markov}} Decision Processes with Delays and Asynchronous Cost Collection. {{IEEE Trans}}. {{Autom}}. {{Control}} 48(4), 568-574},
  shorttitle = {Engelbrecht, {{S}}.{{E}}.},
  author = {Katsikopoulos, Konstantinos and Engelbrecht, S.E.},
  date = {2003-05-01},
  journaltitle = {Automatic Control, IEEE Transactions on},
  shortjournal = {Automatic Control, IEEE Transactions on},
  volume = {48},
  pages = {568--574},
  doi = {10.1109/TAC.2003.809799},
  abstract = {Markov decision processes (MDPs) may involve three types of delays. First, state information, rather than being available instantaneously, may arrive with a delay (observation delay). Second, an action may take effect at a later decision stage rather than immediately (action delay). Third, the cost induced by an action may be collected after a number of stages (cost delay). We de rive two results, one for constant and one for random delays, for reducing an MDP with delays to an MDP without delays, which differs only in the size of the state space. The results are based on the intuition that costs may be collected asynchronously, i.e., at a stage other than the one in which they are induced, as long as they are discounted properly.},
  keywords = {#nosource}
}

@inproceedings{kerberCompositionalPropertiesPassivity2011,
  title = {Compositional Properties of Passivity},
  booktitle = {2011 50th {{IEEE Conference}} on {{Decision}} and {{Control}} and {{European Control Conference}}},
  author = {Kerber, Florian and family=Schaft, given=Arjan, prefix=van der, useprefix=true},
  date = {2011-12},
  pages = {4628--4633},
  issn = {0743-1546},
  doi = {10.1109/CDC.2011.6160591},
  abstract = {The classical passivity theorem states that the negative feedback interconnection of passive systems is again passive. The converse statement, - passivity of the interconnected system implies passivity of the subsystems -, turns out to be equally valid. This result implies that among all feasible storage functions of a passive interconnected system there is always one that is the sum of storage functions of the subsystems. Sufficient conditions guaranteeing that all storage functions are of this type are derived. Closely related is the question when and how the stability of the closed-loop interconnected system implies passivity of the subsystems. We recall a folklore theorem which was proved for SISO linear systems, and derive some preliminary results towards a more general result, using the theory of simulation relations.},
  eventtitle = {2011 50th {{IEEE Conference}} on {{Decision}} and {{Control}} and {{European Control Conference}}},
  keywords = {Additives,Cognition,Integrated circuit interconnections,Interconnected systems,Negative feedback,Nonlinear systems,Vectors},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/kerber_van_der_schaft_2011_compositional_properties_of_passivity.pdf}
}

@online{KeyPapersDeep,
  title = {Key {{Papers}} in {{Deep RL}} — {{Spinning Up}} Documentation},
  url = {https://spinningup.openai.com/en/latest/spinningup/keypapers.html},
  urldate = {2020-06-29},
  keywords = {#nosource,deep reinforcement learning,feed,reinforcement learning},
  annotation = {00000}
}

@article{khaderDataefficientModelLearning2020,
  title = {Data-Efficient {{Model Learning}} and {{Prediction}} for {{Contact-rich Manipulation Tasks}}},
  author = {Khader, Shahbaz Abdul and Yin, Hang and Falco, Pietro and Kragic, Danica},
  date = {2020-07},
  journaltitle = {IEEE Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {5},
  number = {3},
  eprint = {1909.04915},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {4321--4328},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2020.2996067},
  url = {http://arxiv.org/abs/1909.04915},
  urldate = {2022-05-24},
  abstract = {In this letter, we investigate learning forward dynamics models and multi-step prediction of state variables (long-term prediction) for contact-rich manipulation. The problems are formulated in the context of model-based reinforcement learning (MBRL). We focus on two aspects-discontinuous dynamics and data-efficiency-both of which are important in the identified scope and pose significant challenges to State-of-the-Art methods. We contribute to closing this gap by proposing a method that explicitly adopts a specific hybrid structure for the model while leveraging the uncertainty representation and data-efficiency of Gaussian process. Our experiments on an illustrative moving block task and a 7-DOF robot demonstrate a clear advantage when compared to popular baselines in low data regimes.},
  keywords = {Computer Science - Robotics},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/khader_et_al_2020_data-efficient_model_learning_and_prediction_for_contact-rich_manipulation_tasks.pdf}
}

@article{khaderLearningDeepEnergy2021,
  title = {Learning {{Deep Energy Shaping Policies}} for {{Stability-Guaranteed Manipulation}}},
  author = {Khader, Shahbaz Abdul and Yin, Hang and Falco, Pietro and Kragic, Danica},
  date = {2021-10},
  journaltitle = {Ieee Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {6},
  number = {4},
  pages = {8583--8590},
  publisher = {{Ieee-Inst Electrical Electronics Engineers Inc}},
  location = {{Piscataway}},
  issn = {2377-3766},
  doi = {10.1109/LRA.2021.3111962},
  url = {https://ieeexplore.ieee.org/document/9536404},
  urldate = {2022-05-02},
  abstract = {Deep reinforcement learning (DRL) has been successfully used to solve various robotic manipulation tasks. However, most of the existing works do not address the issue of control stability. This is in sharp contrast to the control theory community where the well-established norm is to prove stability whenever a control law is synthesized. What makes traditional stability analysis difficult for DRL are the uninterpretable nature of the neural network policies and unknown system dynamics. In this work, stability is obtained by deriving an interpretable deep policy structure based on the energy shaping control of Lagrangian systems. Then, stability during physical interaction with an unknown environment is established based on passivity. The result is a stability guaranteeing DRL in a model-free framework that is general enough for contact-rich manipulation tasks. With an experiment on a peg-in-hole task, we demonstrate, to the best of our knowledge, the first DRL with stability guarantee on a real robotic manipulator.},
  langid = {english},
  keywords = {IMPORTANT,Machinelearning for robot control,reinforcement learning,STABILITY},
  annotation = {WOS:000701239400004},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/khader_et_al_2021_learning_deep_energy_shaping_policies_for_stability-guaranteed_manipulation.pdf}
}

@unpublished{khaderLearningDeepNeural2021,
  title = {Learning {{Deep Neural Policies}} with {{Stability Guarantees}}.},
  author = {Khader, Shahbaz Abdul and Yin, Hang and Falco, Pietro and Kragic, Danica},
  date = {2021},
  eprint = {2103.16432},
  eprinttype = {arxiv},
  keywords = {IMPORTANT},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/khader_et_al_2021_learning_deep_neural_policies_with_stability_guarantees.pdf}
}

@inproceedings{khaderLearningStableNormalizingFlow2021,
  title = {Learning {{Stable Normalizing-Flow Control}} for {{Robotic Manipulation}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Khader, Shahbaz Abdul and Yin, Hang and Falco, Pietro and Kragic, Danica},
  date = {2021-05},
  pages = {1644--1650},
  issn = {2577-087X},
  doi = {10.1109/ICRA48506.2021.9562071},
  abstract = {Reinforcement Learning (RL) of robotic manipulation skills, despite its impressive successes, stands to benefit from incorporating domain knowledge from control theory. One of the most important properties that is of interest is control stability. Ideally, one would like to achieve stability guarantees while staying within the framework of state-of-the-art deep RL algorithms. Such a solution does not exist in general, especially one that scales to complex manipulation tasks. We contribute towards closing this gap by introducing normalizing-flow control structure, that can be deployed in any latest deep RL algorithms. While stable exploration is not guaranteed, our method is designed to ultimately produce deterministic controllers with provable stability. In addition to demonstrating our method on challenging contact-rich manipulation tasks, we also show that it is possible to achieve considerable exploration efficiency–reduced state space coverage and actuation efforts– without losing learning efficiency.},
  eventtitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  keywords = {Aerospace electronics,Automation,Conferences,Control theory,Reinforcement learning,Space exploration,STABILITY,Task analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/khader_et_al_2021_learning_stable_normalizing-flow_control_for_robotic_manipulation.pdf}
}

@unpublished{khaderStabilityGuaranteedReinforcementLearning2020,
  title = {Stability-{{Guaranteed Reinforcement Learning}} for {{Contact-rich Manipulation}}},
  author = {Khader, Shahbaz A. and Yin, Hang and Falco, Pietro and Kragic, Danica},
  date = {2020-09-27},
  eprint = {2004.10886},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2004.10886},
  urldate = {2022-04-20},
  abstract = {Reinforcement learning (RL) has had its fair share of success in contact-rich manipulation tasks but it still lags behind in benefiting from advances in robot control theory such as impedance control and stability guarantees. Recently, the concept of variable impedance control (VIC) was adopted into RL with encouraging results. However, the more important issue of stability remains unaddressed. To clarify the challenge in stable RL, we introduce the term all-the-time-stability that unambiguously means that every possible rollout will be stability certified. Our contribution is a model-free RL method that not only adopts VIC but also achieves all-the-time-stability. Building on a recently proposed stable VIC controller as the policy parameterization, we introduce a novel policy search algorithm that is inspired by Cross-Entropy Method and inherently guarantees stability. Our experimental studies confirm the feasibility and usefulness of stability guarantee and also features, to the best of our knowledge, the first successful application of RL with all-the-time-stability on the benchmark problem of peg-in-hole.},
  keywords = {Computer Science - Robotics,IMPORTANT,READ,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/khader_et_al_2020_stability-guaranteed_reinforcement_learning_for_contact-rich_manipulation.pdf}
}

@online{khadirTeleoperatorImitationContinuoustime2019,
  title = {Teleoperator {{Imitation}} with {{Continuous-time Safety}}},
  author = {Khadir, Bachir El and Varley, Jake and Sindhwani, Vikas},
  date = {2019-05-23},
  eprint = {1905.09499},
  eprinttype = {arxiv},
  eprintclass = {cs, math},
  url = {http://arxiv.org/abs/1905.09499},
  urldate = {2022-09-12},
  abstract = {Learning to effectively imitate human teleoperators, with generalization to unseen and dynamic environments, is a promising path to greater autonomy enabling robots to steadily acquire complex skills from supervision. We propose a new motion learning technique rooted in contraction theory and sum-of-squares programming for estimating a control law in the form of a polynomial vector field from a given set of demonstrations. Notably, this vector field is provably optimal for the problem of minimizing imitation loss while providing continuous-time guarantees on the induced imitation behavior. Our method generalizes to new initial and goal poses of the robot and can adapt in real-time to dynamic obstacles during execution, with convergence to teleoperator behavior within a well-defined safety tube. We present an application of our framework for pick-and-place tasks in the presence of moving obstacles on a 7-DOF KUKA IIWA arm. The method compares favorably to other learning-from-demonstration approaches on benchmark handwriting imitation tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics,Mathematics - Optimization and Control,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/khadir_et_al_2019_teleoperator_imitation_with_continuous-time_safety.pdf}
}

@book{khalilNonlinearControl2015,
  title = {Nonlinear Control},
  author = {Khalil, Hassan K.},
  date = {2015},
  volume = {406},
  publisher = {{Pearson New York}},
  keywords = {IMPORTANT},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/khalil_2015_nonlinear_control.pdf}
}

@book{khalilNonlinearSystems2002,
  title = {Nonlinear {{Systems}}},
  author = {Khalil, Hassan K.},
  date = {2002},
  eprint = {t_d1QgAACAAJ},
  eprinttype = {googlebooks},
  publisher = {{Prentice Hall}},
  abstract = {This book is written is such a way that the level of mathematical sophistication builds up from chapter to chapter. It has been reorganized into four parts: basic analysis, analysis of feedback systems, advanced analysis, and nonlinear feedback control.   Updated content includes subjects which have proven useful in nonlinear control design in recent years—new in the 3rd edition are: expanded treatment of passivity and passivity-based control; integral control, high-gain feedback, recursive methods, optimal stabilizing control, control Lyapunov functions, and observers.   For use as a self-study or reference guide by engineers and applied mathematicians.},
  isbn = {978-0-13-067389-3},
  langid = {english},
  pagetotal = {750},
  keywords = {IMPORTANT,Mathematics / Linear & Nonlinear Programming},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/khalil_2002_nonlinear_systems.pdf}
}

@article{khansari-zadehDynamicalSystemApproach2012,
  title = {A Dynamical System Approach to Realtime Obstacle Avoidance},
  author = {Khansari-Zadeh, Seyed Mohammad and Billard, Aude},
  date = {2012-05-01},
  journaltitle = {Autonomous Robots},
  shortjournal = {Auton Robot},
  volume = {32},
  number = {4},
  pages = {433--454},
  issn = {1573-7527},
  doi = {10.1007/s10514-012-9287-y},
  url = {https://doi.org/10.1007/s10514-012-9287-y},
  urldate = {2022-12-15},
  abstract = {This paper presents a novel approach to real-time obstacle avoidance based on Dynamical Systems (DS) that ensures impenetrability of multiple convex shaped objects. The proposed method can be applied to perform obstacle avoidance in Cartesian and Joint spaces and using both autonomous and non-autonomous DS-based controllers. Obstacle avoidance proceeds by modulating the original dynamics of the controller. The modulation is parameterizable and allows to determine a safety margin and to increase the robot’s reactiveness in the face of uncertainty in the localization of the obstacle. The method is validated in simulation on different types of DS including locally and globally asymptotically stable DS, autonomous and non-autonomous DS, limit cycles, and unstable DS. Further, we verify it in several robot experiments on the 7 degrees of freedom Barrett WAM arm.},
  langid = {english},
  keywords = {Harmonic potential function,Nonlinear dynamical system,Realtime obstacle avoidance,Robot manipulator},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/khansari-zadeh_billard_2012_a_dynamical_system_approach_to_realtime_obstacle_avoidance.pdf}
}

@inproceedings{khansari-zadehImitationLearningGlobally2010,
  title = {Imitation Learning of Globally Stable Non-Linear Point-to-Point Robot Motions Using Nonlinear Programming},
  booktitle = {2010 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Khansari-Zadeh, S. Mohammad and Billard, Aude},
  date = {2010-10},
  pages = {2676--2683},
  issn = {2153-0866},
  doi = {10.1109/IROS.2010.5651259},
  abstract = {This paper presents a methodology for learning arbitrary discrete motions from a set of demonstrations. We model a motion as a nonlinear autonomous (i.e. time-invariant) dynamical system, and define the sufficient conditions to make such a system globally asymptotically stable at the target. The convergence of all trajectories is ensured starting from any point in the operational space. We propose a learning method, called Stable Estimator of Dynamical Systems (SEDS), that estimates parameters of a Gaussian Mixture Model via an optimization problem under non-linear constraints. Being time-invariant and globally stable, the system is able to handle both temporal and spatial perturbations, while performing the motion as close to the demonstrations as possible. The method is evaluated through a set of robotic experiments.},
  eventtitle = {2010 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  keywords = {Asymptotic stability,Numerical stability,Optimization,READ,Robot kinematics,Stability analysis,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/khansari-zadeh_billard_2010_imitation_learning_of_globally_stable_non-linear_point-to-point_robot_motions.pdf}
}

@article{khansari-zadehLearningPlayMinigolf2012,
  title = {Learning to Play Minigolf: {{A}} Dynamical System-Based Approach},
  shorttitle = {Learning to Play Minigolf},
  author = {Khansari-Zadeh, Seyed Mohammad and Kronander, Klas and Billard, Aude},
  date = {2012},
  journaltitle = {Advanced Robotics},
  volume = {26},
  number = {17},
  pages = {1967--1993},
  publisher = {{Taylor \& Francis}},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/khansari-zadeh_et_al_2012_learning_to_play_minigolf.pdf}
}

@article{khansari-zadehLearningPotentialFunctions2017,
  title = {Learning Potential Functions from Human Demonstrations with Encapsulated Dynamic and Compliant Behaviors},
  author = {Khansari-Zadeh, Seyed Mohammad and Khatib, Oussama},
  date = {2017-01},
  journaltitle = {Autonomous Robots},
  shortjournal = {Auton. Robot.},
  volume = {41},
  number = {1},
  pages = {45--69},
  publisher = {{Springer}},
  location = {{Dordrecht}},
  issn = {0929-5593},
  doi = {10.1007/s10514-015-9528-y},
  url = {https://link.springer.com/article/10.1007/s10514-015-9528-y},
  urldate = {2022-05-02},
  abstract = {We consider the problem of devising a unified control policy capable of regulating both the robot motion and its physical interaction with the environment. We formulate this control policy by a non-parametric potential function and a dissipative field, which both can be learned from human demonstrations. We show that the robot motion and its stiffness behaviors can be encapsulated by the potential function's gradient and curvature, respectively. The dissipative field can also be used to model desired damping behavior throughout the motion, hence generating motions that follows the same velocity profile as the demonstrations. The proposed controller can be realized as a unification approach between "realtime motion generation" and "variable impedance control", with the advantages that it has guaranteed stability as well as does not rely on following a reference trajectory. Our approach, called unified motion and variable impedance control (UMIC), is completely time-invariant and can be learned from a few demonstrations via solving two (convex) constrained quadratic optimization problems. We validate UMIC on a library of 30 human handwriting motions and on a set of experiments on 7-DoF KUKA light weight robot.},
  langid = {english},
  keywords = {Compliant control,Imitation   learning,impedance control,IMPORTANT,manipulators,Motion control,Motion primitives,Physical interaction control,Potential field,READ,Robot   learning,robot control,STABILITY,task,time obstacle avoidance,Variable impedance control},
  annotation = {WOS:000392051600004},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/khansari-zadeh_khatib_2017_learning_potential_functions_from_human_demonstrations_with_encapsulated.pdf}
}

@article{khansari-zadehLearningStableNonlinear2011,
  title = {Learning {{Stable Nonlinear Dynamical Systems With Gaussian Mixture Models}}},
  author = {Khansari-Zadeh, S. Mohammad and Billard, Aude},
  date = {2011-10},
  journaltitle = {IEEE Transactions on Robotics},
  volume = {27},
  number = {5},
  pages = {943--957},
  issn = {1941-0468},
  doi = {10.1109/TRO.2011.2159412},
  abstract = {This paper presents a method to learn discrete robot motions from a set of demonstrations. We model a motion as a nonlinear autonomous (i.e., time-invariant) dynamical system (DS) and define sufficient conditions to ensure global asymptotic stability at the target. We propose a learning method, which is called Stable Estimator of Dynamical Systems (SEDS), to learn the parameters of the DS to ensure that all motions closely follow the demonstrations while ultimately reaching and stopping at the target. Time-invariance and global asymptotic stability at the target ensures that the system can respond immediately and appropriately to perturbations that are encountered during the motion. The method is evaluated through a set of robot experiments and on a library of human handwriting motions.},
  eventtitle = {{{IEEE Transactions}} on {{Robotics}}},
  keywords = {Asymptotic stability,Dynamical systems (DS),Dynamics,Gaussian mixture model,imitation learning,IMPORTANT,Numerical stability,point-to-point motions,READ,Robot kinematics,STABILITY,stability analysis,Stability analysis,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/khansari-zadeh_billard_2011_learning_stable_nonlinear_dynamical_systems_with_gaussian_mixture_models.pdf}
}

@article{khansari-zadehModelingRobotDiscrete2014,
  title = {Modeling Robot Discrete Movements with State-Varying Stiffness and Damping: {{A}} Framework for Integrated Motion Generation and Impedance Control},
  shorttitle = {Modeling Robot Discrete Movements with State-Varying Stiffness and Damping},
  author = {Khansari-Zadeh, S. Mohammad and Kronander, Klas and Billard, Aude},
  date = {2014},
  journaltitle = {Proceedings of robotics: Science and systems X (RSS 2014)},
  volume = {10},
  pages = {2014},
  keywords = {IMPORTANT,READ,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/khansari_et_al_2014_modeling_robot_discrete_movements_with_state-varying_stiffness_and_damping2.pdf}
}

@article{khansariAdaptiveHumaninspiredCompliant2016,
  title = {Adaptive Human-Inspired Compliant Contact Primitives to Perform           Surface–Surface Contact under Uncertainty},
  author = {Khansari, Mohammad and Klingbeil, Ellen and Khatib, Oussama},
  date = {2016-11-01},
  journaltitle = {The International Journal of Robotics Research},
  volume = {35},
  number = {13},
  pages = {1651--1675},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {0278-3649},
  doi = {10.1177/0278364916648389},
  url = {https://doi.org/10.1177/0278364916648389},
  urldate = {2022-12-15},
  abstract = {This paper focuses on devising a control policy that is inspired by human strategy to enable robots to perform surface?surface contact between a hand-held object (e.g. a box) and the environment (e.g. table). We assume the object?s shape is partially known and consider uncertainties in both the environment and the object grasp. Our analysis on ten untrained subjects indicates that during this task: (1) the subjects start decreasing the angular velocity before the complete alignment (in contrast to existing approaches), and (2) they do not control the contact force to remain at a fixed value. Our study is also consistent with the hypothesis that the subjects determine an over-estimate of the relative angle between the object in hand and the environment. Based on these observations we propose a novel control policy, called Surface-Surface Contact Primitive (SSCP), which can perform the surface?surface alignment task with only partial information about the hand-held object and the environment. Furthermore, SSCP only requires a rough estimate of the surface normal vector and does not rely on either the estimation of contact type (i.e. point or edge contact) or locations of contact points. We evaluate the performance of the proposed controller on a set of robot experiments using two seven-degree-of-freedom robots, one for imposing uncertainty to the environment, and the other to perform the experiment. We show the applicability of the controller on four objects with different geometries, its generalization to four different surface materials, and its robustness to uncertainty in environment and grasp as well as external perturbations.},
  langid = {english},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/khansari_et_al_2016_adaptive_human-inspired_compliant_contact_primitives_to_perform.pdf}
}

@article{khoramshahiDynamicalSystemApproach2020,
  title = {A Dynamical System Approach for Detection and Reaction to Human Guidance in Physical Human–Robot Interaction},
  author = {Khoramshahi, Mahdi and Billard, Aude},
  date = {2020-11-01},
  journaltitle = {Autonomous Robots},
  shortjournal = {Auton Robot},
  volume = {44},
  number = {8},
  pages = {1411--1429},
  issn = {1573-7527},
  doi = {10.1007/s10514-020-09934-9},
  url = {https://doi.org/10.1007/s10514-020-09934-9},
  urldate = {2022-08-17},
  abstract = {A seamless interaction requires two robotic behaviors: the leader role where the robot rejects the external perturbations and focuses on the autonomous execution of the task, and the follower role where the robot ignores the task and complies with human intentional forces. The goal of this work is to provide (1) a unified robotic architecture to produce these two roles, and (2) a human-guidance detection algorithm to switch across the two roles. In the absence of human-guidance, the robot performs its task autonomously and upon detection of such guidances the robot passively follows the human motions. We employ dynamical systems to generate task-specific motion and admittance control to generate reactive motions toward the human-guidance. This structure enables the robot to reject undesirable perturbations, track the motions precisely, react to human-guidance by providing proper compliant behavior, and re-plan the motion reactively. We provide analytical investigation of our method in terms of tracking and compliant behavior. Finally, we evaluate our method experimentally using a 6-DoF manipulator.},
  langid = {english},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/khoramshahi_billard_2020_a_dynamical_system_approach_for_detection_and_reaction_to_human_guidance_in.pdf}
}

@article{khosraviConvexNonparametricFormulation2021,
  title = {Convex {{Nonparametric Formulation}} for {{Identification}} of {{Gradient Flows}}},
  author = {Khosravi, Mohammad and Smith, Roy S.},
  date = {2021-07},
  journaltitle = {IEEE Control Systems Letters},
  volume = {5},
  number = {3},
  pages = {1097--1102},
  issn = {2475-1456},
  doi = {10.1109/LCSYS.2020.3000176},
  abstract = {We develop a nonparametric identification method for nonlinear gradient-flow dynamics. In these systems, the vector field is the gradient field of a potential energy function. This fundamental fact about the dynamics is a structural prior knowledge and a constraint in the proposed identification method. While the nature of the identification problem is an estimation in the space of functions, we derive an equivalent finite dimensional formulation, which is a convex optimization in the form of a quadratic program. This provides scalability and the opportunity for utilizing recently developed large-scale optimization solvers. The central idea is representing the energy function as a difference of two convex functions and learning these convex functions jointly. Based on necessary and sufficient conditions for function convexity, the identification problem is formulated, and then, the existence, uniqueness and smoothness of the solution is addressed. We also illustrate and evaluate the method numerically with two demonstrative examples.},
  eventtitle = {{{IEEE Control Systems Letters}}},
  keywords = {Convex functions,convex optimization,Dynamics,Estimation,gradient flows,Hidden Markov models,Nonlinear dynamical systems,Nonlinear system identification,Optimization,Potential energy,quadratic program,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/khosravi_smith_2021_convex_nonparametric_formulation_for_identification_of_gradient_flows2.pdf}
}

@article{kimImpedanceLearningRobotic2010,
  title = {Impedance {{Learning}} for {{Robotic Contact Tasks Using Natural Actor-Critic Algorithm}}},
  author = {Kim, Byungchan and Park, Jooyoung and Park, Shinsuk and Kang, Sungchul},
  date = {2010-04},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
  volume = {40},
  number = {2},
  pages = {433--443},
  issn = {1941-0492},
  doi = {10.1109/TSMCB.2009.2026289},
  abstract = {Compared with their robotic counterparts, humans excel at various tasks by using their ability to adaptively modulate arm impedance parameters. This ability allows us to successfully perform contact tasks even in uncertain environments. This paper considers a learning strategy of motor skill for robotic contact tasks based on a human motor control theory and machine learning schemes. Our robot learning method employs impedance control based on the equilibrium point control theory and reinforcement learning to determine the impedance parameters for contact tasks. A recursive least-square filter-based episodic natural actor-critic algorithm is used to find the optimal impedance parameters. The effectiveness of the proposed method was tested through dynamic simulations of various contact tasks. The simulation results demonstrated that the proposed method optimizes the performance of the contact tasks in uncertain conditions of the environment.},
  eventtitle = {{{IEEE Transactions}} on {{Systems}}, {{Man}}, and {{Cybernetics}}, {{Part B}} ({{Cybernetics}})},
  keywords = {Contact task,Control theory,equilibrium point control,Humans,Impedance,Learning systems,Machine learning,Machine learning algorithms,Motor drives,Optimization methods,READ,reinforcement learning,robot manipulation,Robots,Testing},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/kim_et_al_2010_impedance_learning_for_robotic_contact_tasks_using_natural_actor-critic.pdf}
}

@inproceedings{kimLearningGeneralizationDynamic2018,
  title = {Learning and {{Generalization}} of {{Dynamic Movement Primitives}} by {{Hierarchical Deep Reinforcement Learning}} from {{Demonstration}}},
  booktitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Kim, Wonchul and Lee, Chungkeun and Kim, H. Jin},
  date = {2018-10},
  pages = {3117--3123},
  issn = {2153-0866},
  doi = {10.1109/IROS.2018.8594476},
  abstract = {This paper presents an approach to learn and generalize robotic skills from a demonstration using deep reinforcement learning (deep RL). Dynamic Movement Primitives (DMPs) formulate a nonlinear differential equation and produce the observed movement from a demonstration. However, it is hard to generate new behaviors from using DMPs. Thus, we apply DMPs framework into deep RL as an initial setting for learning the robotic skills. First, we build a network to represent this differential equation, and learn and generalize the movements by optimizing the shape of DMPs with respect to the rewards up to the end of each sequence of movement primitives. In order to do this, we consider a deterministic actor-critic algorithm for deep RL and we also apply a hierarchical strategy. This drastically reduces the search space for a robot by decomposing the task, which allows to solve the sparse reward problem from a complex task. In order to integrate DMPs with hierarchical deep RL, the differential equation is considered as temporal abstraction of option. The overall structure is mainly composed of two controllers: meta-controller and sub-controller. The meta-controller learns a policy over intrinsic goals and a sub-controller learns a policy over actions to accomplish the given goals. We demonstrate our approach on a 6 degree-of-freedom (DOF) arm with a I-DOF gripper and evaluate our approach through a pick-and-place task.},
  eventtitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  keywords = {Deep learning,Differential equations,Dynamics,Mathematical model,READ,Reinforcement learning,Robots,Task analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/kim_et_al_2018_learning_and_generalization_of_dynamic_movement_primitives_by_hierarchical_deep.pdf}
}

@inproceedings{kimLearningRobotStiffness2008,
  title = {Learning Robot Stiffness for Contact Tasks Using the Natural Actor-Critic},
  booktitle = {2008 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Kim, Byungchan and Kang, Byungduk and Park, Shinsuk and Kang, Sungchul},
  date = {2008-05},
  pages = {3832--3837},
  issn = {1050-4729},
  doi = {10.1109/ROBOT.2008.4543799},
  abstract = {This paper introduces a novel motor learning strategy for robotic contact task based on a human motor control theory and machine learning schemes. Humans modulate their arm joint impedance parameters during contact tasks, and such aspect suggests a key feature how human successfully executes various contact tasks in variable environments. Our strategy for successful contact tasks is to find appropriate impedance parameters for optimal task execution by Reinforcement Learning (RL). In this study Recursive Least-Square (RLS) filter based episodic Natural Actor-Critic is employed to determine the optimal impedance parameters. Through dynamic simulations of contact tasks, this paper demonstrates the effectiveness of the proposed strategy. The simulation results show that the proposed method successfully optimizes the performance of the contact task and adapts to uncertain conditions of the environment.},
  eventtitle = {2008 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  keywords = {Artificial neural networks,Humans,Impedance,Intelligent robots,Learning systems,Machine learning,Mechanical engineering,Motor drives,READ,Robot sensing systems,Robotics and automation,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/kim_et_al_2008_learning_robot_stiffness_for_contact_tasks_using_the_natural_actor-critic.pdf}
}

@article{kimReinforcementLearningBased2020,
  title = {Reinforcement Learning Based on Movement Primitives for Contact Tasks},
  author = {Kim, Young-Loul and Ahn, Kuk-Hyun and Song, Jae-Bok},
  date = {2020-04-01},
  journaltitle = {Robotics and Computer-Integrated Manufacturing},
  shortjournal = {Robotics and Computer-Integrated Manufacturing},
  volume = {62},
  pages = {101863},
  issn = {0736-5845},
  doi = {10.1016/j.rcim.2019.101863},
  url = {https://www.sciencedirect.com/science/article/pii/S0736584518306197},
  urldate = {2023-04-07},
  abstract = {Recently, robot learning through deep reinforcement learning has incorporated various robot tasks through deep neural networks, without using specific control or recognition algorithms. However, this learning method is difficult to apply to the contact tasks of a robot, due to the exertion of excessive force from the random search process of reinforcement learning. Therefore, when applying reinforcement learning to contact tasks, solving the contact problem using an existing force controller is necessary. A neural-network-based movement primitive (NNMP) that generates a continuous trajectory which can be transmitted to the force controller and learned through a deep deterministic policy gradient (DDPG) algorithm is proposed for this study. In addition, an imitation learning algorithm suitable for NNMP is proposed such that the trajectories similar to the demonstration trajectory are stably generated. The performance of the proposed algorithms was verified using a square peg-in-hole assembly task with a tolerance of 0.1\,mm. The results confirm that the complicated assembly trajectory can be learned stably through NNMP by the proposed imitation learning algorithm, and that the assembly trajectory is improved by learning the proposed NNMP through the DDPG algorithm.},
  langid = {english},
  keywords = {AI-based methods,Deep Learning in robotics and automation,Force control,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/kim_et_al_2020_reinforcement_learning_based_on_movement_primitives_for_contact_tasks.pdf}
}

@unpublished{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  date = {2017-01-29},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1412.6980},
  urldate = {2020-08-12},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  keywords = {#nosource,Computer Science - Machine Learning,optimizer},
  annotation = {00000}
}

@inproceedings{klankRealtimeCADModel2009,
  title = {Real-Time {{CAD}} Model Matching for Mobile Manipulation and Grasping},
  booktitle = {2009 9th {{IEEE-RAS International Conference}} on {{Humanoid Robots}}},
  author = {Klank, U. and Pangercic, D. and Rusu, R. B. and Beetz, M.},
  date = {2009-12},
  pages = {290--296},
  doi = {10.1109/ICHR.2009.5379561},
  abstract = {Humanoid robotic assistants need capable and comprehensive perception systems that enable them to perform complex manipulation and grasping tasks. This requires the identification and recognition of supporting planes and objects in the world, together with their precise 6D poses. In this paper, we propose a 3D perception system architecture that can robustly fit CAD models in cluttered table setting scenes for the purpose of grasping with a mobile manipulator. Our approach uses a powerful combination of two different camera technologies, time-of-flight (TOF) and RGB, to robustly segment the scene and extract object clusters. Using an a-priori database of object models we then perform a CAD matching in 2D camera images. We validate the proposed system in a number of experiments, and compare the system's performance and reliability with similar initiatives.},
  eventtitle = {2009 9th {{IEEE-RAS International Conference}} on {{Humanoid Robots}}},
  keywords = {#nosource,2D camera images,3D perception system architecture,6D poses,CAD,CAD models,Cameras,comprehensive perception systems,feature extraction,grasping tasks,Hardware,humanoid robotic assistants,humanoid robots,Humanoid robots,image colour analysis,Image databases,image matching,image segmentation,Intelligent robots,Layout,machine learning,manipulators,mobile manipulation,object clusters extraction,object grasping,Power system modeling,Real time systems,real-time CAD model matching,robot vision,robotic grasping,Robustness,Shape,supporting planes recognition,time-of-flight,tno internship},
  annotation = {00075}
}

@article{koberImitationReinforcementLearning2010,
  title = {Imitation and {{Reinforcement Learning}}},
  author = {Kober, J. and Peters, J.},
  date = {2010-06},
  journaltitle = {IEEE Robotics Automation Magazine},
  volume = {17},
  number = {2},
  pages = {55--62},
  issn = {1070-9932},
  doi = {10.1109/MRA.2010.936952},
  abstract = {In this article, we present both novel learning algorithms and experiments using the dynamical system MPs. As such, we describe this MP representation in a way that it is straightforward to reproduce. We review an appropriate imitation learning method, i.e., locally weighted regression, and show how this method can be used both for initializing RL tasks as well as for modifying the start-up phase in a rhythmic task. We also show our current best-suited RL algorithm for this framework, i.e., PoWER. We present two complex motor tasks, i.e., ball-in-a-cup and ball paddling, learned on a real, physical Barrett WAM, using the methods presented in this article. Of particular interest is the ball-paddling application, as it requires a combination of both rhythmic and discrete dynamical systems MPs during the start-up phase to achieve a particular task.},
  keywords = {#nosource,Anthropomorphism,ball paddling,ball-in-a-cup,discrete dynamical system,discrete systems,Humans,imitation learning,industrial robots,Intelligent robots,learning (artificial intelligence),Learning systems,Legged locomotion,Manufacturing,motor primitive,Planar motors,regression analysis,reinforcement learning,Robot programming,robotic grasping,robots,Service robots,Stability,tno internship,weighted regression,whole arm manipulator}
}

@inproceedings{koberLearningPerceptualCoupling2008,
  title = {Learning Perceptual Coupling for Motor Primitives},
  booktitle = {2008 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Kober, Jens and Mohler, Betty and Peters, Jan},
  date = {2008-09},
  pages = {834--839},
  issn = {2153-0866},
  doi = {10.1109/IROS.2008.4650953},
  abstract = {Dynamic system-based motor primitives have enabled robots to learn complex tasks ranging from Tennis-swings to locomotion. However, to date there have been only few extensions which have incorporated perceptual coupling to variables of external focus, and, furthermore, these modifications have relied upon handcrafted solutions. Humans learn how to couple their movement primitives with external variables. Clearly, such a solution is needed in robotics. In this paper, we propose an augmented version of the dynamic systems motor primitives which incorporates perceptual coupling to an external variable. The resulting perceptually driven motor primitives include the previous primitives as a special case and can inherit some of their interesting properties. We show that these motor primitives can perform complex tasks such a Ball-in-a-Cup or Kendama task even with large variances in the initial conditions where a skilled human player would be challenged. For doing so, we initialize the motor primitives in the traditional way by imitation learning without perceptual coupling. Subsequently, we improve the motor primitives using a novel reinforcement learning method which is particularly well-suited for motor primitives.},
  eventtitle = {2008 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  keywords = {Acceleration,Approximation algorithms,Couplings,Humans,Learning,READ,Robots,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/kober_et_al_2008_learning_perceptual_coupling_for_motor_primitives.pdf}
}

@inproceedings{koberMovementTemplatesLearning2010,
  title = {Movement Templates for Learning of Hitting and Batting},
  booktitle = {2010 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Kober, Jens and Mülling, Katharina and Krömer, Oliver and Lampert, Christoph H. and Schölkopf, Bernhard and Peters, Jan},
  date = {2010-05},
  pages = {853--858},
  issn = {1050-4729},
  doi = {10.1109/ROBOT.2010.5509672},
  abstract = {Hitting and batting tasks, such as tennis forehands, ping-pong strokes, or baseball batting, depend on predictions where the ball can be intercepted and how it can properly be returned to the opponent. These predictions get more accurate over time, hence the behaviors need to be continuously modified. As a result, movement templates with a learned global shape need to be adapted during the execution so that the racket reaches a target position and velocity that will return the ball over to the other side of the net or court. It requires altering learned movements to hit a varying target with the necessary velocity at a specific instant in time. Such a task cannot be incorporated straightforwardly in most movement representations suitable for learning. For example, the standard formulation of the dynamical system based motor primitives (introduced by Ijspeert et al. [1]) does not satisfy this property despite their flexibility which has allowed learning tasks ranging from locomotion to kendama. In order to fulfill this requirement, we reformulate the Ijspeert framework to incorporate the possibility of specifying a desired hitting point and a desired hitting velocity while maintaining all advantages of the original formulation. We show that the proposed movement template formulation works well in two scenarios, i.e., for hitting a ball on a string with a table tennis racket at a specified velocity and for returning balls launched by a ball gun successfully over the net using forehand movements. All experiments were carried out on a Barrett WAM using a four camera vision system.},
  eventtitle = {2010 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  keywords = {Biological information theory,Cameras,Cybernetics,Delay,Learning,Legged locomotion,Machine vision,READ,Robotics and automation,Shape,USA Councils},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/kober_et_al_2010_movement_templates_for_learning_of_hitting_and_batting.pdf}
}

@book{kocijanModellingControlDynamic2016,
  title = {Modelling and {{Control}} of {{Dynamic Systems Using Gaussian Process Models}}},
  author = {Kocijan, Juš},
  date = {2016},
  series = {Advances in {{Industrial Control}}},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-21021-6},
  url = {http://link.springer.com/10.1007/978-3-319-21021-6},
  urldate = {2019-04-18},
  isbn = {978-3-319-21020-9 978-3-319-21021-6},
  langid = {english},
  keywords = {#nosource,machine learning control,safe-rl}
}

@unpublished{kollerLearningbasedModelPredictive2018,
  title = {Learning-Based {{Model Predictive Control}} for {{Safe Exploration}}},
  author = {Koller, Torsten and Berkenkamp, Felix and Turchetta, Matteo and Krause, Andreas},
  date = {2018-03-22},
  eprint = {1803.08287},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1803.08287},
  urldate = {2019-04-18},
  abstract = {Learning-based methods have been successful in solving complex control tasks without significant prior knowledge about the system. However, these methods typically do not provide any safety guarantees, which prevents their use in safety-critical, real-world applications. In this paper, we present a learning-based model predictive control scheme that can provide provable high-probability safety guarantees. To this end, we exploit regularity assumptions on the dynamics in terms of a Gaussian process prior to construct provably accurate confidence intervals on predicted trajectories. Unlike previous approaches, we do not assume that model uncertainties are independent. Based on these predictions, we guarantee that trajectories satisfy safety constraints. Moreover, we use a terminal set constraint to recursively guarantee the existence of safe control actions at every iteration. In our experiments, we show that the resulting algorithm can be used to safely and efficiently explore and learn about dynamic systems.},
  langid = {english},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Computer Science - Systems and Control,machine learning control,safe-rl}
}

@article{kongAdaptiveFuzzyControl2019,
  title = {Adaptive {{Fuzzy Control}} for {{Coordinated Multiple Robots With Constraint Using Impedance Learning}}},
  author = {Kong, Linghuan and He, Wei and Yang, Chenguang and Li, Zhijun and Sun, Changyin},
  date = {2019-08},
  journaltitle = {IEEE Transactions on Cybernetics},
  volume = {49},
  number = {8},
  pages = {3052--3063},
  issn = {2168-2275},
  doi = {10.1109/TCYB.2018.2838573},
  abstract = {In this paper, we investigate fuzzy neural network (FNN) control using impedance learning for coordinated multiple constrained robots carrying a common object in the presence of the unknown robotic dynamics and the unknown environment with which the robot comes into contact. First, an FNN learning algorithm is developed to identify the unknown plant model. Second, impedance learning is introduced to regulate the control input in order to improve the environment-robot interaction, and the robot can track the desired trajectory generated by impedance learning. Third, in light of the condition requiring the robot to move in a finite space or to move at a limited velocity in a finite space, the algorithm based on the position constraint and the velocity constraint are proposed, respectively. To guarantee the position constraint and the velocity constraint, an integral barrier Lyapunov function is introduced to avoid the violation of the constraint. According to Lyapunov's stability theory, it can be proved that the tracking errors are uniformly bounded ultimately. At last, some simulation examples are carried out to verify the effectiveness of the designed control.},
  eventtitle = {{{IEEE Transactions}} on {{Cybernetics}}},
  keywords = {Adaptive control,Fuzzy control,Fuzzy neural networks,fuzzy systems,Impedance,impedance learning,Lyapunov methods,multiple robots,neural networks,Neural networks,Robot kinematics,time-varying constraint},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/kong_et_al_2019_adaptive_fuzzy_control_for_coordinated_multiple_robots_with_constraint_using.pdf}
}

@article{koopmanHamiltonianSystemsTransformation1931,
  title = {Hamiltonian Systems and Transformation in {{Hilbert}} Space},
  author = {Koopman, Bernard O.},
  date = {1931},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {17},
  number = {5},
  pages = {315--318},
  publisher = {{National Acad Sciences}},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/koopman_1931_hamiltonian_systems_and_transformation_in_hilbert_space.pdf}
}

@inproceedings{kormushevRobotMotorSkill2010,
  title = {Robot Motor Skill Coordination with {{EM-based Reinforcement Learning}}},
  booktitle = {2010 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Kormushev, Petar and Calinon, Sylvain and Caldwell, Darwin G.},
  date = {2010-10},
  pages = {3232--3237},
  issn = {2153-0866},
  doi = {10.1109/IROS.2010.5649089},
  abstract = {We present an approach allowing a robot to acquire new motor skills by learning the couplings across motor control variables. The demonstrated skill is first encoded in a compact form through a modified version of Dynamic Movement Primitives (DMP) which encapsulates correlation information. Expectation-Maximization based Reinforcement Learning is then used to modulate the mixture of dynamical systems initialized from the user's demonstration. The approach is evaluated on a torque-controlled 7 DOFs Barrett WAM robotic arm. Two skill learning experiments are conducted: a reaching task where the robot needs to adapt the learned movement to avoid an obstacle, and a dynamic pancake-flipping task.},
  eventtitle = {2010 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  keywords = {Couplings,Force,Joints,READ,Robot kinematics,Service robots,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/kormushev_et_al_2010_robot_motor_skill_coordination_with_em-based_reinforcement_learning.pdf}
}

@inproceedings{krambergerPassivityBasedIterative2018,
  title = {Passivity {{Based Iterative Learning}} of {{Admittance-Coupled Dynamic Movement Primitives}} for {{Interaction}} with {{Changing Environments}}},
  booktitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Kramberger, Aljaž and Shahriari, Erfan and Gams, Andrej and Nemec, Bojan and Ude, Aleš and Haddadin, Sami},
  date = {2018-10},
  pages = {6023--6028},
  issn = {2153-0866},
  doi = {10.1109/IROS.2018.8593647},
  abstract = {Encoding desired motions into dynamic movement primitives (DMPs) is a common way for generating compact task representations that are able to handle sensor-based goal adaptations. At the same time, a robot should not only express adaptive motion capabilities at planning level, but use also contact wrench feedback in the adaptation and learning process of the DMP. Despite first approaches exist in this direction, no fully integrated approach has been proposed so far. In this paper, we introduce a new class of admittance-coupled DMPs that addresses environmental changes by including contact wrench feedback dynamics into the DMP formalism. Moreover, a novel iterative learning approach is devised that is based on monitoring the overall system passivity analysis in terms of reference power tracking. Simulations and experimental results with the Kuka LWR robot maintaining a non-rigid contact with the environment (wiping a surface) are shown for supporting the validity of our approach.},
  eventtitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  keywords = {Admittance,Dynamics,Force feedback,Impedance,READ,Robots,Task analysis,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/kramberger_et_al_2018_passivity_based_iterative_learning_of_admittance-coupled_dynamic_movement.pdf}
}

@inproceedings{krizhevskyImagenetClassificationDeep2012,
  title = {Imagenet Classification with Deep Convolutional Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  date = {2012},
  pages = {1097--1105},
  keywords = {#nosource,robotic grasping,tno internship},
  annotation = {38239}
}

@article{kroemerReviewRobotLearning2021,
  title = {A Review of Robot Learning for Manipulation: Challenges, Representations, and Algorithms},
  shorttitle = {A Review of Robot Learning for Manipulation},
  author = {Kroemer, Oliver and Niekum, Scott and Konidaris, George},
  date = {2021-01-01},
  journaltitle = {The Journal of Machine Learning Research},
  shortjournal = {J. Mach. Learn. Res.},
  volume = {22},
  number = {1},
  pages = {30:1395--30:1476},
  issn = {1532-4435},
  abstract = {A key challenge in intelligent robotics is creating robots that are capable of directly interacting with the world around them to achieve their goals. The last decade has seen substantial growth in research on the problem of robot manipulation, which aims to exploit the increasing availability of affordable robot arms and grippers to create robots capable of directly interacting with the world to achieve their goals. Learning will be central to such autonomous systems, as the real world contains too much variation for a robot to expect to have an accurate model of its environment, the objects in it, or the skills required to manipulate them, in advance. We aim to survey a representative subset of that research which uses machine learning for manipulation. We describe a formalization of the robot manipulation learning problem that synthesizes existing research into a single coherent framework and highlight the many remaining research opportunities and challenges.},
  keywords = {learning,manipulation,MDPs,review,REVIEW,robots},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/kroemer_et_al_2021_a_review_of_robot_learning_for_manipulation.pdf}
}

@article{kronanderIncrementalMotionLearning2015,
  title = {Incremental Motion Learning with Locally Modulated Dynamical Systems},
  author = {Kronander, K. and Khansari, M. and Billard, A.},
  date = {2015-08-01},
  journaltitle = {Robotics and Autonomous Systems},
  shortjournal = {Robotics and Autonomous Systems},
  volume = {70},
  pages = {52--62},
  issn = {0921-8890},
  doi = {10.1016/j.robot.2015.03.010},
  url = {https://www.sciencedirect.com/science/article/pii/S0921889015000822},
  urldate = {2022-05-11},
  abstract = {Dynamical Systems (DS) for robot motion modeling are a promising approach for efficient robot learning and control. Our focus in this paper is on autonomous dynamical systems, which represent a motion plan without dependency on time. We develop a method that allows to locally reshape an existing, stable nonlinear autonomous DS while preserving important stability properties of the original system. Our system is based on local transformations of the dynamics. We propose an incremental learning algorithm based on Gaussian Processes for learning to reshape dynamical systems using this representation. The approach is validated in a 2d task of learning handwriting motions, a periodic polishing motion and in a manipulation task with the 7 degrees of freedom Barrett WAM manipulator.},
  langid = {english},
  keywords = {Dynamical systems,IMPORTANT,Motion modeling,READ,Robotics,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/kronander_et_al_2015_incremental_motion_learning_with_locally_modulated_dynamical_systems.pdf}
}

@inproceedings{kronanderOnlineLearningVarying2012,
  title = {Online Learning of Varying Stiffness through Physical Human-Robot Interaction},
  booktitle = {2012 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Kronander, Klas and Billard, Aude},
  date = {2012-05},
  pages = {1842--1849},
  issn = {1050-4729},
  doi = {10.1109/ICRA.2012.6224877},
  abstract = {Programming by Demonstration offers an intuitive framework for teaching robots how to perform various tasks without having to preprogram them. It also offers an intuitive way to provide corrections and refine teaching during task execution. Previously, mostly position constraints have been taken into account when teaching tasks from demonstrations. In this work, we tackle the problem of teaching tasks that require or can benefit from varying stiffness. This extension is not trivial, as the teacher needs to have a way of communicating to the robot what stiffness it should use. We propose a method by which the teacher can modulate the stiffness of the robot in any direction through physical interaction. The system is incremental and works online, so that the teacher can instantly feel how the robot learns from the interaction. We validate the proposed approach on two experiments on a 7-Dof Barrett WAM arm.},
  eventtitle = {2012 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  keywords = {Covariance matrix,Education,Humans,Impedance,Kinematics,Robots,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/kronander_billard_2012_online_learning_of_varying_stiffness_through_physical_human-robot_interaction.pdf}
}

@article{kronanderPassiveInteractionControl2016,
  title = {Passive {{Interaction Control With Dynamical Systems}}},
  author = {Kronander, Klas and Billard, Aude},
  date = {2016-01},
  journaltitle = {Ieee Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {1},
  number = {1},
  pages = {106--113},
  publisher = {{Ieee-Inst Electrical Electronics Engineers Inc}},
  location = {{Piscataway}},
  issn = {2377-3766},
  doi = {10.1109/LRA.2015.2509025},
  url = {https://ieeexplore.ieee.org/document/7358081},
  urldate = {2022-05-02},
  abstract = {Autonomous dynamical systems (DS) has emerged as an extremely flexible and powerful method for modeling robotic tasks. Task execution of DS models is typically done in an open-loop manner in combination with standard low level controller, e.g. position controller or impedance controller. Such an arrangement has two important drawbacks: 1) it is not passive and 2) the DS model cannot respond to physical perturbations on the robot body. These can imply severe consequences in motion tasks involving expected or unexpected contact with objects whose position is unknown and dynamic. We propose a novel control architecture that closes the loop around the DS, ensures passivity and allows intuitive tuning of the robot impedance. We evaluate our approach in a comparative study in an uncertain manipulation task with unexpected contact.},
  langid = {english},
  keywords = {Compliance and Impedance Control,IMPORTANT,manipulators,motion,Motion Control of Manipulators,Physical Human-Robot Interaction,READ,STABILITY},
  annotation = {WOS:000413719900015},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/kronander_billard_2016_passive_interaction_control_with_dynamical_systems.pdf}
}

@article{kronanderStabilityConsiderationsVariable2016,
  title = {Stability {{Considerations}} for {{Variable Impedance Control}}},
  author = {Kronander, Klas and Billard, Aude},
  date = {2016-10},
  journaltitle = {IEEE Transactions on Robotics},
  volume = {32},
  number = {5},
  pages = {1298--1305},
  issn = {1941-0468},
  doi = {10.1109/TRO.2016.2593492},
  abstract = {Impedance control is a commonly used control architecture for robotic manipulation. For increased flexibility, the impedance can be programmed to vary during the task. This has important implications on the stability properties of the control system, which are often overlooked in practice. In fact, the standard stability analysis is not valid in the case that the impedance parameters vary over time. Simulations show that, depending on how the impedance parameters are varied, stable or unstable behavior can arise even in regulation without contact. In this paper, we elucidate this issue and propose a state-independent stability constraint that relates the stiffness, and also the time derivative of the stiffness to the damping. Our approach is illustrated and evaluated in comparison with an online stabilization method [8] which uses a tank-based stability criterion.},
  eventtitle = {{{IEEE Transactions}} on {{Robotics}}},
  keywords = {Adaptive control,Asymptotic stability,Cartesian impedance,Damping,Impedance,IMPORTANT,Joint impedance,Manipulator dynamics,READ,robot control,Robots,STABILITY,stability criteria,Stability criteria},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/kronander_billard_2016_stability_considerations_for_variable_impedance_control.pdf}
}

@inproceedings{krugerImitationLearningNonlinear2012,
  title = {Imitation Learning of Non-Linear Point-to-Point Robot Motions Using Dirichlet Processes},
  booktitle = {2012 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Krüger, Volker and Tikhanoff, Vadim and Natale, Lorenzo and Sandini, Giulio},
  date = {2012-05},
  pages = {2029--2034},
  issn = {1050-4729},
  doi = {10.1109/ICRA.2012.6224674},
  abstract = {In this paper we discuss the use of the infinite Gaussian mixture model and Dirichlet processes for learning robot movements from demonstrations. Starting point of this work is an earlier paper where the authors learn a non-linear dynamic robot movement model from a small number of observations. The model in that work is learned using a classical finite Gaussian mixture model (FGMM) where the Gaussian mixtures are appropriately constrained. The problem with this approach is that one needs to make a good guess for how many mixtures the FGMM should use. In this work, we generalize this approach to use an infinite Gaussian mixture model (IGMM) which does not have this limitation. Instead, the IGMM automatically finds the number of mixtures that are necessary to reflect the data complexity. For use in the context of a non-linear dynamic model, we develop a Constrained IGMM (CIGMM). We validate our algorithm on the same data that was used in [5], where the authors use motion capture devices to record the demonstrations. As further validation we test our approach on novel data acquired on our iCub in a different demonstration scenario in which the robot is physically driven by the human demonstrator.},
  eventtitle = {2012 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  keywords = {Complexity theory,Computational modeling,Data models,Optimization,READ,Robots,STABILITY,Training data,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/krüger_et_al_2012_imitation_learning_of_non-linear_point-to-point_robot_motions_using_dirichlet.pdf}
}

@article{kulakActiveLearningBayesian2021,
  title = {Active {{Learning}} of {{Bayesian Probabilistic Movement Primitives}}},
  author = {Kulak, Thibaut and Girgin, Hakan and Odobez, Jean-Marc and Calinon, Sylvain},
  date = {2021-04},
  journaltitle = {Ieee Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {6},
  number = {2},
  pages = {2163--2170},
  publisher = {{Ieee-Inst Electrical Electronics Engineers Inc}},
  location = {{Piscataway}},
  issn = {2377-3766},
  doi = {10.1109/LRA.2021.3060414},
  url = {https://ieeexplore.ieee.org/document/9357895},
  urldate = {2022-12-02},
  abstract = {Learning from Demonstration permits non-expert users to easily and intuitively reprogram robots. Among approaches embracing this paradigm, probabilistic movement primitives (ProMPs) are a well-established and widely used method to learn trajectory distributions. However, providing or requesting useful demonstrations is not easy, as quantifying what constitutes a good demonstration in terms of generalization capabilities is not trivial. In this letter, we propose an active learning method for contextual ProMPs for addressing this problem. More specifically, we learn the trajectory distributions using a Bayesian Gaussian mixture model (BGMM) and then leverage the notion of epistemic uncertainties to iteratively choose new context query points for demonstrations. We show that this approach reduces the required number of human demonstrations. We demonstrate the effectiveness of the approach on a pouring task, both in simulation and on a real 7-DoF Franka Emika robot.},
  langid = {english},
  keywords = {Imitation learning,incremental learning,learning from demonstration,READ},
  annotation = {WOS:000629731200009},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/kulak_et_al_2021_active_learning_of_bayesian_probabilistic_movement_primitives.pdf}
}

@article{kulviciusJoiningMovementSequences2012,
  title = {Joining {{Movement Sequences}}: {{Modified Dynamic Movement Primitives}} for {{Robotics Applications Exemplified}} on {{Handwriting}}},
  shorttitle = {Joining {{Movement Sequences}}},
  author = {Kulvicius, Tomas and Ning, KeJun and Tamosiunaite, Minija and Worgötter, Florentin},
  date = {2012-02},
  journaltitle = {IEEE Transactions on Robotics},
  volume = {28},
  number = {1},
  pages = {145--157},
  issn = {1941-0468},
  doi = {10.1109/TRO.2011.2163863},
  abstract = {The generation of complex movement patterns, in particular, in cases where one needs to smoothly and accurately join trajectories in a dynamic way, is an important problem in robotics. This paper presents a novel joining method that is based on the modification of the original dynamic movement primitive formulation. The new method can reproduce the target trajectory with high accuracy regarding both the position and the velocity profile and produces smooth and natural transitions in position space, as well as in velocity space. The properties of the method are demonstrated by its application to simulated handwriting generation, which are also shown on a robot, where an adaptive algorithm is used to learn trajectories from human demonstration. These results demonstrate that the new method is a feasible alternative for joining of movement sequences, which has a high potential for all robotics applications where trajectory joining is required.},
  eventtitle = {{{IEEE Transactions}} on {{Robotics}}},
  keywords = {Accuracy,Delta learning rule,handwriting generation,Humans,joining of dynamic movement primitives (DMPs),Joints,Kernel,overlapping kernels,READ,Robots,Training,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/kulvicius_et_al_2012_joining_movement_sequences.pdf}
}

@inproceedings{kumraRoboticGraspDetection2017,
  title = {Robotic Grasp Detection Using Deep Convolutional Neural Networks},
  booktitle = {2017 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Kumra, Sulabh and Kanan, Christopher},
  date = {2017},
  pages = {769--776},
  publisher = {{IEEE}},
  keywords = {#nosource,grasp pose detection,neural network,object grasping,robotic grasping,tno internship},
  annotation = {00041}
}

@inproceedings{kurutachModelEnsembleTrustRegionPolicy2018,
  title = {Model-{{Ensemble Trust-Region Policy Optimization}}},
  author = {Kurutach, Thanard and Clavera, Ignasi and Duan, Yan and Tamar, Aviv and Abbeel, Pieter},
  date = {2018-02-15},
  url = {https://openreview.net/forum?id=SJJinbWRZ&noteId=SJJinbWRZ},
  urldate = {2020-07-02},
  abstract = {Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning.  However, they tend to suffer from high sample complexity...},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  keywords = {#nosource,learned model,model-based,reinforcement learning}
}

@article{lachnerEnergyBudgetsCoordinate2021,
  title = {Energy Budgets for Coordinate Invariant Robot Control in Physical Human-Robot Interaction},
  author = {Lachner, Johannes and Allmendinger, Felix and Hobert, Eddo and Hogan, Neville and Stramigioli, Stefano},
  date = {2021-08},
  journaltitle = {International Journal of Robotics Research},
  shortjournal = {Int. J. Robot. Res.},
  volume = {40},
  number = {8-9},
  pages = {968--985},
  publisher = {{Sage Publications Ltd}},
  location = {{London}},
  issn = {0278-3649},
  doi = {10.1177/02783649211011639},
  url = {https://journals.sagepub.com/doi/pdf/10.1177/02783649211011639},
  urldate = {2022-05-02},
  abstract = {In this work we consider the current certification process of applications with physical human-robot interaction (pHRI). Two major hazards are collisions and clamping scenarios. The implementation of safety measures in pHRI applications typically depends strongly on coordinates, e.g., to monitor the robot velocity or to predict external forces. We show that the current certification process does not, in general, guarantee a safe robot behavior. In particular, in unstructured environments it is not possible to predict all risks in advance. We therefore propose to control the energy of the robot, which is a coordinate invariant entity. For an impedance controlled robot, the total energy consists of potential energy and kinetic energy. The energy flow from task description to physical interaction follows a strict causality. We assign a safe energy budget for the robot. With this energy budget, the presented controller auto-tunes its parameters to limit the exchanged kinetic energy during a collision and the potential energy during clamping scenarios. In contact, the robot behaves compliantly and therefore eliminates clamping danger. After contact, the robot automatically continues to follow the desired trajectory. With this approach the number of safety-related parameters to be determined can be reduced to one energy value, which has the potential to significantly speed up the commissioning of pHRI applications. The proposed technique is validated by experiments.},
  langid = {english},
  keywords = {coordinate invariance,energy-aware   control,impedance control,Joint impedance,mass,Physical human-robot interaction,safety,STABILITY,velocity},
  annotation = {WOS:000679748600001},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/lachner_et_al_2021_energy_budgets_for_coordinate_invariant_robot_control_in_physical_human-robot.pdf}
}

@article{laghiUnifyingBilateralTeleoperation2020,
  title = {Unifying Bilateral Teleoperation and Tele-Impedance for Enhanced User Experience},
  author = {Laghi, Marco and Ajoudani, Arash and Catalano, Manuel G. and Bicchi, Antonio},
  date = {2020-03-01},
  journaltitle = {The International Journal of Robotics Research},
  shortjournal = {The International Journal of Robotics Research},
  volume = {39},
  number = {4},
  pages = {514--539},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {0278-3649},
  doi = {10.1177/0278364919891773},
  url = {https://doi.org/10.1177/0278364919891773},
  urldate = {2022-05-17},
  abstract = {Usability is one of the most important aspects of teleoperation. Ideally, the operator’s experience should be one of complete command over the remote environment, but also be as close as possible to what they would have if physically present at the remote end, i.e., transparency in terms of both action and perception. These two aspects may coincide in favorable conditions, where classic approaches such as the four-channel architecture ensures transparency of the control framework. In the presence of substantial delays between the user and the slave, however, the stability–performance trade-off inherent to bilateral teleoperation deteriorates not only transparency, but also command. An alternative, unilateral approach is given by tele-impedance, which controls the slave–environment interaction by measuring and remotely replicating the user’s limb endpoint position and impedance. Not including force feedback to the operator, tele-impedance is absolutely robust to delays, whereas it completely lacks transparency. This article introduces a novel control framework that integrates a new, fully transparent, two-channel bilateral architecture with the tele-impedance paradigm. The result is a unified solution that mitigates problems of classical approaches, and provides the user with additional tools to modulate the slave robot’s physical interaction behavior, resulting in a better operator experience in spite of time inconsistencies. The validity and effectiveness of the proposed solution is demonstrated in terms of performance in the interaction tasks, of user fatigue and overall experience.},
  langid = {english},
  keywords = {bilateral,stability,STABILITY,Teleoperation,transparency,usability},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/laghi_et_al_2020_unifying_bilateral_teleoperation_and_tele-impedance_for_enhanced_user_experience.pdf}
}

@inproceedings{landiAdmittanceControlParameter2017,
  title = {Admittance Control Parameter Adaptation for Physical Human-Robot Interaction},
  booktitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Landi, Chiara Talignani and Ferraguti, Federica and Sabattini, Lorenzo and Secchi, Cristian and Fantuzzi, Cesare},
  date = {2017-05},
  pages = {2911--2916},
  doi = {10.1109/ICRA.2017.7989338},
  abstract = {In physical human-robot interaction, the coexistence of robots and humans in the same workspace requires the guarantee of a stable interaction, trying to minimize the effort for the operator. To this aim, the admittance control is widely used and the appropriate selection of the its parameters is crucial, since they affect both the stability and the ability of the robot to interact with the user. In this paper, we present a strategy for detecting deviations from the nominal behavior of an admittance-controlled robot and for adapting the parameters of the controller while guaranteeing the passivity. The proposed methodology is validated on a KUKA LWR 4+.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  keywords = {Admittance,Damping,Force,Robot sensing systems,Service robots,Stability analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/landi_et_al_2017_admittance_control_parameter_adaptation_for_physical_human-robot_interaction.pdf}
}

@inproceedings{landiVariableAdmittanceControl2017,
  title = {Variable Admittance Control Preventing Undesired Oscillating Behaviors in Physical Human-Robot Interaction},
  booktitle = {2017 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Landi, Chiara Talignani and Ferraguti, Federica and Sabattini, Lorenzo and Secchi, Cristian and Bonfè, Marcello and Fantuzzi, Cesare},
  date = {2017-09},
  pages = {3611--3616},
  issn = {2153-0866},
  doi = {10.1109/IROS.2017.8206207},
  abstract = {Admittance control is a widely used approach for guaranteeing a compliant behavior of the robot in physical human-robot interaction. When an admittance-controlled robot is coupled with a human, the dynamics of the human can cause deviations from the desired behavior of the robot, mainly due to a stiffening of the human arm, and thus generate high-frequency unsafe oscillations of the robot. In this paper we present a novel methodology for detecting the rising oscillations in the human-robot interaction. Furthermore, we propose a passivity-preserving strategy to adapt the parameter of the admittance control in order to get rid of the high-frequency oscillations and, when possible, to restore the desired interaction model. A thorough experimental validation of the proposed strategy is performed on a group of 26 users performing a cooperative task.},
  eventtitle = {2017 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  keywords = {Acceleration,Adaptation models,Admittance,Damping,Human-robot interaction,Oscillators,Robots},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/landi_et_al_2017_variable_admittance_control_preventing_undesired_oscillating_behaviors_in.pdf}
}

@inproceedings{langeAutonomousReinforcementLearning2012,
  title = {Autonomous Reinforcement Learning on Raw Visual Input Data in a Real World Application},
  booktitle = {The 2012 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Lange, Sascha and Riedmiller, Martin and Voigtländer, Arne},
  date = {2012-06},
  pages = {1--8},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2012.6252823},
  abstract = {We propose a learning architecture, that is able to do reinforcement learning based on raw visual input data. In contrast to previous approaches, not only the control policy is learned. In order to be successful, the system must also autonomously learn, how to extract relevant information out of a high-dimensional stream of input information, for which the semantics are not provided to the learning system. We give a first proof-of-concept of this novel learning architecture on a challenging benchmark, namely visual control of a racing slot car. The resulting policy, learned only by success or failure, is hardly beaten by an experienced human player.},
  eventtitle = {The 2012 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  keywords = {#nosource,Australia,Indexes}
}

@inproceedings{laskeyRobotGraspingClutter2016,
  title = {Robot Grasping in Clutter: {{Using}} a Hierarchy of Supervisors for Learning from Demonstrations},
  shorttitle = {Robot Grasping in Clutter},
  booktitle = {2016 {{IEEE International Conference}} on {{Automation Science}} and {{Engineering}} ({{CASE}})},
  author = {Laskey, M. and Lee, J. and Chuck, C. and Gealy, D. and Hsieh, W. and Pokorny, F. T. and Dragan, A. D. and Goldberg, K.},
  date = {2016-08},
  pages = {827--834},
  doi = {10.1109/COASE.2016.7743488},
  abstract = {For applications such as Amazon warehouse order fulfillment, robots must grasp a desired object amid clutter: other objects that block direct access. This can be difficult to program explicitly due to uncertainty in friction and push mechanics and the variety of objects that can be encountered. Deep Learning networks combined with Online Learning from Demonstration (LfD) algorithms such as DAgger and SHIV have potential to learn robot control policies for such tasks where the input is a camera image and system dynamics and the cost function are unknown. To explore this idea, we introduce a version of the grasping in clutter problem where a yellow cylinder must be grasped by a planar robot arm amid extruded objects in a variety of shapes and positions. To reduce the burden on human experts to provide demonstrations, we propose using a hierarchy of three levels of supervisors: a fast motion planner that ignores obstacles, crowd-sourced human workers who provide appropriate robot control values remotely via online videos, and a local human expert. Physical experiments suggest that with 160 expert demonstrations, using the hierarchy of supervisors can increase the probability of a successful grasp (reliability) from 55\% to 90\%.},
  eventtitle = {2016 {{IEEE International Conference}} on {{Automation Science}} and {{Engineering}} ({{CASE}})},
  keywords = {#nosource,camera image,clutter,collision avoidance,deep learning networks,fast motion planner,Grasping,grippers,learning by example,manipulators,Manipulators,object grasping,obstacles,online learning from demonstration,planar robot arm,Planning,robot control policies,robotic grasping,Service robots,supervised learning,tno internship,Trajectory}
}

@article{lasotaSurveyMethodsSafe2017,
  title = {A {{Survey}} of {{Methods}} for {{Safe Human-Robot Interaction}}},
  author = {Lasota, Przemyslaw A. and Fong, Terrence and Shah, Julie A.},
  date = {2017},
  journaltitle = {Foundations and Trends in Robotics},
  shortjournal = {FNT in Robotics},
  volume = {5},
  number = {3},
  pages = {261--349},
  issn = {1935-8253, 1935-8261},
  doi = {10.1561/2300000052},
  url = {http://www.nowpublishers.com/article/Details/ROB-052},
  urldate = {2022-05-24},
  langid = {english},
  keywords = {REVIEW},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/lasota_et_al_2017_a_survey_of_methods_for_safe_human-robot_interaction.pdf}
}

@inproceedings{lawrenceAlmostSurelyStable2020,
  title = {Almost {{Surely Stable Deep Dynamics}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lawrence, Nathan and Loewen, Philip and Forbes, Michael and Backstrom, Johan and Gopaluni, Bhushan},
  date = {2020},
  volume = {33},
  pages = {18942--18953},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/daecf755df5b1d637033bb29b319c39a-Abstract.html},
  urldate = {2022-10-28},
  abstract = {We introduce a method for learning provably stable deep neural network based dynamic models from observed data. Specifically, we consider discrete-time stochastic dynamic models, as they are of particular interest in practical applications such as estimation and control. However, these aspects exacerbate the challenge of guaranteeing stability. Our method works by embedding a Lyapunov neural network into the dynamic model, thereby inherently satisfying the stability criterion. To this end, we propose two approaches and apply them in both the deterministic and stochastic settings: one exploits convexity of the Lyapunov function, while the other enforces stability through an implicit output layer. We demonstrate the utility of each approach through numerical examples.},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/lawrence_et_al_2020_almost_surely_stable_deep_dynamics.pdf}
}

@inproceedings{ledererGaussianProcessBasedRealTime2021,
  title = {Gaussian {{Process-Based Real-Time Learning}} for {{Safety Critical Applications}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}, {{Vol}} 139},
  author = {Lederer, Armin and Ordonez Conejo, Alejandro Jose and Maier, Korbinian and Xiao, Wenxin and Umlauft, Jonas and Hirche, Sandra},
  editor = {Meila, M. and Zhang, T.},
  date = {2021},
  volume = {139},
  publisher = {{Jmlr-Journal Machine Learning Research}},
  location = {{San Diego}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/lederer21a.html},
  urldate = {2022-12-07},
  abstract = {The safe operation of physical systems typically relies on high-quality models. Since a continuous stream of data is generated during run-time, such models are often obtained through the application of Gaussian process regression because it provides guarantees on the prediction error. Due to its high computational complexity, Gaussian process regression must be used offline on batches of data, which prevents applications, where a fast adaptation through online learning is necessary to ensure safety. In order to overcome this issue, we propose the LoG-GP. It achieves a logarithmic update and prediction complexity in the number of training points through the aggregation of locally active Gaussian process models. Under weak assumptions on the aggregation scheme, it inherits safety guarantees from exact Gaussian process regression. These theoretical advantages are exemplarily exploited in the design of a safe and data-efficient, online-learning control policy. The efficiency and performance of the proposed real-time learning approach is demonstrated in a comparison to state-of-the-art methods.},
  langid = {english},
  keywords = {bounds,READ},
  annotation = {WOS:000683104606008},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/lederer_et_al_2021_gaussian_process-based_real-time_learning_for_safety_critical_applications.pdf}
}

@article{leeForceTrackingImpedance2008,
  title = {Force {{Tracking Impedance Control}} with {{Variable Target Stiffness}}},
  author = {Lee, K. and Buss, M.},
  date = {2008-01-01},
  journaltitle = {IFAC Proceedings Volumes},
  shortjournal = {IFAC Proceedings Volumes},
  series = {17th {{IFAC World Congress}}},
  volume = {41},
  number = {2},
  pages = {6751--6756},
  issn = {1474-6670},
  doi = {10.3182/20080706-5-KR-1001.01144},
  url = {https://www.sciencedirect.com/science/article/pii/S1474667016400285},
  urldate = {2022-05-22},
  abstract = {In this paper, a novel force tracking impedance control strategy is presented in which target stiffness is varied on-line to regulate the desired contact force without any knowledge of the environment. Humans can control contact force by adjusting their arm stiffness. The contact force can be either increased by making one's arm stiffer or decreased by reducing the arm stiffness. Furthermore, humans can keep the force tracking error within a certain range without any knowledge of environmental parameters as long as how much force they exert on the object is known to them. Analogously, the proposed control scheme achieves a contact force regulation control by adjusting the target stiffness of the impedance control. The new force tracking impedance control scheme does not require estimating environment stiffness or locations since the controller is adapted only based on the previous force tracking error between the desired and real contact force. Stability of the proposed scheme is discussed with a quadratic Lyapunov function. Extensive simulation studies with a 7 degree of freedom (DoF) robot manipulator using full arm dynamics are conducted to demonstrate the validity of the proposed scheme.},
  langid = {english},
  keywords = {READ,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/lee_buss_2008_force_tracking_impedance_control_with_variable_target_stiffness.pdf}
}

@book{leeIntroductionRiemannianManifolds2018,
  title = {Introduction to {{Riemannian Manifolds}}},
  author = {Lee, John M.},
  date = {2018},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  volume = {176},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-91755-9},
  url = {http://link.springer.com/10.1007/978-3-319-91755-9},
  urldate = {2022-10-31},
  isbn = {978-3-319-91754-2 978-3-319-91755-9},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/lee_2018_introduction_to_riemannian_manifolds.pdf}
}

@book{leeIntroductionSmoothManifolds2012,
  title = {Introduction to {{Smooth Manifolds}}},
  author = {Lee, John M.},
  date = {2012},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  volume = {218},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4419-9982-5},
  url = {http://link.springer.com/10.1007/978-1-4419-9982-5},
  urldate = {2022-10-31},
  isbn = {978-1-4419-9981-8 978-1-4419-9982-5},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/lee_2012_introduction_to_smooth_manifolds.pdf}
}

@book{leeIntroductionTopologicalManifolds2011,
  title = {Introduction to {{Topological Manifolds}}},
  author = {Lee, John M.},
  date = {2011},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  volume = {202},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4419-7940-7},
  url = {http://link.springer.com/10.1007/978-1-4419-7940-7},
  urldate = {2022-10-31},
  isbn = {978-1-4419-7939-1 978-1-4419-7940-7},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/lee_2011_introduction_to_topological_manifolds.pdf}
}

@article{leePassiveSetPositionModulationFrameworkInteractive2010,
  title = {Passive-{{Set-Position-Modulation Framework}} for {{Interactive Robotic Systems}}},
  author = {Lee, Dongjun and Huang, Ke},
  date = {2010-04},
  journaltitle = {IEEE Transactions on Robotics},
  volume = {26},
  number = {2},
  pages = {354--369},
  issn = {1941-0468},
  doi = {10.1109/TRO.2010.2041877},
  abstract = {In this paper, we propose a novel framework, passive set-position modulation (PSPM), which enables us to connect a (continuous-time) robot's position to a sequence of slowly updating/sparse (discrete-time) set-position signal via the simple (yet frequently used in practice) spring coupling with damping injection, while enforcing passivity of the closed-loop robotic system. The PSPM modulates the original set-position signal in such a way that the modulated signal is as close to the original signal as possible (i.e., maximum information recovery for better performance), yet only to the extent permissible by the available energy in the system (i.e., passivity constraints). We present its algorithm and theoretically show its passivity and performance. We also show how this PSPM can be applied for two applications, with some experimental results: Internet teleoperation with varying delay and packet loss; and haptics with slow and variable-rate data update.},
  eventtitle = {{{IEEE Transactions}} on {{Robotics}}},
  keywords = {Damping,Delay,Haptic interfaces,Humans,Hybrid system,Intelligent robots,Internet,Internet teleoperation,IP networks,passivity,position feedback,Servomechanisms,Springs,STABILITY,variable-rate haptics,Wireless LAN},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/lee_huang_2010_passive-set-position-modulation_framework_for_interactive_robotic_systems.pdf}
}

@article{lemmeNeuralLearningVector2014,
  title = {Neural Learning of Vector Fields for Encoding Stable Dynamical Systems},
  author = {Lemme, Andre and Neumann, Klaus and Reinhart, René Felix and Steil, Jochen J.},
  date = {2014},
  journaltitle = {Neurocomputing},
  volume = {141},
  pages = {3--14},
  publisher = {{Elsevier}},
  keywords = {Dynamical systems learning,Extreme learning machine,Motion generation,Movement generation,Neural network,READ,Vector fields},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/lemme_et_al_2014_neural_learning_of_vector_fields_for_encoding_stable_dynamical_systems.pdf}
}

@inproceedings{lemmeNeurallyImprintedStable2013,
  title = {Neurally Imprinted Stable Vector Fields.},
  booktitle = {{{ESANN}}},
  author = {Lemme, Andre and Neumann, Klaus and Reinhart, René Felix and Steil, Jochen J.},
  date = {2013},
  publisher = {{Citeseer}},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/lemme_et_al_2013_neurally_imprinted_stable_vector_fields.pdf}
}

@article{lenzDeepLearningDetecting2015,
  title = {Deep Learning for Detecting Robotic Grasps},
  author = {Lenz, Ian and Lee, Honglak and Saxena, Ashutosh},
  date = {2015},
  journaltitle = {The International Journal of Robotics Research},
  volume = {34},
  number = {4-5},
  pages = {705--724},
  keywords = {#nosource,deep learning,neural network,robotic grasping,tno internship}
}

@article{levineEndtoendTrainingDeep2016,
  title = {End-to-End {{Training}} of {{Deep Visuomotor Policies}}},
  author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
  date = {2016-01},
  journaltitle = {J. Mach. Learn. Res.},
  volume = {17},
  number = {1},
  pages = {1334--1373},
  issn = {1532-4435},
  url = {http://dl.acm.org/citation.cfm?id=2946645.2946684},
  urldate = {2019-05-08},
  abstract = {Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.},
  keywords = {#nosource,neural networks,object grasping,optimal control,reinforcement learning,robotic grasping,tno internship,vision},
  annotation = {01075}
}

@unpublished{levineLearningHandEyeCoordination2016,
  title = {Learning {{Hand-Eye Coordination}} for {{Robotic Grasping}} with {{Deep Learning}} and {{Large-Scale Data Collection}}},
  author = {Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Quillen, Deirdre},
  date = {2016-08-28},
  eprint = {1603.02199},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1603.02199},
  urldate = {2019-11-27},
  abstract = {We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images and independently of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. To train our network, we collected over 800,000 grasp attempts over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and hardware. Our experimental evaluation demonstrates that our method achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,object grasping,reinforcement learning,robotic grasping,tno internship}
}

@article{liActorcriticLearningFramework2023,
  title = {An Actor-Critic Learning Framework Based on {{Lyapunov}} Stability for Automatic Assembly},
  author = {Li, Xinwang and Xiao, Juliang and Cheng, Yu and Liu, Haitao},
  date = {2023-02-01},
  journaltitle = {Applied Intelligence},
  shortjournal = {Appl Intell},
  volume = {53},
  number = {4},
  pages = {4801--4812},
  issn = {1573-7497},
  doi = {10.1007/s10489-022-03844-2},
  url = {https://doi.org/10.1007/s10489-022-03844-2},
  urldate = {2023-05-04},
  abstract = {With the continuous improvement of the reinforcement learning (RL) algorithm, the algorithm has achieved excellent performance in an increasing number of automatic control tasks. However, there are still some challenges when applying the algorithm to realistic automatic assembly. The most significant challenge is that the stability of these model-free RL methods cannot be effectively guaranteed. Stability is the most critical characteristic of a control system, and stability is closely related to reliability and safety. To ensure the stability of the system, we reconstruct the RL algorithm based on the Lyapunov stability theory of the stochastic system proposed in this paper. An actor-critic learning framework based on Lyapunov stability (LSAC) is proposed for automatic assembly. In addition, this paper proposes a median Q-value theory to alleviate the Q-value estimation deviation that restricts the performance of the RL algorithm. To allow RL agents to better complete the automatic assembly task, this paper designs an adaptive impedance control algorithm. This impedance algorithm executes the actions output by the LSAC framework. Finally, a realistic experiment on automatic assembly is carried out to verify the robustness and superiority of the proposed strategy.},
  langid = {english},
  keywords = {Automatic assembly,Lyapunov stability,Median Q-value,Reinforcement learning,Reward reshaping},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/li_et_al_2023_an_actor-critic_learning_framework_based_on_lyapunov_stability_for_automatic.pdf}
}

@article{liAdaptiveImpedanceControl2016,
  title = {Adaptive Impedance Control for an Upper Limb Robotic Exoskeleton Using Biological Signals},
  author = {Li, Zhijun and Huang, Zhicong and He, Wei and Su, Chun-Yi},
  date = {2016},
  journaltitle = {IEEE Transactions on Industrial Electronics},
  volume = {64},
  number = {2},
  pages = {1664--1674},
  publisher = {{IEEE}},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/li_et_al_2016_adaptive_impedance_control_for_an_upper_limb_robotic_exoskeleton_using.pdf}
}

@article{liangAdaptiveTimeVaryingImpedance2022,
  title = {An {{Adaptive Time-Varying Impedance Controller}} for {{Manipulators}}},
  author = {Liang, Xu and Su, Tingting and Zhang, Zhonghai and Zhang, Jie and Liu, Shengda and Zhao, Quanliang and Yuan, Junjie and Huang, Can and Zhao, Lei and He, Guangping},
  date = {2022-03-18},
  journaltitle = {Frontiers in Neurorobotics},
  shortjournal = {Front Neurorobot},
  volume = {16},
  eprint = {35370593},
  eprinttype = {pmid},
  pages = {789842},
  issn = {1662-5218},
  doi = {10.3389/fnbot.2022.789842},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8971993/},
  urldate = {2022-04-28},
  abstract = {Aiming at the situation that the structural parameters of the general manipulators are uncertain, a time-varying impedance controller based on model reference adaptive control (MRAC) is proposed in this article. The proposed controller does not need to use acceleration-based feedback or to measure external loads and can tolerate considerable structure parameter errors. The global uniform asymptotic stability of the time-varying closed-loop system is analyzed, and a selection approach for control parameters is presented. It is demonstrated that, by using the proposed control parameter selection approach, the closed-loop system under the adaptive controller is equivalent to an existing result. The feasibility of the presented controller for the general manipulators is demonstrated by some numerical simulations.},
  pmcid = {PMC8971993},
  keywords = {IMPORTANT,READ,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/liang_et_al_2022_an_adaptive_time-varying_impedance_controller_for_manipulators.pdf}
}

@article{liangNovelImpedanceControl2021,
  title = {A Novel Impedance Control Method of Rubber Unstacking Robot Dealing with Unpredictable and Time-Variable Adhesion Force},
  author = {Liang, Le and Chen, Yanyan and Liao, Liangchuang and Sun, Hongwei and Liu, Yanjie},
  date = {2021-02},
  journaltitle = {Robotics and Computer-Integrated Manufacturing},
  shortjournal = {Robotics and Computer-Integrated Manufacturing},
  volume = {67},
  pages = {102038},
  issn = {07365845},
  doi = {10.1016/j.rcim.2020.102038},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0736584520302490},
  urldate = {2022-05-03},
  abstract = {Unpredictable and time-variable adhesion force between the rubber unstacking robot and the rubber block is generated, which makes it difficult for the robot to smoothly complete the rubber disassembly task, thereby bringing about new robot control problems. For solving the above problems, a novel method of inner/outer loop impedance control based on natural gradient actor-critic (NAC) reinforcement learning is proposed in this paper. The required impedance is applied by the inner/outer loop impedance control with time delay estimation, which can correct the modeling error and compensate the nonlinear dynamics term to improve the computational efficiency of the system. In addition, the NAC reinforcement learning algorithm based on recursive least squares filtering is used to optimize the impedance parameters online, which can improve the impedance accuracy and robustness in the unstructured dynamic environment. At the same time, three stability constraints of the control strategy are derived in the analysis process. Finally, by setting up the experimental platform, it is verified that the control strategy can make the robot work smoothly under the action of unpredictable and time-variable adhesion force to reduce vibration and improve rubber unstacking performance.},
  langid = {english},
  keywords = {STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/liang_et_al_2021_a_novel_impedance_control_method_of_rubber_unstacking_robot_dealing_with.pdf}
}

@unpublished{liangRLlibAbstractionsDistributed2018,
  title = {{{RLlib}}: {{Abstractions}} for {{Distributed Reinforcement Learning}}},
  shorttitle = {{{RLlib}}},
  author = {Liang, Eric and Liaw, Richard and Moritz, Philipp and Nishihara, Robert and Fox, Roy and Goldberg, Ken and Gonzalez, Joseph E. and Jordan, Michael I. and Stoica, Ion},
  date = {2018-06-28},
  eprint = {1712.09381},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1712.09381},
  urldate = {2020-07-02},
  abstract = {Reinforcement learning (RL) algorithms involve the deep nesting of highly irregular computation patterns, each of which typically exhibits opportunities for distributed computation. We argue for distributing RL components in a composable way by adapting algorithms for top-down hierarchical control, thereby encapsulating parallelism and resource requirements within short-running compute tasks. We demonstrate the benefits of this principle through RLlib: a library that provides scalable software primitives for RL. These primitives enable a broad range of algorithms to be implemented with high performance, scalability, and substantial code reuse. RLlib is available at https://rllib.io/.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,reinforcement learning,scaling RL}
}

@article{liaoDynamicSkillLearning2022,
  title = {Dynamic {{Skill Learning From Human Demonstration Based}} on the {{Human Arm Stiffness Estimation Model}} and {{Riemannian DMP}}},
  author = {Liao, Zhiwei and Jiang, Gedong and Zhao, Fei and Wu, Yuqiang and Yue, Yang and Mei, Xuesong},
  date = {2022},
  journaltitle = {IEEE/ASME Transactions on Mechatronics},
  pages = {1--12},
  issn = {1941-014X},
  doi = {10.1109/TMECH.2022.3212707},
  abstract = {Traditional skill transfer frameworks mainly focus on kinematics, while there has been a lack of studies on dynamics. This article develops a Riemannian-based dynamic movement primitive (DMP) framework for learning and generalizing multispace data, including position, orientation, and stiffness from human demonstration. A simplified geometric configuration of the human arm skeleton is adopted to extract its endpoint stiffness. The dynamic skills of a variable stiffness are obtained in real time and then transferred to robots. The effectiveness of the presented approach is verified by two experiments in real scenarios on the Franka Emika Panda robot. The experimental results indicate that both kinematic and dynamic skills can be learned and generalized by the extended DMP framework with high accuracy and strong correlation. The human-like variable impedance control of a robot can be successfully realized by using the proposed approach. Thus, the proposed approach, including the stiffness estimation model and the skill learning and generalization framework, is suitable for applications of human–robot collaboration, contact operation, and teleoperation, where the position, orientation, and stiffness need to be considered simultaneously.},
  eventtitle = {{{IEEE}}/{{ASME Transactions}} on {{Mechatronics}}},
  keywords = {Dynamic movement primitive (DMP),Ellipsoids,Feature extraction,human-like variable impedance (HVI),Impedance,Manifolds,multispace skills,Quaternions,READ,Riemannian manifold,Robots,skill transfer,Task analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/liao_et_al_2022_dynamic_skill_learning_from_human_demonstration_based_on_the_human_arm.pdf}
}

@article{liaoExtendedDMPsFramework2022,
  title = {Extended {{DMPs Framework}} for {{Position}} and {{Decoupled Quaternion Learning}} and {{Generalization}}},
  author = {Liao, Zhiwei and Zhao, Fei and Jiang, Gedong and Mei, Xuesong},
  date = {2022-07-16},
  journaltitle = {Chinese Journal of Mechanical Engineering},
  shortjournal = {Chinese Journal of Mechanical Engineering},
  volume = {35},
  number = {1},
  pages = {95},
  issn = {2192-8258},
  doi = {10.1186/s10033-022-00761-w},
  url = {https://doi.org/10.1186/s10033-022-00761-w},
  urldate = {2022-10-30},
  abstract = {Dynamic movement primitives (DMPs) as a robust and efficient framework has been studied widely for robot learning from demonstration. Classical DMPs framework mainly focuses on the movement learning in Cartesian or joint space, and can't properly represent end-effector orientation. In this paper, we present an extended DMPs framework (EDMPs) both in Cartesian space and 2-Dimensional (2D) sphere manifold for Quaternion-based orientation learning and generalization. Gaussian mixture model and Gaussian mixture regression (GMM-GMR) are adopted as the initialization phase of EDMPs to handle multi-demonstrations and obtain their mean and covariance. Additionally, some evaluation indicators including reachability and similarity are defined to characterize the learning and generalization abilities of EDMPs. Finally, a real-world experiment was conducted with human demonstrations, the endpoint poses of human arm were recorded and successfully transferred from human to the robot. The experimental results show that the absolute errors of the Cartesian and Riemannian space skills are less than 3.5 mm and 1.0°, respectively. The Pearson’s correlation coefficients of the Cartesian and Riemannian space skills are mostly greater than 0.9. The developed EDMPs exhibits superior reachability and similarity for the multi-space skills’ learning and generalization. This research proposes a fused framework with EDMPs and GMM-GMR which has sufficient capability to handle the multi-space skills in multi-demonstrations.},
  keywords = {2D sphere manifold,Dynamic movement primitives,Gaussian mixture model,Gaussian mixture regression,Learning from demonstration,Quaternion-based orientation,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/liao_et_al_2022_extended_dmps_framework_for_position_and_decoupled_quaternion_learning_and.pdf}
}

@online{liawTuneResearchPlatform2018,
  title = {Tune: {{A Research Platform}} for {{Distributed Model Selection}} and {{Training}}},
  shorttitle = {Tune},
  author = {Liaw, Richard and Liang, Eric and Nishihara, Robert and Moritz, Philipp and Gonzalez, Joseph E. and Stoica, Ion},
  date = {2018-07-13},
  eprint = {1807.05118},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1807.05118},
  url = {http://arxiv.org/abs/1807.05118},
  urldate = {2024-01-09},
  abstract = {Modern machine learning algorithms are increasingly computationally demanding, requiring specialized hardware and distributed computation to achieve high performance in a reasonable time frame. Many hyperparameter search algorithms have been proposed for improving the efficiency of model selection, however their adaptation to the distributed compute environment is often ad-hoc. We propose Tune, a unified framework for model selection and training that provides a narrow-waist interface between training scripts and search algorithms. We show that this interface meets the requirements for a broad range of hyperparameter search algorithms, allows straightforward scaling of search to large clusters, and simplifies algorithm implementation. We demonstrate the implementation of several state-of-the-art hyperparameter search algorithms in Tune. Tune is available at http://ray.readthedocs.io/en/latest/tune.html.},
  pubstate = {preprint},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/Liaw et al_2018_Tune.pdf}
}

@article{liEfficientForceControl2018,
  title = {Efficient {{Force Control Learning System}} for {{Industrial Robots Based}} on {{Variable Impedance Control}}},
  author = {Li, Chao and Zhang, Zhi and Xia, Guihua and Xie, Xinru and Zhu, Qidan},
  date = {2018-08},
  journaltitle = {Sensors},
  volume = {18},
  number = {8},
  pages = {2539},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1424-8220},
  doi = {10.3390/s18082539},
  url = {https://www.mdpi.com/1424-8220/18/8/2539},
  urldate = {2022-04-22},
  abstract = {Learning variable impedance control is a powerful method to improve the performance of force control. However, current methods typically require too many interactions to achieve good performance. Data-inefficiency has limited these methods to learn force-sensitive tasks in real systems. In order to improve the sampling efficiency and decrease the required interactions during the learning process, this paper develops a data-efficient learning variable impedance control method that enables the industrial robots automatically learn to control the contact force in the unstructured environment. To this end, a Gaussian process model is learned as a faithful proxy of the system, which is then used to predict long-term state evolution for internal simulation, allowing for efficient strategy updates. The effects of model bias are reduced effectively by incorporating model uncertainty into long-term planning. Then the impedance profiles are regulated online according to the learned humanlike impedance strategy. In this way, the flexibility and adaptivity of the system could be enhanced. Both simulated and experimental tests have been performed on an industrial manipulator to verify the performance of the proposed method.},
  issue = {8},
  langid = {english},
  keywords = {efficient learning,force control,Gaussian processes,industrial robot,READ,variable impedance control},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/li_et_al_2018_efficient_force_control_learning_system_for_industrial_robots_based_on_variable.pdf}
}

@article{liEfficientLearningVariable2019,
  title = {Efficient Learning Variable Impedance Control for Industrial Robots},
  author = {Li, C. and Zhang, Z. and Xia, G. and Xie, X. and Zhu, Q.},
  date = {2019},
  journaltitle = {Bulletin of the Polish Academy of Sciences. Technical Sciences},
  volume = {67},
  number = {2},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/li_et_al_2019_efficient_learning_variable_impedance_control_for_industrial_robots.pdf}
}

@article{liForceImpedanceTrajectory2018,
  title = {Force, {{Impedance}}, and {{Trajectory Learning}} for {{Contact Tooling}} and {{Haptic Identification}}},
  author = {Li, Yanan and Ganesh, Gowrishankar and Jarrassé, Nathanaël and Haddadin, Sami and Albu-Schaeffer, Alin and Burdet, Etienne},
  date = {2018-10},
  journaltitle = {IEEE Transactions on Robotics},
  volume = {34},
  number = {5},
  pages = {1170--1182},
  issn = {1941-0468},
  doi = {10.1109/TRO.2018.2830405},
  abstract = {Humans can skilfully use tools and interact with the environment by adapting their movement trajectory, contact force, and impedance. Motivated by the human versatility, we develop here a robot controller that concurrently adapts feedforward force, impedance, and reference trajectory when interacting with an unknown environment. In particular, the robot's reference trajectory is adapted to limit the interaction force and maintain it at a desired level, while feedforward force and impedance adaptation compensates for the interaction with the environment. An analysis of the interaction dynamics using Lyapunov theory yields the conditions for convergence of the closed-loop interaction mediated by this controller. Simulations exhibit adaptive properties similar to human motor adaptation. The implementation of this controller for typical interaction tasks including drilling, cutting, and haptic exploration shows that this controller can outperform conventional controllers in contact tooling.},
  eventtitle = {{{IEEE Transactions}} on {{Robotics}}},
  keywords = {Adaptation models,Adaptive control,biological systems control,contact tasks,Dynamics,Force,force control,Impedance,iterative learning control,READ,robot control,Robots,Surface impedance,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/li_et_al_2018_force,_impedance,_and_trajectory_learning_for_contact_tooling_and_haptic.pdf}
}

@article{liHumanRobotCollaboration2013,
  title = {Human–Robot Collaboration Based on Motion Intention Estimation},
  author = {Li, Yanan and Ge, Shuzhi Sam},
  date = {2013},
  journaltitle = {IEEE/ASME Transactions on Mechatronics},
  volume = {19},
  number = {3},
  pages = {1007--1014},
  publisher = {{IEEE}},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/li_ge_2013_human–robot_collaboration_based_on_motion_intention_estimation.pdf}
}

@inproceedings{liImpedanceControlMultipoint2011,
  title = {Impedance Control for Multi-Point Human-Robot Interaction},
  booktitle = {2011 8th {{Asian Control Conference}} ({{ASCC}})},
  author = {Li, Yanan and Ge, Shuzhi Sam and Yang, Chenguang},
  date = {2011-05},
  pages = {1187--1192},
  abstract = {In this paper, impedance control is discussed for multi-point human-robot interaction. The impedance parameters at each interaction point are obtained by taking the environment dynamics into consideration. An optimization method is proposed to deal with the over-constrained problem along with the multipoint interaction control. The desired impedance model in the joint space is accordingly developed. A model-free control method is utilized to guarantee the robot dynamics governed by the desired model. The validity of the proposed methods is verified through simulation.},
  eventtitle = {2011 8th {{Asian Control Conference}} ({{ASCC}})},
  keywords = {Force,Impedance,Joints,Robot sensing systems,Stability analysis,Torque},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/li_et_al_2011_impedance_control_for_multi-point_human-robot_interaction.pdf}
}

@article{liImpedanceLearningRobots2014,
  title = {Impedance {{Learning}} for {{Robots Interacting With Unknown Environments}}},
  author = {Li, Yanan and Ge, Shuzhi Sam},
  date = {2014-07},
  journaltitle = {IEEE Transactions on Control Systems Technology},
  volume = {22},
  number = {4},
  pages = {1422--1432},
  issn = {1558-0865},
  doi = {10.1109/TCST.2013.2286194},
  abstract = {In this paper, impedance learning is investigated for robots interacting with unknown environments. A two-loop control framework is employed and adaptive control is developed for the inner-loop position control. The environments are described as time-varying systems with unknown parameters in the state-space form. The gradient-following and betterment schemes are employed to obtain a desired impedance model, subject to unknown environments. The desired interaction performance is achieved in the sense that a defined cost function is minimized. Simulation and experiment studies are carried out to verify the validity of the proposed method.},
  eventtitle = {{{IEEE Transactions}} on {{Control Systems Technology}}},
  keywords = {Adaptive control,Cost function,Force,Impedance,impedance learning,interaction control,Position control,READ,robotic control,Robots,Trajectory,unknown environment,unknown environment.},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/li_ge_2014_impedance_learning_for_robots_interacting_with_unknown_environments.pdf}
}

@article{liLearningImpedanceControl2012,
  title = {Learning Impedance Control for Physical Robot–Environment Interaction},
  author = {Li, Yanan and Sam Ge, Shuzhi and Yang, Chenguang},
  date = {2012},
  journaltitle = {International Journal of Control},
  volume = {85},
  number = {2},
  pages = {182--193},
  publisher = {{Taylor \& Francis}},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/li_et_al_2012_learning_impedance_control_for_physical_robot–environment_interaction.pdf}
}

@unpublished{lillicrapContinuousControlDeep2015,
  title = {Continuous Control with Deep Reinforcement Learning},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  date = {2015},
  eprint = {1509.02971},
  eprinttype = {arxiv},
  keywords = {#nosource,machine learning control,policy optimization,q-learning}
}

@unpublished{lillicrapContinuousControlDeep2019,
  title = {Continuous Control with Deep Reinforcement Learning},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  date = {2019-07-05},
  eprint = {1509.02971},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1509.02971},
  urldate = {2020-07-02},
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  keywords = {#nosource,Computer Science - Machine Learning,deterministic policy gradients,model-free,policy optimization,q-learning,reinforcement learning,Statistics - Machine Learning},
  annotation = {03725}
}

@inproceedings{liModelfreeImpedanceControl2011,
  title = {Model-Free Impedance Control for Safe Human-Robot Interaction},
  booktitle = {2011 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Li, Yanan and Ge, Shuzhi Sam and Yang, Chenguang and Li, Xinyang and Tee, Keng Peng},
  date = {2011},
  pages = {6021--6026},
  publisher = {{IEEE}},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/li_et_al_2011_model-free_impedance_control_for_safe_human-robot_interaction.pdf}
}

@article{liNeuralNetworksImpedance2013,
  title = {Neural Networks Impedance Control of Robots Interacting with Environments},
  author = {Li, Yanan and Ge, Shuzhi Sam and Zhang, Qun and Lee, Tong Heng},
  date = {2013},
  journaltitle = {IET Control Theory \& Applications},
  volume = {7},
  number = {11},
  pages = {1509--1519},
  issn = {1751-8652},
  doi = {10.1049/iet-cta.2012.1032},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1049/iet-cta.2012.1032},
  urldate = {2022-08-25},
  abstract = {In this study, neural networks (NN) impedance control is proposed for robot–environment interaction. Iterative learning control is developed to make the robot dynamics follow a given target impedance model. To cope with the problem of unknown robot dynamics, NN are employed such that neither the robot structure nor the physical parameters are required for the control design. The stability and performance of the resulted closed-loop system are discussed through rigorous analysis and extensive remarks. The validity and feasibility of the proposed method are verified through simulation studies.},
  langid = {english},
  keywords = {closed loop system performance,closed loop systems,control design,control system synthesis,impedance model,iterative learning control,iterative methods,learning systems,neural network impedance control,neurocontrollers,NN impedance control,robot dynamics,robot-environment interaction,self-adjusting systems,stability},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/li_et_al_2013_neural_networks_impedance_control_of_robots_interacting_with_environments.pdf}
}

@article{linObjectiveLearningHuman2021,
  title = {Objective Learning from Human Demonstrations},
  author = {Lin, Jonathan Feng-Shun and Carreno-Medrano, Pamela and Parsapour, Mahsa and Sakr, Maram and Kulić, Dana},
  date = {2021-01-01},
  journaltitle = {Annual Reviews in Control},
  shortjournal = {Annual Reviews in Control},
  volume = {51},
  pages = {111--129},
  issn = {1367-5788},
  doi = {10.1016/j.arcontrol.2021.04.003},
  url = {https://www.sciencedirect.com/science/article/pii/S1367578821000213},
  urldate = {2022-10-24},
  abstract = {Researchers in biomechanics, neuroscience, human–machine interaction and other fields are interested in inferring human intentions and objectives from observed actions. The problem of inferring objectives from observations has received extensive theoretical and methodological development from both the controls and machine learning communities. In this paper, we provide an integrating view of objective learning from human demonstration data. We differentiate algorithms based on the assumptions made about the objective function structure, how the similarity between the inferred objectives and the observed demonstrations is assessed, the assumptions made about the agent and environment model, and the properties of the observed human demonstrations. We review the application domains and validation approaches of existing works and identify the key open challenges and limitations. The paper concludes with an identification of promising directions for future work.},
  langid = {english},
  keywords = {Inverse optimal control,Inverse reinforcement learning,READ,REVIEW,Reward learning},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/lin_et_al_2021_objective_learning_from_human_demonstrations.pdf}
}

@inproceedings{liNovelForceOscillations2021,
  title = {A {{Novel Force Oscillations Reduction Method Based}} on {{Nonlinear Tracking Differentiator}} for {{Robot Contact Transition Process}}},
  booktitle = {2021 6th {{IEEE International Conference}} on {{Advanced Robotics}} and {{Mechatronics}} ({{ICARM}})},
  author = {Li, Xiangfei and Zhao, Huan and Ding, Han},
  date = {2021-07},
  pages = {329--334},
  doi = {10.1109/ICARM52023.2021.9536068},
  abstract = {Impedance control is an effective force tracking approach for robot contact operation tasks such as grinding/polishing, deburring, assembly and dexterous hand manipulations, etc. Within the impedance control framework, when the robot begins to contact with the environment, however, force oscillations may occur even if the optimal approach velocity is adopted, which may increase the settling time of the system, result in serious damage to the parts, and even make the otherwise stable controller unstable. To this end, based on the real-time adjustment of the desired force by the nonlinear tracking differentiator (NTD) strategy, a novel force oscillations reduction method for robot contact tasks is proposed in this article, which can reduce the force oscillations in a feed-forward style. To the best of our knowledge, this is the first time that the NTD is used to reduce the force oscillations. The performance of the proposed method is evaluated by simulations and experiments, and the results demonstrate that the proposed method can effectively reduce the force oscillations, which provides a simple and useful solution for practical robot contact applications.},
  eventtitle = {2021 6th {{IEEE International Conference}} on {{Advanced Robotics}} and {{Mechatronics}} ({{ICARM}})},
  keywords = {Aerospace electronics,Control systems,Force,Impedance,Process control,Real-time systems,Switches},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/li_et_al_2021_a_novel_force_oscillations_reduction_method_based_on_nonlinear_tracking.pdf}
}

@article{linUnifiedMotionForce2021,
  title = {Unified {{Motion}}/{{Force}}/{{Impedance Control}} for {{Manipulators}} in {{Unknown Contact Environments Based}} on {{Robust Model-Reaching Approach}}},
  author = {Lin, Yinjie and Chen, Zheng and Yao, Bin},
  date = {2021-08},
  journaltitle = {IEEE/ASME Transactions on Mechatronics},
  volume = {26},
  number = {4},
  pages = {1905--1913},
  issn = {1941-014X},
  doi = {10.1109/TMECH.2021.3081594},
  abstract = {With the developments of intelligent and autonomous robotic technology, robots are usually designed to confront sophisticated tasks such as automated assembly, which requires both high-speed positioning capabilities and compliance with unknown contact environments. As we know, a high-performance motion tracking control in free space can achieve efficient and accurate positioning, while impedance or force control shows superior performance in terms of sensitive force and compliance with unknown contact environments. However, it is still challenging to achieve both high-performance motion tracking and compliance within one single control framework, especially in unknown contact environments. To this end, in this article, a unified motion/force/impedance approach for unknown contact environments is proposed by robust model-reaching control with dynamic trajectory adaptation. Specifically, the overall control scheme includes two loops: the outer loop replans the trajectories of motion and force in real time to meet the environmental constraints, which are estimated by the recursive least squares estimation law; in the inner loop, the robust model-reaching control law is proposed to realize a target model, which is designed to establish a dynamic relationship between the motion and force tracking errors of the replanned trajectories. Then, by changing the matrices of the target model, the compromise between motion tracking and force tracking can be achieved, as well as different control objectives. Experiments are conducted on a seven-degree-of-freedom manipulator to validate the advantages of the proposed scheme.},
  eventtitle = {{{IEEE}}/{{ASME Transactions}} on {{Mechatronics}}},
  keywords = {Dynamics,Force,Force control,Impedance,impedance control,manipulator control,Manipulator dynamics,READ,STABILITY,Target tracking,Tracking,Trajectory,trajectory adaptation,unknown environment},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/lin_et_al_2021_unified_motion-force-impedance_control_for_manipulators_in_unknown_contact.pdf}
}

@inproceedings{lioutikovSamplebasedInformationltheoreticStochastic2014,
  title = {Sample-Based Informationl-Theoretic Stochastic Optimal Control},
  booktitle = {2014 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Lioutikov, Rudolf and Paraschos, Alexandros and Peters, Jan and Neumann, Gerhard},
  date = {2014-05},
  pages = {3896--3902},
  publisher = {{IEEE}},
  location = {{Hong Kong, China}},
  doi = {10.1109/ICRA.2014.6907424},
  url = {http://ieeexplore.ieee.org/document/6907424/},
  urldate = {2019-04-18},
  abstract = {Many Stochastic Optimal Control (SOC) approaches rely on samples to either obtain an estimate of the value function or a linearisation of the underlying system model. However, these approaches typically neglect the fact that the accuracy of the policy update depends on the closeness of the resulting trajectory distribution to these samples. The greedy operator does not consider such closeness constraint to the samples. Hence, the greedy operator can lead to oscillations or even instabilities in the policy updates. Such undesired behaviour is likely to result in an inferior performance of the estimated policy. We reuse inspiration from the reinforcement learning community and relax the greedy operator used in SOC with an information theoretic bound that limits the ‘distance’ of two subsequent trajectory distributions in a policy update. The introduced bound ensures a smooth and stable policy update. Our method is also well suited for model-based reinforcement learning, where we estimate the system dynamics model from data. As this model is likely to be inaccurate, it might be dangerous to exploit the model greedily. Instead, our bound ensures that we generate new data in the vicinity of the current data, such that we can improve our estimate of the system dynamics model. We show that our approach outperforms several state of the art approaches on challenging simulated robot control tasks.},
  eventtitle = {2014 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  isbn = {978-1-4799-3685-4},
  langid = {english},
  keywords = {#nosource,machine learning control}
}

@inproceedings{liPhysicalHumanRobotInteraction2018,
  title = {Physical {{Human-Robot Interaction Coupled}} with a {{Moving Environment}} or {{Target}}: {{Contact}} and {{Track}}},
  shorttitle = {Physical {{Human-Robot Interaction Coupled}} with a {{Moving Environment}} or {{Target}}},
  booktitle = {2018 {{IEEE}} 14th {{International Conference}} on {{Automation Science}} and {{Engineering}} ({{CASE}})},
  author = {Li, Hsieh-Yu and Paranawithana, Ishara and Yang, Liangjing and Tan, U-Xuan},
  date = {2018-08},
  pages = {43--49},
  issn = {2161-8089},
  doi = {10.1109/COASE.2018.8560702},
  abstract = {There is an increasing number of applications in physical human-robot interaction (pHRI) where the end-effector of the robot is compliant in response to the force exerted by the human. The force sensor is normally mounted with an instrument on the end-effector to measure the human operational force. However, when the robot is in contact with the human and an environment simultaneously, the force sensor reading includes both the human and the environmental force resulting in ineffective contacting interaction within these three objects (robot, human and environment). In addition, if the environment is moving, it is more challenging for the operator to track the target with the robot. Therefore, in this paper, we address the issue of pHRI coupled with a moving environment. More specifically, we use a collaborative robot with an ultrasound probe as an illustration due to its sophisticated condition: the operator needs to contact the environment using a sufficient force to get clearer images and track the moving target. The proposed control scheme is employed using only one force sensor to guarantee a stable physical interaction within three objects and provide the compliant and intuitive operation for human. Experiments with a collaborative robot are conducted to evaluate the effectiveness of the proposed controller.},
  eventtitle = {2018 {{IEEE}} 14th {{International Conference}} on {{Automation Science}} and {{Engineering}} ({{CASE}})},
  keywords = {Cartesian impedance,Collaboration,End effectors,Force,Force sensors,Human-robot interaction,moving environment,Robot sensing systems,STABILITY,Target tracking},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/li_et_al_2018_physical_human-robot_interaction_coupled_with_a_moving_environment_or_target.pdf}
}

@inproceedings{lippiDataDrivenApproachContact2021,
  title = {A {{Data-Driven Approach}} for {{Contact Detection}}, {{Classification}} and {{Reaction}} in {{Physical Human-Robot Collaboration}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Lippi, Martina and Gillini, Giuseppe and Marino, Alessandro and Arrichiello, Filippo},
  date = {2021-05},
  pages = {3597--3603},
  issn = {2577-087X},
  doi = {10.1109/ICRA48506.2021.9561827},
  abstract = {This paper considers a scenario where a robot and a human operator share the same workspace, and the robot is able to both carry out autonomous tasks and physically interact with the human in order to achieve common goals. In this context, both intentional and accidental contacts between human and robot might occur due to the complexity of tasks and environment, to the uncertainty of human behavior, and to the typical lack of awareness of each other actions. Here, a two stage strategy based on Recurrent Neural Networks (RNNs) is designed to detect intentional and accidental contacts: the occurrence of a contact with the human is detected at the first stage, while the classification between intentional and accidental is performed at the second stage. An admittance control strategy or an evasive action is then performed by the robot, respectively. The approach also works in the case the robot simultaneously interacts with the human and the environment, where the interaction wrench of the latter is modeled via Gaussian Mixture Models (GMMs). Control Barrier Functions (CBFs) are included, at the control level, to guarantee the satisfaction of robot and task constraints while performing the proper interaction strategy. The approach has been validated on a real setup composed of a Kinova Jaco2 robot.},
  eventtitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  keywords = {Automation,Collaboration,Complexity theory,Conferences,Recurrent neural networks,Time series analysis,Uncertainty},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/lippi_et_al_2021_a_data-driven_approach_for_contact_detection,_classification_and_reaction_in.pdf}
}

@online{liProDMPsUnifiedPerspective2022,
  title = {{{ProDMPs}}: {{A Unified Perspective}} on {{Dynamic}} and {{Probabilistic Movement Primitives}}},
  shorttitle = {{{ProDMPs}}},
  author = {Li, Ge and Jin, Zeqi and Volpp, Michael and Otto, Fabian and Lioutikov, Rudolf and Neumann, Gerhard},
  date = {2022-10-04},
  eprint = {2210.01531},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.01531},
  url = {http://arxiv.org/abs/2210.01531},
  urldate = {2022-10-30},
  abstract = {Movement Primitives (MPs) are a well-known concept to represent and generate modular trajectories. MPs can be broadly categorized into two types: (a) dynamics-based approaches that generate smooth trajectories from any initial state, e. g., Dynamic Movement Primitives (DMPs), and (b) probabilistic approaches that capture higher-order statistics of the motion, e. g., Probabilistic Movement Primitives (ProMPs). To date, however, there is no method that unifies both, i. e. that can generate smooth trajectories from an arbitrary initial state while capturing higher-order statistics. In this paper, we introduce a unified perspective of both approaches by solving the ODE underlying the DMPs. We convert expensive online numerical integration of DMPs into basis functions that can be computed offline. These basis functions can be used to represent trajectories or trajectory distributions similar to ProMPs while maintaining all the properties of dynamical systems. Since we inherit the properties of both methodologies, we call our proposed model Probabilistic Dynamic Movement Primitives (ProDMPs). Additionally, we embed ProDMPs in deep neural network architecture and propose a new cost function for efficient end-to-end learning of higher-order trajectory statistics. To this end, we leverage Bayesian Aggregation for non-linear iterative conditioning on sensory inputs. Our proposed model achieves smooth trajectory generation, goal-attractor convergence, correlation analysis, non-linear conditioning, and online re-planing in one framework.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/li_et_al_2022_prodmps.pdf}
}

@article{liReinforcementLearningManipulation2018,
  title = {Reinforcement {{Learning}} of {{Manipulation}} and {{Grasping Using Dynamical Movement Primitives}} for a {{Humanoidlike Mobile Manipulator}}},
  author = {Li, Zhijun and Zhao, Ting and Chen, Fei and Hu, Yingbai and Su, Chun-Yi and Fukuda, Toshio},
  date = {2018-02},
  journaltitle = {IEEE/ASME Transactions on Mechatronics},
  volume = {23},
  number = {1},
  pages = {121--131},
  issn = {1941-014X},
  doi = {10.1109/TMECH.2017.2717461},
  abstract = {It is important for humanoid-like mobile robots to learn the complex motion sequences in human-robot environment such that the robots can adapt such motions. This paper describes a reinforcement learning (RL) strategy for manipulation and grasping of a mobile manipulator, which reduces the complexity of the visual feedback and handle varying manipulation dynamics and uncertain external perturbations. Two hierarchies plannings have been considered in the proposed strategy: 1) high-level online redundancy resolution based on the neural-dynamic optimization algorithm in operational space; and 2) low-level RL in joint space. At this level, the dynamic movement primitives have been considered to model and learn the joint trajectories, and then the RL is employed to learn the trajectories with uncertainties. Experimental results on the developed humanoidlike mobile robot demonstrate that the presented approach can suppress the uncertain external perturbations.},
  eventtitle = {{{IEEE}}/{{ASME Transactions}} on {{Mechatronics}}},
  keywords = {Dynamic movement primitive (DMP),Learning (artificial intelligence),Manipulator dynamics,Mobile communication,mobile manipulation,redundancy resolution,reinforcement learning (RL),Robot sensing systems,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/li_et_al_2018_reinforcement_learning_of_manipulation_and_grasping_using_dynamical_movement.pdf}
}

@inproceedings{liReviewMachineLearning2019,
  title = {A Review: Machine Learning on Robotic Grasping},
  shorttitle = {A Review},
  booktitle = {Eleventh {{International Conference}} on {{Machine Vision}} ({{ICMV}} 2018)},
  author = {family=Li, given=You, prefix=hao, useprefix=false and family=Lei, given=Qu, prefix=jiang, useprefix=false and Cheng, ChaoPeng and Zhang, Gong and Wang, Weijun and Xu, Zheng},
  editor = {Nikolaev, Dmitry P. and Radeva, Petia and Verikas, Antanas and Zhou, Jianhong},
  date = {2019-03-15},
  pages = {54},
  publisher = {{SPIE}},
  location = {{Munich, Germany}},
  doi = {10.1117/12.2522945},
  url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11041/2522945/A-review-machine-learning-on-robotic-grasping/10.1117/12.2522945.full},
  urldate = {2019-04-14},
  abstract = {Machine learning has made breakthroughs in areas such as computer vision and natural language processing. In recent years, more and more research has been done on the application of machine learning on robotic grasping. This article summarizes the research progress of machine learning on robotic grasping, from the aspects of object grasping datasets, two main categories of methods that differ from the criteria for successful grasping with deep learning or reinforcement learning algorithm, discusses what current researches have done and the problems that have not yet been solved, and hopes to inspire new ideas in research of robotic grasping based on machine learning.},
  eventtitle = {Eleventh {{International Conference}} on {{Machine Vision}}},
  isbn = {978-1-5106-2748-2 978-1-5106-2749-9},
  langid = {english},
  keywords = {#nosource,robotic grasping,tno internship}
}

@article{liStableCompliantMotion2018,
  title = {Stable and {{Compliant Motion}} of {{Physical Human}}–{{Robot Interaction Coupled With}} a {{Moving Environment Using Variable Admittance}} and {{Adaptive Control}}},
  author = {Li, Hsieh-Yu and Paranawithana, Ishara and Yang, Liangjing and Lim, Terence Sey Kiat and Foong, Shaohui and Ng, Foo Cheong and Tan, U-Xuan},
  date = {2018-07},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {3},
  number = {3},
  pages = {2493--2500},
  issn = {2377-3766},
  doi = {10.1109/LRA.2018.2812916},
  abstract = {There are an increasing number of applications that require the end-effector of a robotic manipulator to be compliant in response to the intended force exerted by a human operator. In physical human-robot interaction, the stability and compliance are often affected in the presence of both human and an environment, especially during the transition from noncontact to contact. Unlike previous works, the physical human-robot interaction in this letter takes into consideration of the interaction of the human, robot, and a moving environment. To achieve this, this letter proposes a variable admittance control strategy with an adaptive controller to produce a time-varying stiffness interacting with the human and a moving environment simultaneously. The passivity of the system is guaranteed for time-varying admittance with human interaction coupled with a moving environment. Experiments are conducted to demonstrate the capability of the controller.},
  eventtitle = {{{IEEE Robotics}} and {{Automation Letters}}},
  keywords = {Adaptive control,Admittance,Compliance and impedance control,Force,Force sensors,Human-robot interaction,physical human-robot interaction (pHRI),Robot sensing systems},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/li_et_al_2018_stable_and_compliant_motion_of_physical_human–robot_interaction_coupled_with_a.pdf}
}

@inreference{ListMathematicalSymbols2022,
  title = {List of Mathematical Symbols by Subject},
  booktitle = {Wikipedia},
  date = {2022-06-03T03:30:30Z},
  url = {https://en.wikipedia.org/w/index.php?title=List_of_mathematical_symbols_by_subject&oldid=1091247735},
  urldate = {2022-06-20},
  abstract = {The following list of mathematical symbols by subject features a selection of the most common symbols used in modern mathematical notation within formulas, grouped by mathematical topic. As it is impossible to know if a complete list existing today of all symbols used in history is a representation of all ever used in history, as this would necessitate knowing if extant records are of all usages, only those symbols which occur often in mathematics or mathematics education are included. Many of the characters are standardized, for example in DIN 1302 General mathematical symbols or DIN EN ISO 80000-2 Quantities and units – Part 2: Mathematical signs for science and technology. The following list is largely limited to non-alphanumeric characters. It is divided by areas of mathematics and grouped within sub-regions. Some symbols have a different meaning depending on the context and appear accordingly several times in the list. Further information on the symbols and their meaning can also be found in the respective linked articles.},
  langid = {english},
  annotation = {Page Version ID: 1091247735}
}

@unpublished{liuActiondepedentControlVariates2018,
  title = {Action-Depedent {{Control Variates}} for {{Policy Optimization}} via {{Stein}}'s {{Identity}}},
  author = {Liu, Hao and Feng, Yihao and Mao, Yi and Zhou, Dengyong and Peng, Jian and Liu, Qiang},
  date = {2018-02-23},
  eprint = {1710.11198},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1710.11198},
  urldate = {2020-07-02},
  abstract = {Policy gradient methods have achieved remarkable successes in solving challenging reinforcement learning problems. However, it still often suffers from the large variance issue on policy gradient estimation, which leads to poor sample efficiency during training. In this work, we propose a control variate method to effectively reduce variance for policy gradient methods. Motivated by the Stein's identity, our method extends the previous control variate methods used in REINFORCE and advantage actor-critic by introducing more general action-dependent baseline functions. Empirical studies show that our method significantly improves the sample efficiency of the state-of-the-art policy gradient approaches.},
  keywords = {#nosource,action-dependent baseline,Computer Science - Machine Learning,model-free,policy gradients,reinforcement learning,Statistics - Machine Learning}
}

@article{liuAdaptiveEnhancedAdmittance2022,
  title = {Adaptive Enhanced Admittance Force-Tracking Controller Design for Highly Dynamic Interactive Tasks},
  author = {Liu, Chengguo and He, Ye and Chen, Xiaoan and Cao, Hongli},
  date = {2022-01-01},
  journaltitle = {Industrial Robot: the international journal of robotics research and application},
  volume = {ahead-of-print},
  issn = {0143-991X},
  doi = {10.1108/IR-10-2021-0222},
  url = {https://doi.org/10.1108/IR-10-2021-0222},
  urldate = {2022-05-02},
  abstract = {Purpose As more and more robots are used in industry, it is necessary for robots to interact with high dynamic environments. For this reason, the purpose of this research is to form an excellent force controller by considering the transient contact force response, overshoot and steady-state force-tracking accuracy. Design/methodology/approach Combining the active disturbance rejection control (ADRC) and the adaptive fuzzy PD controller, an enhanced admittance force-tracking controller framework and a well-designed control scheme are proposed. Tracking differentiator balances the contradiction between inertia and jump control signal of the control object. Kalman filter and extended state observer are introduced to obtain purer feedback force signal and uncertainty compensation. Adaptive fuzzy PD controller is introduced to account for transient and steady state performance of the system. Findings The proposed controller has achieved successful results through simulation and actual test of 6-axis robot with minimum error. Practical implications The controller is simple and practical in real industrial scenarios, where force control by robots is required. Originality/value In this research, a new practical force control algorithm is proposed to guarantee the performance of the force controller for robots interacting with high dynamic environments.},
  issue = {ahead-of-print},
  keywords = {Adaptive control,Admittance control,Force overshoot,Fuzzy control,READ,Robot force control,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/liu_et_al_2022_adaptive_enhanced_admittance_force-tracking_controller_design_for_highly.pdf}
}

@article{liuAdaptiveVariableImpedance2022,
  title = {Adaptive {{Variable Impedance Control}} with {{Fuzzy-PI Compound Controller}} for {{Robot Trimming System}}},
  author = {Liu, Ze and Sun, Yu},
  date = {2022-03-26},
  journaltitle = {Arabian Journal for Science and Engineering},
  shortjournal = {Arab J Sci Eng},
  issn = {2191-4281},
  doi = {10.1007/s13369-022-06755-z},
  url = {https://doi.org/10.1007/s13369-022-06755-z},
  urldate = {2022-04-29},
  abstract = {The supporting element is an important part of the robot trimming system and is used to locate and support thin-walled parts. However, the unstable supporting force directly leads to vibration and even deformation of thin-walled parts, so maintaining an ideal supporting force is essential to raise the processing quality of thin-walled parts. In this paper, an adaptive variable impedance control with Fuzzy-PI compound controller designed for the supporting element is presented, which has the ability to resolve the force tracking problem under unknown environments. The method combines the fast response of Fuzzy control with the steady-state error suppression of PI control and can quickly and stably track the ideal force. In this study, the defects of the traditional constant impedance control in an unknown environment are firstly pointed out, and then an adaptive variable impedance control method using a Fuzzy-PI compound controller to adjust the target damping online is proposed to compensate for force tracking errors caused by the unknown environment based on these defects. In addition, the stability and convergence of the proposed method in the force tracking process are demonstrated. The simulation and experimental results show that, compared to the traditional constant impedance control, the force tracking performance of the proposed control method is significantly improved in terms of response speed and steady-state accuracy.},
  langid = {english},
  keywords = {Adaptive variable impedance control,convergence analysis,Force tracking,Fuzzy-PI compound controller,READ,Robot trimming system,STABILITY,Unknown environment},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/liu_sun_2022_adaptive_variable_impedance_control_with_fuzzy-pi_compound_controller_for_robot.pdf}
}

@inproceedings{liuForceTrackingImpedance2017,
  title = {Force Tracking Impedance Control with Moving Target},
  booktitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Biomimetics}} ({{ROBIO}})},
  author = {Liu, Houde and Lu, Weifeng and Zhu, Xiaojun and Wang, Xueqian and Liang, Bin},
  date = {2017},
  pages = {1369--1374},
  publisher = {{IEEE}},
  keywords = {Fixed impedance,moving environment,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/liu_et_al_2017_force_tracking_impedance_control_with_moving_target.pdf}
}

@article{liuImprovingRobustnessMPCbased2019,
  title = {Improving the Robustness of an {{MPC-based}} Obstacle Avoidance Algorithm to Parametric Uncertainty Using Worst-Case Scenarios},
  author = {Liu, Jiechao and Jayakumar, Paramsothy and Stein, Jeffrey L. and Ersal, Tulga},
  date = {2019},
  journaltitle = {Vehicle System Dynamics},
  volume = {57},
  number = {6},
  pages = {874--913},
  keywords = {#nosource,machine learning control,safe-rl}
}

@article{liuLearningHowGrasp2017,
  title = {Learning How to {{Grasp Objects}} with {{Robotic Gripper}} Using {{Deep Reinforcement Learning}}},
  author = {Liu, Jiaxi and Johns, Dr Edward},
  date = {2017},
  pages = {97},
  langid = {english},
  keywords = {#nosource,robotic grasping,tno internship}
}

@inproceedings{liuPredictingNextLocation2016,
  title = {Predicting the next Location: {{A}} Recurrent Model with Spatial and Temporal Contexts},
  shorttitle = {Predicting the next Location},
  booktitle = {Thirtieth {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Liu, Qiang and Wu, Shu and Wang, Liang and Tan, Tieniu},
  date = {2016},
  keywords = {#nosource,path planning,robotic grasping,tno internship},
  annotation = {00143}
}

@article{lohmillerContractionAnalysisNonlinear1998,
  title = {On {{Contraction Analysis}} for {{Non-linear Systems}}},
  author = {family=Lohmiller, given=WINFRIED, given-i=WINFRIED and Slotine, JEAN-JACQUES E.},
  date = {1998-06-01},
  journaltitle = {Automatica},
  shortjournal = {Automatica},
  volume = {34},
  number = {6},
  pages = {683--696},
  issn = {0005-1098},
  doi = {10.1016/S0005-1098(98)00019-3},
  url = {https://www.sciencedirect.com/science/article/pii/S0005109898000193},
  urldate = {2022-09-30},
  abstract = {This paper derives new results in non-linear system analysis using methods inspired from fluid mechanics and differential geometry. Based on a differential analysis of convergence, these results may be viewed as generalizing the classical Krasovskii theorem, and, more loosely, linear eigenvalue analysis. A central feature is that convergence and limit behavior are in a sense treated separately, leading to significant conceptual simplifications. The approach is illustrated by controller and observer designs for simple physical examples.},
  langid = {english},
  keywords = {contraction analysis,gain-scheduling,non-linear control,Non-linear dynamics,observers},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/lohmiller_slotine_1998_on_contraction_analysis_for_non-linear_systems.pdf}
}

@article{lohmillerContractionAnalysisNonlinear1998a,
  title = {Contraction Analysis for Non-Linear Systems},
  author = {Lohmiller, Winfried and Slotine, Jean-Jacques E.},
  date = {1998},
  journaltitle = {Automatica},
  volume = {34},
  number = {6},
  pages = {683--696},
  publisher = {{Elsevier}},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/lohmiller_slotine_1998_on_contraction_analysis_for_non-linear_systems.pdf}
}

@inproceedings{loschiOptimizedTwoLayerApproach2021,
  title = {An {{Optimized Two-Layer Approach}} for {{Efficient}} and {{Robustly Stable Bilateral Teleoperation}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Loschi, Filippo and Piccinelli, Nicola and Dall’Alba, Diego and Muradore, Riccardo and Fiorini, Paolo and Secchi, Cristian},
  date = {2021-05},
  pages = {12449--12455},
  issn = {2577-087X},
  doi = {10.1109/ICRA48506.2021.9561542},
  abstract = {In this paper, we propose a novel bilateral teleoperation architecture that allows to optimally render the remote interaction force at the local side while guaranteeing a robustly stable behaviour. Stability is guaranteed by ensuring a proper energy exchange between the local and the remote sides. Desired performance is obtained by optimizing the way energy is exploited for generating the behaviour at each side. The effectiveness of the proposed architecture is experimentally validated on a torque-controlled manipulator and in a surgical scenario, using the da Vinci® Research Kit (dVRK).},
  eventtitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  keywords = {Automation,Conferences,Energy exchange,Force,Manipulators,STABILITY,teleoperation},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/loschi_et_al_2021_an_optimized_two-layer_approach_for_efficient_and_robustly_stable_bilateral.pdf}
}

@inproceedings{lukicLearningCoupledDynamical2012,
  title = {Learning {{Coupled Dynamical Systems}} from {{Human Demonstration}} for {{Robotic Eye-Arm-Hand Coordination}}},
  booktitle = {2012 12th {{Ieee-Ras International Conference}} on {{Humanoid Robots}} (Humanoids)},
  author = {Lukic, Luka and Santos-Victor, Jose and Billard, Aude},
  date = {2012},
  pages = {552--559},
  publisher = {{Ieee}},
  location = {{New York}},
  issn = {2164-0572},
  url = {https://infoscience.epfl.ch/record/181646/files/LUKIC_HUMANOIDS_2012.pdf},
  urldate = {2022-09-12},
  abstract = {Efficient, adaptive and reliable visuomotor control system is crucial to enable robots to display flexibility in the face of changes in the environment. This paper takes inspiration in human eye-arm-hand coordination pattern to develop an equivalently robust robot controller. We recorded gaze, arm, hand, and trunk data from human subjects in reaching and grasping scenarios with/without obstacle in the workspace. An eye-arm-hand controller is developed, based on our extension of Coupled Dynamical Systems (CDS). We exploit the time-invariant properties of the CDS to allow fast adaptation to spatial and temporal perturbations during task completion. CDS global stability guarantees that the eye, the arm and the hand will reach the target in retinal, operational and grasp space respectively. When facing perturbations, the system can re-plan its actions almost instantly, without the need for an additional planning module. Coupling profiles for eye-arm and arm-hand systems can be modulated allowing to adjust the behavior of each slave system with respect to control signals flowing from the corresponding master system. We show how the CDS eye-arm-hand control framework can be used to handle the presence of obstacles in the workspace. The eye-arm-hand controller is validated in a series of experiments conducted with the iCub robot.},
  isbn = {978-1-4673-1369-8},
  langid = {english},
  keywords = {READ,STABILITY,target},
  annotation = {WOS:000392842100084},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/lukic_et_al_2012_learning_coupled_dynamical_systems_from_human_demonstration_for_robotic.pdf}
}

@unpublished{luoReinforcementLearningVariable2019,
  title = {Reinforcement {{Learning}} on {{Variable Impedance Controller}} for {{High-Precision Robotic Assembly}}},
  author = {Luo, Jianlan and Solowjow, Eugen and Wen, Chengtao and Ojea, Juan Aparicio and Agogino, Alice M. and Tamar, Aviv and Abbeel, Pieter},
  date = {2019-03-20},
  eprint = {1903.01066},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1903.01066},
  urldate = {2022-04-29},
  abstract = {Precise robotic manipulation skills are desirable in many industrial settings, reinforcement learning (RL) methods hold the promise of acquiring these skills autonomously. In this paper, we explicitly consider incorporating operational space force/torque information into reinforcement learning; this is motivated by humans heuristically mapping perceived forces to control actions, which results in completing high-precision tasks in a fairly easy manner. Our approach combines RL with force/torque information by incorporating a proper operational space force controller; where we also exploit different ablations on processing this information. Moreover, we propose a neural network architecture that generalizes to reasonable variations of the environment. We evaluate our method on the open-source Siemens Robot Learning Challenge, which requires precise and delicate force-controlled behavior to assemble a tight-fit gear wheel set.},
  keywords = {Computer Science - Robotics,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/luo_et_al_2019_reinforcement_learning_on_variable_impedance_controller_for_high-precision.pdf}
}

@article{lutscherHierarchicalForcePositioning2017,
  title = {Hierarchical Force and Positioning Task Specification for Indirect Force Controlled Robots},
  author = {Lutscher, Ewald and Dean-León, Emmanuel C. and Cheng, Gordon},
  date = {2017},
  journaltitle = {IEEE Transactions on Robotics},
  volume = {34},
  number = {1},
  pages = {280--286},
  publisher = {{IEEE}},
  keywords = {STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/lutscher_et_al_2017_hierarchical_force_and_positioning_task_specification_for_indirect_force.pdf}
}

@inproceedings{lyuHybridSlidingMode2021,
  title = {Hybrid {{Sliding Mode Impedance Control}} of {{Manipulator Based}} on {{Fuzzy Inference Engine}}},
  booktitle = {2021 {{China Automation Congress}} ({{CAC}})},
  author = {Lyu, Tao and Yang, Fangyan and Li, Qingdu and Wang, Qinghai},
  date = {2021-10},
  pages = {6445--6450},
  issn = {2688-0938},
  doi = {10.1109/CAC53003.2021.9727830},
  abstract = {A hybrid sliding mode impedance controller based on fuzzy inference machine(FCSMC) is proposed for the lack of robustness of traditional impedance control in unknown and uncertain environments. Based on the hybrid control idea, the task space is decoupled into two subspaces of position and force. The sliding-mode impedance controller achieves accurate force/position tracking by adjusting the sliding-mode gain, and compensates the disturbance error of the system by approximating the uncertainty of the system with the fuzzy inference machine, so as to achieve the final steady state. The stability of the system is supported by Lyapunov theory. Finally, the practicality and effectiveness of the strategy are verified with MATLAB simulation experiments.},
  eventtitle = {2021 {{China Automation Congress}} ({{CAC}})},
  keywords = {Aerospace electronics,Force,fuzzy inference engine,Fuzzy logic,hybrid control,manipulator,sliding mode impedance control,Stability criteria,Steady-state,Trajectory tracking,Uncertainty},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/lyu_et_al_2021_hybrid_sliding_mode_impedance_control_of_manipulator_based_on_fuzzy_inference.pdf}
}

@inproceedings{maFractionalorderSlidingMode2019,
  title = {Fractional-Order {{Sliding Mode Based Variable Stiffness}} and {{Damping Impedance Control}} with {{Disturbance Observer}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Cybernetics}} and {{Intelligent Systems}} ({{CIS}}) and {{IEEE Conference}} on {{Robotics}}, {{Automation}} and {{Mechatronics}} ({{RAM}})},
  author = {Ma, Zhiqiang and Dong, Gangqi},
  date = {2019-11},
  pages = {316--320},
  issn = {2326-8239},
  doi = {10.1109/CIS-RAM47153.2019.9095801},
  abstract = {This paper proposes a fractional-order sliding mode based variable stiffness and damping impedance controller. A new closed-loop impedance architecture containing the negative power type stiffness and damping parameters is built, and an integral fractional-order sliding surface synthesizes constant and variable impedance parameters to construct a reduced order system, which indicates an impedance behavior. The external disturbance easily propagates to the preset impedance architecture to degenerate the stable performance of reduced order system, and it is well estimated by a fractional-order disturbance observer to reduce the adverse effect by partially eliminating the disturbance. The proposed impedance architecture owns better transient performance compared with the trivial spring-damper impedance architecture, which is well illustrated by the verified numerical results.},
  eventtitle = {2019 {{IEEE International Conference}} on {{Cybernetics}} and {{Intelligent Systems}} ({{CIS}}) and {{IEEE Conference}} on {{Robotics}}, {{Automation}} and {{Mechatronics}} ({{RAM}})},
  keywords = {Damping,Force,fractional calculus,Impedance,impedance control,READ,Robots,sliding mode control,STABILITY,Stability analysis,Surface impedance,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ma_dong_2019_fractional-order_sliding_mode_based_variable_stiffness_and_damping_impedance.pdf}
}

@inproceedings{mahlerDexnetCloudbasedNetwork2016,
  title = {Dex-Net 1.0: {{A}} Cloud-Based Network of 3d Objects for Robust Grasp Planning Using a Multi-Armed Bandit Model with Correlated Rewards},
  shorttitle = {Dex-Net 1.0},
  booktitle = {2016 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Mahler, Jeffrey and Pokorny, Florian T. and Hou, Brian and Roderick, Melrose and Laskey, Michael and Aubry, Mathieu and Kohlhoff, Kai and Kröger, Torsten and Kuffner, James and Goldberg, Ken},
  date = {2016},
  pages = {1957--1964},
  publisher = {{IEEE}},
  keywords = {#nosource,object grasping,robotic grasping,tno internship}
}

@unpublished{mahlerDexNetComputingRobust2017,
  title = {Dex-{{Net}} 3.0: {{Computing Robust Robot Vacuum Suction Grasp Targets}} in {{Point Clouds}} Using a {{New Analytic Model}} and {{Deep Learning}}},
  shorttitle = {Dex-{{Net}} 3.0},
  author = {Mahler, Jeffrey and Matl, Matthew and Liu, Xinyu and Li, Albert and Gealy, David and Goldberg, Ken},
  date = {2017-09-19},
  eprint = {1709.06670},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1709.06670},
  urldate = {2019-04-17},
  abstract = {Vacuum-based end effectors are widely used in industry and are often preferred over parallel-jaw and multifinger grippers due to their ability to lift objects with a single point of contact. Suction grasp planners often target planar surfaces on point clouds near the estimated centroid of an object. In this paper, we propose a compliant suction contact model that computes the quality of the seal between the suction cup and local target surface and a measure of the ability of the suction grasp to resist an external gravity wrench. To characterize grasps, we estimate robustness to perturbations in end-effector and object pose, material properties, and external wrenches. We analyze grasps across 1,500 3D object models to generate Dex-Net 3.0, a dataset of 2.8 million point clouds, suction grasps, and grasp robustness labels. We use Dex-Net 3.0 to train a Grasp Quality Convolutional Neural Network (GQ-CNN) to classify robust suction targets in point clouds containing a single object. We evaluate the resulting system in 350 physical trials on an ABB YuMi fitted with a pneumatic suction gripper. When evaluated on novel objects that we categorize as Basic (prismatic or cylindrical), Typical (more complex geometry), and Adversarial (with few available suction-grasp points) Dex-Net 3.0 achieves success rates of 98\$\textbackslash\%\$, 82\$\textbackslash\%\$, and 58\$\textbackslash\%\$ respectively, improving to 81\$\textbackslash\%\$ in the latter case when the training set includes only adversarial objects. Code, datasets, and supplemental material can be found at http://berkeleyautomation.github.io/dex-net .},
  keywords = {#nosource,Computer Science - Robotics,object grasping,robotic grasping,tno internship}
}

@unpublished{mahlerDexNetDeepLearning2017,
  title = {Dex-{{Net}} 2.0: {{Deep Learning}} to {{Plan Robust Grasps}} with {{Synthetic Point Clouds}} and {{Analytic Grasp Metrics}}},
  shorttitle = {Dex-{{Net}} 2.0},
  author = {Mahler, Jeffrey and Liang, Jacky and Niyaz, Sherdil and Laskey, Michael and Doan, Richard and Liu, Xinyu and Ojea, Juan Aparicio and Goldberg, Ken},
  date = {2017-03-27},
  eprint = {1703.09312},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1703.09312},
  urldate = {2019-04-16},
  abstract = {To reduce data collection time for deep learning of robust robotic grasp plans, we explore training from a synthetic dataset of 6.7 million point clouds, grasps, and analytic grasp metrics generated from thousands of 3D models from Dex-Net 1.0 in randomized poses on a table. We use the resulting dataset, DexNet 2.0, to train a Grasp Quality Convolutional Neural Network (GQ-CNN) model that rapidly predicts the probability of success of grasps from depth images, where grasps are specified as the planar position, angle, and depth of a gripper relative to an RGB-D sensor. Experiments with over 1,000 trials on an ABB YuMi comparing grasp planning methods on singulated objects suggest that a GQ-CNN trained with only synthetic data from Dex-Net 2.0 can be used to plan grasps in 0.8s with a success rate of 93\% on eight known objects with adversarial geometry and is 3× faster than registering point clouds to a precomputed dataset of objects and indexing grasps. The Dex-Net 2.0 grasp planner also has the highest success rate on a dataset of 10 novel rigid objects and achieves 99\% precision (one false positive out of 69 grasps classified as robust) on a dataset of 40 novel household objects, some of which are articulated or deformable. Code, datasets, videos, and supplementary material are available at http://berkeleyautomation.github.io/dex-net.},
  langid = {english},
  keywords = {#nosource,Computer Science - Robotics,object grasping,robotic grasping,tno internship}
}

@article{mahlerLearningAmbidextrousRobot2019,
  title = {Learning Ambidextrous Robot Grasping Policies},
  author = {Mahler, Jeffrey and Matl, Matthew and Satish, Vishal and Danielczuk, Michael and DeRose, Bill and McKinley, Stephen and Goldberg, Ken},
  date = {2019-01-16},
  journaltitle = {Science Robotics},
  volume = {4},
  number = {26},
  pages = {eaau4984},
  issn = {2470-9476},
  doi = {10.1126/scirobotics.aau4984},
  url = {https://robotics.sciencemag.org/content/4/26/eaau4984},
  urldate = {2019-04-17},
  abstract = {Universal picking (UP), or reliable robot grasping of a diverse range of novel objects from heaps, is a grand challenge for e-commerce order fulfillment, manufacturing, inspection, and home service robots. Optimizing the rate, reliability, and range of UP is difficult due to inherent uncertainty in sensing, control, and contact physics. This paper explores “ambidextrous” robot grasping, where two or more heterogeneous grippers are used. We present Dexterity Network (Dex-Net) 4.0, a substantial extension to previous versions of Dex-Net that learns policies for a given set of grippers by training on synthetic datasets using domain randomization with analytic models of physics and geometry. We train policies for a parallel-jaw and a vacuum-based suction cup gripper on 5 million synthetic depth images, grasps, and rewards generated from heaps of three-dimensional objects. On a physical robot with two grippers, the Dex-Net 4.0 policy consistently clears bins of up to 25 novel objects with reliability greater than 95\% at a rate of more than 300 mean picks per hour. An ambidextrous grasping policy trained on synthetic datasets consistently clears bins of up to 25 novel objects with a physical robot. An ambidextrous grasping policy trained on synthetic datasets consistently clears bins of up to 25 novel objects with a physical robot.},
  langid = {english},
  keywords = {#nosource,object grasping,robotic grasping,tno internship}
}

@inproceedings{mahlerLearningDeepPolicies2017,
  title = {Learning {{Deep Policies}} for {{Robot Bin Picking}} by {{Simulating Robust Grasping Sequences}}},
  booktitle = {Conference on {{Robot Learning}}},
  author = {Mahler, Jeffrey and Goldberg, Ken},
  date = {2017-10-18},
  pages = {515--524},
  url = {http://proceedings.mlr.press/v78/mahler17a.html},
  urldate = {2019-05-08},
  abstract = {Recent results suggest that it is possible to grasp a variety of singulated objects with high precision using Convolutional Neural Networks (CNNs) trained on synthetic data. This paper considers th...},
  eventtitle = {Conference on {{Robot Learning}}},
  langid = {english},
  keywords = {#nosource,object grasping,reinforcement learning,robotic grasping,tno internship},
  annotation = {00027}
}

@unpublished{mahmoodBenchmarkingReinforcementLearning2018,
  title = {Benchmarking {{Reinforcement Learning Algorithms}} on {{Real-World Robots}}},
  author = {Mahmood, A. Rupam and Korenkevych, Dmytro and Vasan, Gautham and Ma, William and Bergstra, James},
  date = {2018-09-20},
  eprint = {1809.07731},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1809.07731},
  urldate = {2020-07-02},
  abstract = {Through many recent successes in simulation, model-free reinforcement learning has emerged as a promising approach to solving continuous control robotic tasks. The research community is now able to reproduce, analyze and build quickly on these results due to open source implementations of learning algorithms and simulated benchmark tasks. To carry forward these successes to real-world applications, it is crucial to withhold utilizing the unique advantages of simulations that do not transfer to the real world and experiment directly with physical robots. However, reinforcement learning research with physical robots faces substantial resistance due to the lack of benchmark tasks and supporting source code. In this work, we introduce several reinforcement learning tasks with multiple commercially available robots that present varying levels of learning difficulty, setup, and repeatability. On these tasks, we test the learning performance of off-the-shelf implementations of four reinforcement learning algorithms and analyze sensitivity to their hyper-parameters to determine their readiness for applications in various real-world tasks. Our results show that with a careful setup of the task interface and computations, some of these implementations can be readily applicable to physical robots. We find that state-of-the-art learning algorithms are highly sensitive to their hyper-parameters and their relative ordering does not transfer across tasks, indicating the necessity of re-tuning them for each task for best performance. On the other hand, the best hyper-parameter configuration from one task may often result in effective learning on held-out tasks even with different robots, providing a reasonable default. We make the benchmark tasks publicly available to enhance reproducibility in real-world reinforcement learning.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,real-world,reinforcement learning,Statistics - Machine Learning}
}

@online{mamakoukasLearningStableModels2022,
  title = {Learning {{Stable Models}} for {{Prediction}} and {{Control}}},
  author = {Mamakoukas, Giorgos and Abraham, Ian and Murphey, Todd D.},
  date = {2022-03-24},
  eprint = {2005.04291},
  eprinttype = {arxiv},
  eprintclass = {cs, math},
  url = {http://arxiv.org/abs/2005.04291},
  urldate = {2023-01-19},
  abstract = {This paper demonstrates the benefits of imposing stability on data-driven Koopman operators. The data-driven identification of stable Koopman operators (DISKO) is implemented using an algorithm \textbackslash cite\{mamakoukas\_stableLDS2020\} that computes the nearest \textbackslash textit\{stable\} matrix solution to a least-squares reconstruction error. As a first result, we derive a formula that describes the prediction error of Koopman representations for an arbitrary number of time steps, and which shows that stability constraints can improve the predictive accuracy over long horizons. As a second result, we determine formal conditions on basis functions of Koopman operators needed to satisfy the stability properties of an underlying nonlinear system. As a third result, we derive formal conditions for constructing Lyapunov functions for nonlinear systems out of stable data-driven Koopman operators, which we use to verify stabilizing control from data. Lastly, we demonstrate the benefits of DISKO in prediction and control with simulations using a pendulum and a quadrotor and experiments with a pusher-slider system. The paper is complemented with a video: \textbackslash url\{https://sites.google.com/view/learning-stable-koopman\}.},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics,Mathematics - Optimization and Control,READ,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/mamakoukas_et_al_2022_learning_stable_models_for_prediction_and_control.pdf}
}

@inproceedings{manchesterContractionbasedMethodsStable2021,
  title = {Contraction-Based Methods for Stable Identification and Robust Machine Learning: A Tutorial},
  shorttitle = {Contraction-Based Methods for Stable Identification and Robust Machine Learning},
  booktitle = {2021 60th {{IEEE Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Manchester, Ian R. and Revay, Max and Wang, Ruigang},
  date = {2021},
  pages = {2955--2962},
  publisher = {{IEEE}},
  keywords = {READ,REVIEW},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/manchester_et_al_2021_contraction-based_methods_for_stable_identification_and_robust_machine_learning.pdf}
}

@article{manchesterStableDynamicWalking2011,
  title = {Stable Dynamic Walking over Uneven Terrain},
  author = {Manchester, Ian R and Mettin, Uwe and Iida, Fumiya and Tedrake, Russ},
  date = {2011-03-01},
  journaltitle = {The International Journal of Robotics Research},
  shortjournal = {The International Journal of Robotics Research},
  volume = {30},
  number = {3},
  pages = {265--279},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {0278-3649},
  doi = {10.1177/0278364910395339},
  url = {https://doi.org/10.1177/0278364910395339},
  urldate = {2021-04-18},
  abstract = {We propose a constructive control design for stabilization of non-periodic trajectories of underactuated robots. An important example of such a system is an underactuated “dynamic walking” biped robot traversing rough or uneven terrain. The stabilization problem is inherently challenging due to the nonlinearity, open-loop instability, hybrid (impact) dynamics, and target motions which are not known in advance. The proposed technique is to compute a transverse linearization about the desired motion: a linear impulsive system which locally represents “transversal” dynamics about a target trajectory. This system is then exponentially stabilized using a modified receding-horizon control design, providing exponential orbital stability of the target trajectory of the original nonlinear system. The proposed method is experimentally verified using a compass-gait walker: a two-degree-of-freedom biped with hip actuation but pointed stilt-like feet. The technique is, however, very general and can be applied to a wide variety of hybrid nonlinear systems.},
  langid = {english},
  keywords = {#nosource,Dynamics,feedback control,legged robots,nonlinear hybrid systems,rough terrain.,transverse linearization,underactuated robots}
}

@unpublished{manekLearningStableDeep2020,
  title = {Learning {{Stable Deep Dynamics Models}}},
  author = {Manek, Gaurav and Kolter, J. Zico},
  date = {2020-01-16},
  eprint = {2001.06116},
  eprinttype = {arxiv},
  eprintclass = {cs, math, stat},
  url = {http://arxiv.org/abs/2001.06116},
  urldate = {2020-11-13},
  abstract = {Deep networks are commonly used to model dynamical systems, predicting how the state of a system will evolve over time (either autonomously or in response to control inputs). Despite the predictive power of these systems, it has been difficult to make formal claims about the basic properties of the learned systems. In this paper, we propose an approach for learning dynamical systems that are guaranteed to be stable over the entire state space. The approach works by jointly learning a dynamics model and Lyapunov function that guarantees non-expansiveness of the dynamics under the learned Lyapunov function. We show that such learning systems are able to model simple dynamical systems and can be combined with additional deep generative models to learn complex dynamics, such as video textures, in a fully end-to-end fashion.},
  langid = {english},
  keywords = {#nosource,Computer Science - Machine Learning,Mathematics - Dynamical Systems,READ,Statistics - Machine Learning},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/kolter_manek_2019_learning_stable_deep_dynamics_models.pdf}
}

@unpublished{maniaSimpleRandomSearch2018,
  title = {Simple Random Search Provides a Competitive Approach to Reinforcement Learning},
  author = {Mania, Horia and Guy, Aurelia and Recht, Benjamin},
  date = {2018-03-19},
  eprint = {1803.07055},
  eprinttype = {arxiv},
  eprintclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1803.07055},
  urldate = {2020-07-02},
  abstract = {A common belief in model-free reinforcement learning is that methods based on random search in the parameter space of policies exhibit significantly worse sample complexity than those that explore the space of actions. We dispel such beliefs by introducing a random search method for training static, linear policies for continuous control problems, matching state-of-the-art sample efficiency on the benchmark MuJoCo locomotion tasks. Our method also finds a nearly optimal controller for a challenging instance of the Linear Quadratic Regulator, a classical problem in control theory, when the dynamics are not known. Computationally, our random search algorithm is at least 15 times more efficient than the fastest competing model-free methods on these benchmarks. We take advantage of this computational efficiency to evaluate the performance of our method over hundreds of random seeds and many different hyperparameter configurations for each benchmark task. Our simulations highlight a high variability in performance in these benchmark tasks, suggesting that commonly used estimations of sample efficiency do not adequately evaluate the performance of RL algorithms.},
  keywords = {#nosource,benchmarks,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,reinforcement learning,Statistics - Machine Learning},
  annotation = {00150}
}

@unpublished{manuelliKPAMKeyPointAffordances2019,
  title = {{{kPAM}}: {{KeyPoint Affordances}} for {{Category-Level Robotic Manipulation}}},
  shorttitle = {{{kPAM}}},
  author = {Manuelli, Lucas and Gao, Wei and Florence, Peter and Tedrake, Russ},
  date = {2019-03-15},
  eprint = {1903.06684},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1903.06684},
  urldate = {2019-04-17},
  abstract = {We would like robots to achieve purposeful manipulation by placing any instance from a category of objects into a desired set of goal states. Existing manipulation pipelines typically specify the desired configuration as a target 6-DOF pose and rely on explicitly estimating the pose of the manipulated objects. However, representing an object with a parameterized transformation defined on a fixed template cannot capture large intra-category shape variation, and specifying a target pose at a category level can be physically infeasible or fail to accomplish the task – e.g. knowing the pose and size of a coffee mug relative to some canonical mug is not sufficient to successfully hang it on a rack by its handle. Hence we propose a novel formulation of category-level manipulation that uses semantic 3D keypoints as the object representation. This keypoint representation enables a simple and interpretable specification of the manipulation target as geometric costs and constraints on the keypoints, which flexibly generalizes existing pose-based manipulation methods. Using this formulation, we factor the manipulation policy into instance segmentation, 3D keypoint detection, optimizationbased robot action planning and local dense-geometry-based action execution. This factorization allows us to leverage advances in these sub-problems and combine them into a general and effective perception-to-action manipulation pipeline. Our pipeline is robust to large intra-category shape variation and topology changes as the keypoint representation ignores taskirrelevant geometric details. Extensive hardware experiments demonstrate our method can reliably accomplish tasks with never-before seen objects in a category, such as placing shoes and mugs with significant shape variation into category level target configurations. The video demo and source code are available on our project page.},
  langid = {english},
  keywords = {#nosource,Computer Science - Robotics,object manipulation,robotic grasping,tno internship}
}

@unpublished{martin-martinVariableImpedanceControl2019,
  title = {Variable {{Impedance Control}} in {{End-Effector Space}}: {{An Action Space}} for {{Reinforcement Learning}} in {{Contact-Rich Tasks}}},
  shorttitle = {Variable {{Impedance Control}} in {{End-Effector Space}}},
  author = {Martín-Martín, Roberto and Lee, Michelle A. and Gardner, Rachel and Savarese, Silvio and Bohg, Jeannette and Garg, Animesh},
  date = {2019-08-02},
  eprint = {1906.08880},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1906.08880},
  urldate = {2022-04-08},
  abstract = {Reinforcement Learning (RL) of contact-rich manipulation tasks has yielded impressive results in recent years. While many studies in RL focus on varying the observation space or reward model, few efforts focused on the choice of action space (e.g. joint or end-effector space, position, velocity, etc.). However, studies in robot motion control indicate that choosing an action space that conforms to the characteristics of the task can simplify exploration and improve robustness to disturbances. This paper studies the effect of different action spaces in deep RL and advocates for Variable Impedance Control in End-effector Space (VICES) as an advantageous action space for constrained and contact-rich tasks. We evaluate multiple action spaces on three prototypical manipulation tasks: Path Following (task with no contact), Door Opening (task with kinematic constraints), and Surface Wiping (task with continuous contact). We show that VICES improves sample efficiency, maintains low energy consumption, and ensures safety across all three experimental setups. Further, RL policies learned with VICES can transfer across different robot models in simulation, and from simulation to real for the same robot. Further information is available at https://stanfordvl.github.io/vices.},
  keywords = {Cartesian impedance,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Joint impedance,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/martín-martín_et_al_2019_variable_impedance_control_in_end-effector_space.pdf}
}

@misc{mathewOnlineLearningFeedforward2019,
  title = {Online Learning of Feed-Forward Models for Variable Impedance Control in Manipulation Tasks},
  author = {Mathew, Michael J. and Sidhik, Saif and Sridharan, Mohan and Azad, Morteza and Hayashi, Akinobu and Wyatt, Jeremy},
  date = {2019},
  organization = {{Jul}},
  keywords = {READ,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/mathew_et_al_2019_online_learning_of_feed-forward_models_for_variable_impedance_control_in.pdf}
}

@article{matsubaraLearningParametricDynamic2011,
  title = {Learning Parametric Dynamic Movement Primitives from Multiple Demonstrations},
  author = {Matsubara, Takamitsu and Hyon, Sang-Ho and Morimoto, Jun},
  date = {2011-06-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {24},
  number = {5},
  pages = {493--500},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2011.02.004},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608011000566},
  urldate = {2022-09-11},
  abstract = {Learning from demonstration has shown to be a suitable approach for learning control policies (CPs). However, most previous studies learn CPs from a single demonstration, which results in limited scalability and insufficient generalization toward a wide range of applications in real environments. This paper proposes a novel approach to learn highly scalable CPs of basis movement skills from multiple demonstrations. In contrast to conventional studies with a single demonstration, i.e., dynamic movement primitives (DMPs), our approach efficiently encodes multiple demonstrations by shaping a parametric-attractor landscape in a set of differential equations. Assuming a certain similarity among multiple demonstrations, our approach learns the parametric-attractor landscape by extracting a small number of common factors in multiple demonstrations. The learned CPs allow the synthesis of novel movements with novel motion styles by specifying the linear coefficients of the bases as parameter vectors without losing useful properties of the DMPs, such as stability and robustness against perturbations. For both discrete and rhythmic movement skills, we present a unified learning procedure for learning a parametric-attractor landscape from multiple demonstrations. The feasibility and highly extended scalability of DMPs are demonstrated on an actual dual-arm robot.},
  langid = {english},
  keywords = {Imitation learning,Motion styles,Movement primitives,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/matsubara_et_al_2011_learning_parametric_dynamic_movement_primitives_from_multiple_demonstrations.pdf}
}

@inproceedings{mattnerLearnSwingBalance2012,
  title = {Learn to {{Swing Up}} and {{Balance}} a {{Real Pole Based}} on {{Raw Visual Input Data}}},
  booktitle = {Neural {{Information Processing}}},
  author = {Mattner, Jan and Lange, Sascha and Riedmiller, Martin},
  editor = {Huang, Tingwen and Zeng, Zhigang and Li, Chuandong and Leung, Chi Sing},
  date = {2012},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {126--133},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-34500-5_16},
  abstract = {For the challenging pole balancing task we propose a system which uses raw visual input data for reinforcement learning to evolve a control strategy. Therefore we use a neural network – a deep autoencoder – to encode the camera images and thus the system states in a low dimensional feature space. The system is compared to controllers that work directly on the motor sensor data. We show that the performances of both systems are settled in the same order of magnitude.},
  isbn = {978-3-642-34500-5},
  langid = {english},
  keywords = {#nosource,Deep autoencoder,Neural network,Pole balancing,Reinforcement learning,Visual input}
}

@book{mauroyKoopmanOperatorSystems2020,
  title = {Koopman Operator in Systems and Control},
  author = {Mauroy, Alexandre and Susuki, Y. and Mezić, I.},
  date = {2020},
  publisher = {{Springer}},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/mauroy_et_al_2020_koopman_operator_in_systems_and_control.pdf}
}

@inproceedings{maVariableStiffnessDamping2019,
  title = {Variable {{Stiffness}} and {{Damping Impedance Control}} with {{Disturbance Observer}}},
  booktitle = {2019 7th {{International Conference}} on {{Control}}, {{Mechatronics}} and {{Automation}} ({{ICCMA}})},
  author = {Ma, Zhiqiang and Dong, Gangqi},
  date = {2019-11},
  pages = {33--37},
  doi = {10.1109/ICCMA46720.2019.8988686},
  abstract = {This paper proposes a variable stiffness and damping impedance controller with a new closed-loop impedance architecture containing the negative power type stiffness and damping parameters. The sliding mode control technique is employed to construct an asymptotically stable reduced order system using PID sliding surface. The external disturbance is well considered, which easily propagate to the preset impedance architecture to degenerate the impedance performance. To address this problem, a disturbance observer is introduced into the control system to reduce the adverse effect by partially eliminating the disturbance. The proposed impedance architecture owns better transient performance compared with the trivial spring-damper impedance architecture, which is well illustrated by numerical results.},
  eventtitle = {2019 7th {{International Conference}} on {{Control}}, {{Mechatronics}} and {{Automation}} ({{ICCMA}})},
  keywords = {component,disturbance observer,impedance control,READ,sliding mode control,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ma_dong_2019_variable_stiffness_and_damping_impedance_control_with_disturbance_observer.pdf}
}

@inproceedings{medinaLearningStableTask2017,
  title = {Learning {{Stable Task Sequences}} from {{Demonstration}} with {{Linear Parameter Varying Systems}} and {{Hidden Markov Models}}},
  booktitle = {Proceedings of the 1st {{Annual Conference}} on {{Robot Learning}}},
  author = {Medina, Jose R. and Billard, Aude},
  date = {2017-10-18},
  pages = {175--184},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v78/medina17a.html},
  urldate = {2022-12-07},
  abstract = {The problem of acquiring multiple tasks from demonstration is typically divided in two sequential processes: (1) the segmentation or identification of different subgoals/subtasks and (2) a separate learning process that parameterizes a control policy for each subtask. As a result, segmentation criteria typically neglect the characteristics of control policies and rely instead on simplified models. This paper aims for a single model capable of learning sequences of complex time-independent control policies that provide robust and stable behavior. To this end, we first present a novel and efficient approach to learn goal-oriented time-independent motion models by estimating~\textbackslash emphboth attractor and dynamic behavior from data guaranteeing stability using linear parameter varying~(LPV) systems.  This method enables learning complex task sequences with hidden Markov models~(HMMs), where each state/subtask is given by a stable LPV system and where transitions are most likely around the corresponding attractor. We study the dynamics of the HMM-LPV model and propose a motion generation method that guarantees the stability of task sequences. We validate our approach in two sets of demonstrated human motions.},
  eventtitle = {Conference on {{Robot Learning}}},
  langid = {english},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/medina_billard_2017_learning_stable_task_sequences_from_demonstration_with_linear_parameter_varying.pdf}
}

@inproceedings{medinaRisksensitiveInteractionControl2013,
  title = {Risk-Sensitive Interaction Control in Uncertain Manipulation Tasks},
  booktitle = {2013 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Medina, José Ramón and Sieber, Dominik and Hirche, Sandra},
  date = {2013-05},
  pages = {502--507},
  issn = {1050-4729},
  doi = {10.1109/ICRA.2013.6630621},
  abstract = {Manipulation tasks are a great challenge for robots due to the uncertainty arising from unstructured environments. In this paper we propose a novel control scheme for contact tasks based on risk-sensitive optimal feedback control. It provides a systematic approach to adjust the trade-off between motion and force control under uncertainty. Following a previously acquired task model, the proposed approach provides both a variable stiffness solution and a motion reference adaptation. This control scheme achieves increased adaptability under previously unseen environmental variability. An implementation on a robotic manipulator validates the applicability and adaptability of the proposed control approach in two different manipulation tasks.},
  eventtitle = {2013 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  keywords = {Adaptation models,Dynamics,Force,Force control,Noise,Robots,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/medina_et_al_2013_risk-sensitive_interaction_control_in_uncertain_manipulation_tasks.pdf}
}

@inproceedings{mengPathFollowingTerminal2020,
  title = {Path Following and Terminal Force Control of Robotic Manipulators},
  booktitle = {2020 {{IEEE}} 16th {{International Conference}} on {{Control}} \& {{Automation}} ({{ICCA}})},
  author = {Meng, Lingyu and Yu, Shuyou and Chang, Huan and Findeisen, Rolf and Chen, Hong},
  date = {2020-10},
  pages = {1482--1487},
  issn = {1948-3457},
  doi = {10.1109/ICCA51439.2020.9264313},
  abstract = {In this paper, a nonlinear model predictive control (NMPC) scheme is proposed for the path following and terminal force control of manipulators, where not only the precise following of a predefined geometric path but also force regulation are considered. A virtual dynamics of force is added to the involved optimization problem of NMPC which represents the dynamic relationship between the end position and the contact force. Furthermore, an adaptive impedance control is proposed for changes of environmental stiffness. The simulation results show that the manipulator with the proposed scheme can track both the contact force and the path with high precision, and can adapt to the environmental stiffness changes.},
  eventtitle = {2020 {{IEEE}} 16th {{International Conference}} on {{Control}} \& {{Automation}} ({{ICCA}})},
  keywords = {Admittance,Impedance,Manipulator dynamics,Position control,Predictive control,Service robots,Steady-state},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/meng_et_al_2020_path_following_and_terminal_force_control_of_robotic_manipulators.pdf}
}

@article{meszarosLearningPickNonZeroVelocity2022,
  title = {Learning to {{Pick}} at {{Non-Zero-Velocity From Interactive Demonstrations}}},
  author = {Mészáros, Anna and Franzese, Giovanni and Kober, Jens},
  date = {2022-07},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {3},
  pages = {6052--6059},
  issn = {2377-3766},
  doi = {10.1109/LRA.2022.3165531},
  abstract = {This work investigates how the intricate task of a continuous pick \& place (P\&P) motion may be learned from humans based on demonstrations and corrections. Due to the complexity of the task, these demonstrations are often slow and even slightly flawed, particularly at moments when multiple aspects (i.e., end-effector movement, orientation, and gripper width) have to be demonstrated at once. Rather than training a person to give better demonstrations, non-expert users are provided with the ability to interactively modify the dynamics of their initial demonstration through teleoperated corrective feedback. This in turn allows them to teach motions outside of their own physical capabilities. In the end, the goal is to obtain a faster but reliable execution of the task. The presented framework learns the desired movement dynamics based on the current Cartesian position with Gaussian Processes (GPs), resulting in a reactive, time-invariant policy. Using GPs also allows online interactive corrections and active disturbance rejection through epistemic uncertainty minimization. The experimental evaluation of the framework is carried out on a Franka-Emika Panda. Tests were performed to determine i) the framework’s effectiveness in successfully learning how to quickly pick \& place an object, ii) ease of policy correction to environmental changes (i.e., different object sizes and mass), and iii) the framework’s usability for non-expert users.},
  eventtitle = {{{IEEE Robotics}} and {{Automation Letters}}},
  keywords = {Compliance and impedance control,Dynamical systems,Dynamics,imitation learning,Impedance,incremental learning,Robots,Task analysis,Trajectory,Uncertainty},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/mészáros_et_al_2022_learning_to_pick_at_non-zero-velocity_from_interactive_demonstrations.pdf}
}

@article{michelBilateralTeleoperationAdaptive2021,
  title = {Bilateral {{Teleoperation With Adaptive Impedance Control}} for {{Contact Tasks}}},
  author = {Michel, Youssef and Rahal, Rahaf and Pacchierotti, Claudio and Giordano, Paolo Robuffo and Lee, Dongheui},
  date = {2021-07},
  journaltitle = {Ieee Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {6},
  number = {3},
  pages = {5429--5436},
  publisher = {{Ieee-Inst Electrical Electronics Engineers Inc}},
  location = {{Piscataway}},
  issn = {2377-3766},
  doi = {10.1109/LRA.2021.3066974},
  url = {https://ieeexplore.ieee.org/document/9380915},
  urldate = {2022-04-29},
  abstract = {This letter presents an adaptive impedance control architecture for robotic teleoperation of contact tasks featuring continuous interaction with the environment. We use Learning from Demonstration (LID) as a framework to learn variable stiffness control policies. Then, the learnt state-varying stiffness is used to command the remote manipulator, so as to adapt its interaction with the environment based on the sensed forces. Our system only relies on the on-board torque sensors of a commercial robotic manipulator and it does not require any additional hardware or user input for the estimation of the required stiffness. We also provide a passivity analysis of our system, where the concept of energy tanks is used to guarantee a stable behavior. Finally, the system is evaluated in a representative teleoperated cutting application. Results show that the proposed variable-stiffness approach outperforms two standard constant-stiffness approaches in terms of safety and robot tracking performance.},
  langid = {english},
  keywords = {Compliance and impedance control,force,learning from demonstration,STABILITY,telerobotics and teleoperation,variable impedance},
  annotation = {WOS:000649582600011},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/michel_et_al_2021_bilateral_teleoperation_with_adaptive_impedance_control_for_contact_tasks.pdf}
}

@article{michelPassivitybasedVariableImpedance2020,
  title = {Passivity-Based Variable Impedance Control for Redundant Manipulators},
  author = {Michel, Youssef and Ott, Christian and Lee, Dongheui},
  date = {2020-01-01},
  journaltitle = {IFAC-PapersOnLine},
  shortjournal = {IFAC-PapersOnLine},
  series = {21st {{IFAC World Congress}}},
  volume = {53},
  number = {2},
  pages = {9865--9872},
  issn = {2405-8963},
  doi = {10.1016/j.ifacol.2020.12.2692},
  url = {https://www.sciencedirect.com/science/article/pii/S2405896320334546},
  urldate = {2022-05-02},
  abstract = {Kinematic redundancy significantly improves the dexterity and flexibility of robotic manipulators. The redundant degrees of freedom can be exploited to fulfill additional tasks that can be executed without disturbing the primary task. In this work, we investigate how a time varying impedance behavior can be embedded into redundant manipulators where it is desired to achieve such a behavior both for the primary and null space tasks. A passivity based controller is developed, relying on the concept of energy tanks which are filled by the dissipated power in the system, and compensate for non-passive control actions. This guarantees that the system remains passive, which ensures stable interactions with any passive environment. The method is validated in simulations where the interactive behavior of the main and null space tasks is specified by a time varying stiffness profile.},
  langid = {english},
  keywords = {Energy tanks,Passivity-based control,READ,Redundant manipulators,Robot control,STABILITY,Variable Impedance control},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/michel_et_al_2020_passivity-based_variable_impedance_control_for_redundant_manipulators.pdf}
}

@article{michelSafetyAwareHierarchicalPassivityBased2022,
  title = {Safety-{{Aware Hierarchical Passivity-Based Variable Compliance Control}} for {{Redundant Manipulators}}},
  author = {Michel, Youssef and Ott, Christian and Lee, Dongheui},
  date = {2022},
  journaltitle = {IEEE Transactions on Robotics},
  pages = {1--18},
  issn = {1941-0468},
  doi = {10.1109/TRO.2022.3174478},
  abstract = {This article presents a hierarchical passivity-based compliance controller that exploits robot redundancy and aims at achieving an impedance behavior with a time-varying stiffness on all the priority levels. Unfortunately, this gives rise to certain control actions that lead to the loss of the safety-critical passivity feature. To deal with this problem, we employ the concept of virtual energy tanks that keep track of the passivity violating energy in the system, ensuring that it remains bounded. This restores the passivity in the system, which guarantees the stable interaction with any passive environment. Furthermore, we augment our controller with an additional safety layer, which ensures that the energy injected through the tank into the system remains below a safe limit, defined based on the maximum kinetic energy allowed in the system. Finally, our approach is validated in terms of performance during task execution and safety both in simulations and on real-robot hardware.},
  eventtitle = {{{IEEE Transactions}} on {{Robotics}}},
  keywords = {Behavioral sciences,Compliance and impedance control,Impedance,Kinetic energy,passivity-based control,READ,redundant robots,Robot kinematics,robot safety,Robots,Safety,Task analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/michel_et_al_2022_safety-aware_hierarchical_passivity-based_variable_compliance_control_for2.pdf}
}

@inproceedings{mishraSimpleNeuralAttentive2018,
  title = {A {{Simple Neural Attentive Meta-Learner}}},
  author = {Mishra, Nikhil and Rohaninejad, Mostafa and Chen, Xi and Abbeel, Pieter},
  date = {2018-02-15},
  url = {https://openreview.net/forum?id=B1DmUzWAW&noteId=B1DmUzWAW},
  urldate = {2020-07-02},
  abstract = {Deep neural networks excel in regimes with large amounts of data, but tend to struggle when data is scarce or when they need to adapt quickly to changes in the task. In response, recent work in...},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  keywords = {#nosource,meta-RL,reinforcement learning}
}

@inproceedings{mitsioniSafeDataDrivenContactRich2021,
  title = {Safe {{Data-Driven Contact-Rich Manipulation}}},
  booktitle = {2020 {{IEEE-RAS}} 20th {{International Conference}} on {{Humanoid Robots}} ({{Humanoids}})},
  author = {Mitsioni, Ioanna and Tajvar, Pouria and Kragic, Danica and Tumova, Jana and Pek, Christian},
  date = {2021-07},
  pages = {120--127},
  issn = {2164-0580},
  doi = {10.1109/HUMANOIDS47582.2021.9555680},
  abstract = {In this paper, we address the safety of data-driven control for contact-rich manipulation. We propose to restrict the controller’s action space to keep the system in a set of safe states. In the absence of an analytical model, we show how Gaussian Processes (GP) can be used to approximate safe sets. We disable inputs for which the predicted states are likely to be unsafe using the GP. Furthermore, we show how locally designed feedback controllers can be used to improve the execution precision in the presence of modelling errors. We demonstrate the benefits of our method on a pushing task with a variety of dynamics, by using known and unknown surfaces and different object loads. Our results illustrate that the proposed approach significantly improves the performance and safety of the baseline controller.},
  eventtitle = {2020 {{IEEE-RAS}} 20th {{International Conference}} on {{Humanoid Robots}} ({{Humanoids}})},
  keywords = {Aerospace electronics,Analytical models,Closed loop systems,Gaussian processes,Humanoid robots,Predictive models,Safety},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/mitsioni_et_al_2021_safe_data-driven_contact-rich_manipulation.pdf}
}

@unpublished{mnihAsynchronousMethodsDeep2016,
  title = {Asynchronous {{Methods}} for {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  date = {2016-06-16},
  eprint = {1602.01783},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1602.01783},
  urldate = {2020-07-02},
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  keywords = {#nosource,Computer Science - Machine Learning,model-free,policy gradients,policy optimization,reinforcement learning},
  annotation = {03426}
}

@article{mnihPlayingAtariDeep,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  pages = {9},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  langid = {english},
  keywords = {#nosource,DQL,model-free,model-free q-learning,q-learning,reinforcement learning},
  annotation = {04250}
}

@article{mohammadkhansari-zadehLearningControlLyapunov2014,
  title = {Learning Control {{Lyapunov}} Function to Ensure Stability of Dynamical System-Based Robot Reaching Motions},
  author = {Mohammad Khansari-Zadeh, S. and Billard, Aude},
  date = {2014-06-01},
  journaltitle = {Robotics and Autonomous Systems},
  shortjournal = {Robotics and Autonomous Systems},
  volume = {62},
  number = {6},
  pages = {752--765},
  issn = {0921-8890},
  doi = {10.1016/j.robot.2014.03.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0921889014000372},
  urldate = {2023-02-27},
  abstract = {We consider an imitation learning approach to model robot point-to-point (also known as discrete or reaching) movements with a set of autonomous Dynamical Systems (DS). Each DS model codes a behavior (such as reaching for a cup and swinging a golf club) at the kinematic level. An estimate of these DS models are usually obtained from a set of demonstrations of the task. When modeling robot discrete motions with DS, ensuring stability of the learned DS is a key requirement to provide a useful policy. In this paper we propose an imitation learning approach that exploits the power of Control Lyapunov Function (CLF) control scheme to ensure global asymptotic stability of nonlinear DS. Given a set of demonstrations of a task, our approach proceeds in three steps: (1) Learning a valid Lyapunov function from the demonstrations by solving a constrained optimization problem, (2) Using one of the-state-of-the-art regression techniques to model an (unstable) estimate of the motion from the demonstrations, and (3) Using (1) to ensure stability of (2) during the task execution via solving a constrained convex optimization problem. The proposed approach allows learning a larger set of robot motions compared to existing methods that are based on quadratic Lyapunov functions. Additionally, by using the CLF formalism, the problem of ensuring stability of DS motions becomes independent from the choice of regression method. Hence it allows the user to adopt the most appropriate technique based on the requirements of the task at hand without compromising stability. We evaluate our approach both in simulation and on the 7 degrees of freedom Barrett WAM arm.},
  langid = {english},
  keywords = {adaptation,Control Lyapunov   function,Control Lyapunov function,Imitation learning,IMPORTANT,model,Movement   primitives,Movement primitives,Nonlinear dynamical systems,obstacle avoidance,point,READ,reproduction,Robot point-to-point movements,STABILITY,Stability analysis,stabilization,task},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/mohammad_khansari-zadeh_billard_2014_learning_control_lyapunov_function_to_ensure_stability_of_dynamical3.pdf}
}

@unpublished{morrisonClosingLoopRobotic2018,
  title = {Closing the {{Loop}} for {{Robotic Grasping}}: {{A Real-time}}, {{Generative Grasp Synthesis Approach}}},
  shorttitle = {Closing the {{Loop}} for {{Robotic Grasping}}},
  author = {Morrison, Douglas and Corke, Peter and Leitner, Jürgen},
  date = {2018-04-14},
  eprint = {1804.05172},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1804.05172},
  urldate = {2019-05-06},
  abstract = {This paper presents a real-time, object-independent grasp synthesis method which can be used for closed-loop grasping. Our proposed Generative Grasping Convolutional Neural Network (GG-CNN) predicts the quality and pose of grasps at every pixel. This one-to-one mapping from a depth image overcomes limitations of current deep-learning grasping techniques by avoiding discrete sampling of grasp candidates and long computation times. Additionally, our GG-CNN is orders of magnitude smaller while detecting stable grasps with equivalent performance to current state-of-the-art techniques. The lightweight and single-pass generative nature of our GG-CNN allows for closed-loop control at up to 50Hz, enabling accurate grasping in non-static environments where objects move and in the presence of robot control inaccuracies. In our real-world tests, we achieve an 83\% grasp success rate on a set of previously unseen objects with adversarial geometry and 88\% on a set of household objects that are moved during the grasp attempt. We also achieve 81\% accuracy when grasping in dynamic clutter.},
  langid = {english},
  keywords = {#nosource,Computer Science - Robotics,object grasping,reinforcement learning,robotic grasping,tno internship},
  annotation = {00021}
}

@online{mowerROSPyBulletInterfaceFramework2022,
  title = {{{ROS-PyBullet Interface}}: {{A Framework}} for {{Reliable Contact Simulation}} and {{Human-Robot Interaction}}},
  shorttitle = {{{ROS-PyBullet Interface}}},
  author = {Mower, Christopher E. and Stouraitis, Theodoros and Moura, João and Rauch, Christian and Yan, Lei and Behabadi, Nazanin Zamani and Gienger, Michael and Vercauteren, Tom and Bergeles, Christos and Vijayakumar, Sethu},
  date = {2022-10-13},
  eprint = {2210.06887},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.06887},
  url = {http://arxiv.org/abs/2210.06887},
  urldate = {2022-10-31},
  abstract = {Reliable contact simulation plays a key role in the development of (semi-)autonomous robots, especially when dealing with contact-rich manipulation scenarios, an active robotics research topic. Besides simulation, components such as sensing, perception, data collection, robot hardware control, human interfaces, etc. are all key enablers towards applying machine learning algorithms or model-based approaches in real world systems. However, there is a lack of software connecting reliable contact simulation with the larger robotics ecosystem (i.e. ROS, Orocos), for a more seamless application of novel approaches, found in the literature, to existing robotic hardware. In this paper, we present the ROS-PyBullet Interface, a framework that provides a bridge between the reliable contact/impact simulator PyBullet and the Robot Operating System (ROS). Furthermore, we provide additional utilities for facilitating Human-Robot Interaction (HRI) in the simulated environment. We also present several use-cases that highlight the capabilities and usefulness of our framework. Please check our video, source code, and examples included in the supplementary material. Our full code base is open source and can be found at https://github.com/cmower/ros\_pybullet\_interface.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/ricks/Zotero/storage/W8KDW4AU/Mower et al. - 2022 - ROS-PyBullet Interface A Framework for Reliable C.pdf}
}

@article{mullenLectureNonlinearSystems,
  title = {Lecture 4: ({{Nonlinear Systems}} and {{Lyapunov Stability}})},
  author = {Mullen, William and Tripathi, Gyanendra},
  pages = {8},
  langid = {english},
  keywords = {IMPORTANT},
  file = {/home/ricks/Zotero/storage/G7FGDCIU/ee106b_lecture4.pdf}
}

@book{murrayMathematicalIntroductionRobotic2017,
  title = {A Mathematical Introduction to Robotic Manipulation},
  author = {Murray, Richard M. and Li, Zexiang and Sastry, S. Shankar},
  date = {2017},
  publisher = {{CRC press}},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/murray_et_al_2017_a_mathematical_introduction_to_robotic_manipulation.pdf}
}

@unpublished{nachumBridgingGapValue2017,
  title = {Bridging the {{Gap Between Value}} and {{Policy Based Reinforcement Learning}}},
  author = {Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale},
  date = {2017-11-22},
  eprint = {1702.08892},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1702.08892},
  urldate = {2020-07-02},
  abstract = {We establish a new connection between value and policy based reinforcement learning (RL) based on a relationship between softmax temporal value consistency and policy optimality under entropy regularization. Specifically, we show that softmax consistent action values correspond to optimal entropy regularized policy probabilities along any action sequence, regardless of provenance. From this observation, we develop a new RL algorithm, Path Consistency Learning (PCL), that minimizes a notion of soft consistency error along multi-step action sequences extracted from both on- and off-policy traces. We examine the behavior of PCL in different scenarios and show that PCL can be interpreted as generalizing both actor-critic and Q-learning algorithms. We subsequently deepen the relationship by showing how a single model can be used to represent both a policy and the corresponding softmax state values, eliminating the need for a separate critic. The experimental evaluation demonstrates that PCL significantly outperforms strong actor-critic and Q-learning baselines across several benchmarks.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,model-free,path-consistency learning,reinforcement learning,Statistics - Machine Learning}
}

@unpublished{nachumDataEfficientHierarchicalReinforcement2018,
  title = {Data-{{Efficient Hierarchical Reinforcement Learning}}},
  author = {Nachum, Ofir and Gu, Shixiang and Lee, Honglak and Levine, Sergey},
  date = {2018-10-05},
  eprint = {1805.08296},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1805.08296},
  urldate = {2020-07-02},
  abstract = {Hierarchical reinforcement learning (HRL) is a promising approach to extend traditional reinforcement learning (RL) methods to solve more complex tasks. Yet, the majority of current HRL methods require careful task-specific design and on-policy training, making them difficult to apply in real-world scenarios. In this paper, we study how we can develop HRL algorithms that are general, in that they do not make onerous additional assumptions beyond standard RL algorithms, and efficient, in the sense that they can be used with modest numbers of interaction samples, making them suitable for real-world problems such as robotic control. For generality, we develop a scheme where lower-level controllers are supervised with goals that are learned and proposed automatically by the higher-level controllers. To address efficiency, we propose to use off-policy experience for both higher and lower-level training. This poses a considerable challenge, since changes to the lower-level behaviors change the action space for the higher-level policy, and we introduce an off-policy correction to remedy this challenge. This allows us to take advantage of recent advances in off-policy model-free RL to learn both higher- and lower-level policies using substantially fewer environment interactions than on-policy algorithms. We term the resulting HRL agent HIRO and find that it is generally applicable and highly sample-efficient. Our experiments show that HIRO can be used to learn highly complex behaviors for simulated robots, such as pushing objects and utilizing them to reach target locations, learning from only a few million samples, equivalent to a few days of real-time interaction. In comparisons with a number of prior HRL methods, we find that our approach substantially outperforms previous state-of-the-art techniques.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,hierarchical RL,reinforcement learning,Statistics - Machine Learning},
  annotation = {00149}
}

@unpublished{nachumTrustPCLOffPolicyTrust2018,
  title = {Trust-{{PCL}}: {{An Off-Policy Trust Region Method}} for {{Continuous Control}}},
  shorttitle = {Trust-{{PCL}}},
  author = {Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale},
  date = {2018-02-22},
  eprint = {1707.01891},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1707.01891},
  urldate = {2020-07-02},
  abstract = {Trust region methods, such as TRPO, are often used to stabilize policy optimization algorithms in reinforcement learning (RL). While current trust region strategies are effective for continuous control, they typically require a prohibitively large amount of on-policy interaction with the environment. To address this problem, we propose an off-policy trust region method, Trust-PCL. The algorithm is the result of observing that the optimal policy and state values of a maximum reward objective with a relative-entropy regularizer satisfy a set of multi-step pathwise consistencies along any path. Thus, Trust-PCL is able to maintain optimization stability while exploiting off-policy data to improve sample efficiency. When evaluated on a number of continuous control tasks, Trust-PCL improves the solution quality and sample efficiency of TRPO.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,model-free,path-consistency learning,reinforcement learning}
}

@article{nagabandiLEARNINGADAPTDYNAMIC2019,
  title = {{{LEARNING TO ADAPT IN DYNAMIC}}, {{REAL-WORLD ENVIRONMENTS THROUGH META-REINFORCEMENT LEARNING}}},
  author = {Nagabandi, Anusha and Clavera, Ignasi and Liu, Simin and Fearing, Ronald S and Abbeel, Pieter and Levine, Sergey and Finn, Chelsea},
  date = {2019},
  pages = {17},
  langid = {english},
  keywords = {#nosource,machine learning control}
}

@unpublished{nagabandiNeuralNetworkDynamics2017,
  title = {Neural {{Network Dynamics}} for {{Model-Based Deep Reinforcement Learning}} with {{Model-Free Fine-Tuning}}},
  author = {Nagabandi, Anusha and Kahn, Gregory and Fearing, Ronald S. and Levine, Sergey},
  date = {2017-08-08},
  eprint = {1708.02596},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1708.02596},
  urldate = {2019-04-18},
  abstract = {Model-free deep reinforcement learning algorithms have been shown to be capable of learning a wide range of robotic skills, but typically require a very large number of samples to achieve good performance. Model-based algorithms, in principle, can provide for much more efficient learning, but have proven difficult to extend to expressive, high-capacity models such as deep neural networks. In this work, we demonstrate that medium-sized neural network models can in fact be combined with model predictive control (MPC) to achieve excellent sample complexity in a model-based reinforcement learning algorithm, producing stable and plausible gaits to accomplish various complex locomotion tasks. We also propose using deep neural network dynamics models to initialize a model-free learner, in order to combine the sample efficiency of model-based approaches with the high task-specific performance of model-free methods. We empirically demonstrate on MuJoCo locomotion tasks that our pure model-based approach trained on just random action data can follow arbitrary trajectories with excellent sample efficiency, and that our hybrid algorithm can accelerate model-free learning on high-speed benchmark tasks, achieving sample efficiency gains of 3-5x on swimmer, cheetah, hopper, and ant agents. Videos can be found at https://sites.google.com/view/mbmf},
  langid = {english},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,learned model,machine learning control,model-based,reinforcement learning}
}

@article{najafiUsingPotentialField2020,
  title = {Using {{Potential Field Function With}} a {{Velocity Field Controller}} to {{Learn}} and {{Reproduce}} the {{Therapist}}'s {{Assistance}} in {{Robot-Assisted Rehabilitation}}},
  author = {Najafi, Mohammad and Rossa, Carlos and Adams, Kim and Tavakoli, Mahdi},
  date = {2020-06},
  journaltitle = {IEEE/ASME Transactions on Mechatronics},
  volume = {25},
  number = {3},
  pages = {1622--1633},
  issn = {1941-014X},
  doi = {10.1109/TMECH.2020.2981625},
  abstract = {Rehabilitative and assistive practices usually elicit intense and repetitive exercises. Thus, there has been an increasing interest in robotic systems as they are robust and cost-effective in comparison to conventional physical motor-therapy with a therapist. These robots have applications in therapeutic and in-home environments, where there is a necessity for a user-friendly procedure to program the robots for a specific task easily. Our group has suggested robot learning from demonstration (LfD) as an intuitive procedure to program robots via short-term physical interaction in rehabilitation and assistive applications. In this article, a therapist assists a patient, and cooperatively performs a task on a robotic manipulator. Then, using a nonparametric potential field function, the therapist's motion, and interaction force (assistance/resistance) is modeled time-independently via a convex optimization algorithm. Next, in the therapist's absence, the robot provides the patient with the same level of interaction force provided by the therapist along the trajectory. A velocity field controller is also designed to compensate and regulate the patient's deviation from the velocity observed in the demonstration phase. Finally, the efficacy, advantages, and stability of the proposed framework are evaluated in three different experimental scenarios involving spring arrays and an individual with cerebral palsy (CP).},
  eventtitle = {{{IEEE}}/{{ASME Transactions}} on {{Mechatronics}}},
  keywords = {Assistive technology,End effectors,Force,human-robot interaction,motion control,READ,rehabilitation robotics,Robot kinematics,robot learning,STABILITY,Task analysis,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/najafi_et_al_2020_using_potential_field_function_with_a_velocity_field_controller_to_learn_and2.pdf}
}

@inproceedings{nemecTaskAdaptationExploration2009,
  title = {Task Adaptation through Exploration and Action Sequencing},
  booktitle = {2009 9th {{IEEE-RAS International Conference}} on {{Humanoid Robots}}},
  author = {Nemec, Bojan and Tamošiūnaitė, Minija and Wörgötter, Florentin and Ude, Aleš},
  date = {2009-12},
  pages = {610--616},
  issn = {2164-0580},
  doi = {10.1109/ICHR.2009.5379568},
  abstract = {General-purpose autonomous robots need to have the ability to sequence and adapt the available sensorimotor knowledge, which is often given in the form of movement primitives. In order to solve a given task in situations that were not considered during the initial learning, it is necessary to adapt trajectories contained in the library of primitive motions to new situations. In this paper we explore how to apply reinforcement learning to modify the subgoals of primitive movements involved in the given task. As the underlying sensorimotor representation we selected nonlinear dynamic systems, which provide a powerful machinery for the modification of motion trajectories. We propose a new formulation for dynamic systems, which ensures that consecutive primitive movements can be splined together in a continuous way (up to second order derivatives).},
  eventtitle = {2009 9th {{IEEE-RAS International Conference}} on {{Humanoid Robots}}},
  keywords = {Educational robots,Filling,Glass,Humanoid robots,Humans,Learning,Liquids,READ,Robot sensing systems,Shape,Space exploration},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/nemec_et_al_2009_task_adaptation_through_exploration_and_action_sequencing.pdf}
}

@article{neumannLearningRobotMotions2015,
  title = {Learning Robot Motions with Stable Dynamical Systems under Diffeomorphic Transformations},
  author = {Neumann, Klaus and Steil, Jochen J.},
  date = {2015-08-01},
  journaltitle = {Robotics and Autonomous Systems},
  shortjournal = {Robotics and Autonomous Systems},
  volume = {70},
  pages = {1--15},
  issn = {0921-8890},
  doi = {10.1016/j.robot.2015.04.006},
  url = {https://www.sciencedirect.com/science/article/pii/S0921889015000883},
  urldate = {2022-09-09},
  abstract = {Accuracy and stability have in recent studies been emphasized as the two major ingredients to learn robot motions from demonstrations with dynamical systems. Several approaches yield stable dynamical systems but are also limited to specific dynamics that can potentially result in a poor reproduction performance. The current work addresses this accuracy–stability dilemma through a new diffeomorphic transformation approach that serves as a framework generalizing the class of demonstrations that are learnable by means of provably stable dynamical systems. We apply the proposed framework to extend the application domain of the stable estimator of dynamical systems (SEDS) by generalizing the class of learnable demonstrations by means of diffeomorphic transformations τ. The resulting approach is named τ-SEDS and analyzed with rigorous theoretical investigations and robot experiments.},
  langid = {english},
  keywords = {Dynamical system,Imitation learning,Programming by demonstration,READ,Robotics,SEDS,Stability,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/neumann_steil_2015_learning_robot_motions_with_stable_dynamical_systems_under_diffeomorphic.pdf}
}

@inproceedings{neumannNeuralLearningStable2013,
  title = {Neural Learning of Stable Dynamical Systems Based on Data-Driven Lyapunov Candidates},
  booktitle = {2013 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Neumann, Klaus and Lemme, Andre and Steil, Jochen J.},
  date = {2013},
  pages = {1216--1222},
  publisher = {{IEEE}},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/neumann_et_al_2013_neural_learning_of_stable_dynamical_systems_based_on_data-driven_lyapunov.pdf}
}

@article{ngLearnedLiftedLinearization2023,
  title = {Learned {{Lifted Linearization Applied}} to {{Unstable Dynamic Systems Enabled}} by {{Koopman Direct Encoding}}},
  author = {Ng, Jerry and Asada, H. Harry},
  date = {2023},
  journaltitle = {IEEE Control Systems Letters},
  volume = {7},
  pages = {1153--1158},
  issn = {2475-1456},
  doi = {10.1109/LCSYS.2022.3231641},
  abstract = {This letter presents a Koopman lifting linearization method that is applicable to nonlinear dynamical systems having both stable and unstable regions. It is known that Dynamic Mode Decomposition (DMD) and its extended methods are often unable to model unstable systems accurately and reliably. Here we solve the problem through merging three methodologies: decomposition of a lifted linear system into stable and unstable modes, deep learning of a dictionary of observable functions in the separated subspaces, and a new formula for obtaining the Koopman operator, called Direct Encoding. Two sets of effective observable functions are obtained through neural net training where the training data are separated into stable and unstable trajectories. The resultant learned observables are used for lifting the state space, and a linear state transition matrix is constructed using Direct Encoding where inner products of the learned observables are computed. The proposed method shows a dramatic improvement over existing DMD and data-driven methods. Furthermore, a method is developed for determining the boundaries between stable and unstable regions.},
  eventtitle = {{{IEEE Control Systems Letters}}},
  keywords = {Computational modeling,Encoding,Heuristic algorithms,machine learning,Mathematical models,modeling,Neural networks,READ,stability of nonlinear systems,Training,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ng_asada_2023_learned_lifted_linearization_applied_to_unstable_dynamic_systems_enabled_by.pdf}
}

@inproceedings{nguyenConstructingForceclosureGrasps1986,
  title = {Constructing Force-Closure Grasps},
  booktitle = {Proceedings. 1986 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Nguyen, V.-D.},
  date = {1986},
  volume = {3},
  pages = {1368--1373},
  publisher = {{Institute of Electrical and Electronics Engineers}},
  location = {{San Francisco, CA, USA}},
  doi = {10.1109/ROBOT.1986.1087483},
  url = {http://ieeexplore.ieee.org/document/1087483/},
  urldate = {2019-05-06},
  eventtitle = {1986 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  langid = {english},
  keywords = {#nosource,machine learning,object grasping,robotic grasping,tno internship},
  annotation = {00971}
}

@article{ngwompoPassivityAnalysisLinear2017,
  title = {Passivity Analysis of Linear Physical Systems with Internal Energy Sources Modelled by Bond Graphs},
  author = {Ngwompo, Roger and Galindo, Rene},
  date = {2017-01-01},
  journaltitle = {Proceedings of the Institution of Mechanical Engineers, Part I: Journal of Systems and Control Engineering},
  shortjournal = {Proceedings of the Institution of Mechanical Engineers, Part I: Journal of Systems and Control Engineering},
  volume = {231},
  pages = {14--28},
  doi = {10.1177/0959651816682144},
  abstract = {Integrated dynamic systems such as mechatronic or control systems generally contain passive elements and internal energy sources that are appropriately modulated to perform the desired dynamic actions. The overall passivity of such systems is a useful property that relates to the stability and the safety of the system, in the sense that the maximum net amount of energy that the system can impart to the environment is limited by its initial state. In this paper, conditions under which a physical system containing internal modulated sources is globally passive are investigated using bond graph modelling techniques. For the class of systems under consideration, bond graph models include power bonds and active (signals) bonds modulating embedded energy sources, so that the continuity of power (or energy conservation) in the junction structure is not satisfied. For the purpose of the analysis, a so-called bond graph pseudo-junction structure is proposed as an alternative representation for linear time-invariant (LTI) bond graph models with internal modulated sources. The pseudo-junction structure highlights the existence of a multiport coupled resistive field involving the modulation gains of the internal sources and the parameters of dissipative elements, therefore implicitly realizing the balance of internal energy generation and dissipation. Moreover, it can be regarded as consisting of an inner structure which satisfies the continuity of power, and an outer structure in which a power scaling is performed in relation with the dissipative field. The associated multiport coupled resistive field constitutive equations can then be used to determine the passivity property of the overall system. The paper focuses on systems interconnected in cascade (with no loading effect) or in closed-loop configurations which are common in control systems.},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ngwompo_galindo_2017_passivity_analysis_of_linear_physical_systems_with_internal_energy_sources.pdf}
}

@book{nikraveshNonlinearSystemsStability2018,
  title = {Nonlinear Systems Stability Analysis: {{Lyapunov-based}} Approach},
  shorttitle = {Nonlinear Systems Stability Analysis},
  author = {Nikravesh, Seyed Kamaleddin Yadavar},
  date = {2018},
  publisher = {{CRC Press}},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/nikravesh_2018_nonlinear_systems_stability_analysis.pdf}
}

@online{notomistaPassivityBasedDecentralizedControl2019,
  title = {Passivity-{{Based Decentralized Control}} of {{Multi-Robot Systems With Delays Using Control Barrier Functions}}},
  author = {Notomista, Gennaro and Cai, Xiaoyi and Yamauchi, Junya and Egerstedt, Magnus},
  date = {2019-09-03},
  eprint = {1904.04801},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1904.04801},
  url = {http://arxiv.org/abs/1904.04801},
  urldate = {2022-08-12},
  abstract = {In this paper, we present a solution to the problem of coordinating multiple robots across a communication channel that experiences delays. The proposed approach leverages control barrier functions in order to ensure that the multi-robot system remains dissipative. This is achieved by encoding the dissipativity-preserving condition as a set invariance constraint. This constraint is then included in an optimization problem, whose objective is that of modifying, in a minimally invasive fashion, the nominal input to the robots. The formulated optimization problem is decentralized in the sense that, in order to be solved, it does not require the individual robots to have access to global information. Moreover, thanks to its convexity, each robot can solve it using fast and efficient algorithms. The effectiveness of the proposed control framework is demonstrated through the implementation of a formation control algorithm in presence of delays on a team of mobile robots.},
  pubstate = {preprint},
  keywords = {Electrical Engineering and Systems Science - Systems and Control},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/notomista_et_al_2019_passivity-based_decentralized_control_of_multi-robot_systems_with_delays_using.pdf}
}

@online{notomistaSafetyPassivityFilter2021,
  title = {A {{Safety}} and {{Passivity Filter}} for {{Robot Teleoperation Systems}}},
  author = {Notomista, Gennaro and Cai, Xiaoyi},
  date = {2021-02-17},
  eprint = {2102.08630},
  eprinttype = {arxiv},
  eprintclass = {cs, math},
  doi = {10.48550/arXiv.2102.08630},
  url = {http://arxiv.org/abs/2102.08630},
  urldate = {2022-08-12},
  abstract = {In this paper, we present a way of enforcing safety and passivity properties of robot teleoperation systems, where a human operator interacts with a dynamical system modeling the robot. The approach does so in a holistic fashion, by combining safety and passivity constraints in a single optimization-based controller which effectively filters the desired control input before supplying it to the system. The result is a safety and passivity filter implemented as a convex quadratic program which can be solved efficiently and employed in an online fashion in many robotic teleoperation applications. Simulation results show the benefits of the approach developed in this paper applied to the human teleoperation of a second-order dynamical system.},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics,Mathematics - Dynamical Systems},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/notomista_cai_2021_a_safety_and_passivity_filter_for_robot_teleoperation_systems.pdf}
}

@online{NumericalOptimizationSpringerLink,
  title = {Numerical {{Optimization}} | {{SpringerLink}}},
  url = {https://link.springer.com/book/10.1007/978-0-387-40065-5},
  urldate = {2021-03-30},
  keywords = {#nosource,numerical optimization}
}

@unpublished{odonoghueCombiningPolicyGradient2017,
  title = {Combining Policy Gradient and {{Q-learning}}},
  author = {O'Donoghue, Brendan and Munos, Remi and Kavukcuoglu, Koray and Mnih, Volodymyr},
  date = {2017-04-07},
  eprint = {1611.01626},
  eprinttype = {arxiv},
  eprintclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1611.01626},
  urldate = {2020-07-02},
  abstract = {Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as 'PGQL', for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,model-free,policy gradients,QL,reinforcement learning,Statistics - Machine Learning}
}

@unpublished{openaiLearningDexterousInHand2019,
  title = {Learning {{Dexterous In-Hand Manipulation}}},
  author = {OpenAI and Andrychowicz, Marcin and Baker, Bowen and Chociej, Maciek and Jozefowicz, Rafal and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and Schneider, Jonas and Sidor, Szymon and Tobin, Josh and Welinder, Peter and Weng, Lilian and Zaremba, Wojciech},
  date = {2019-01-18},
  eprint = {1808.00177},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1808.00177},
  urldate = {2020-07-02},
  abstract = {We use reinforcement learning (RL) to learn dexterous in-hand manipulation policies which can perform vision-based object reorientation on a physical Shadow Dexterous Hand. The training is performed in a simulated environment in which we randomize many of the physical properties of the system like friction coefficients and an object's appearance. Our policies transfer to the physical robot despite being trained entirely in simulation. Our method does not rely on any human demonstrations, but many behaviors found in human manipulation emerge naturally, including finger gaiting, multi-finger coordination, and the controlled use of gravity. Our results were obtained using the same distributed RL system that was used to train OpenAI Five. We also include a video of our results: https://youtu.be/jwSbzNHGflM},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,real-world,reinforcement learning,Statistics - Machine Learning}
}

@article{ortegaInterconnectionDampingAssignment2002,
  title = {Interconnection and Damping Assignment Passivity-Based Control of Port-Controlled {{Hamiltonian}} Systems},
  author = {Ortega, Romeo and family=Schaft, given=Arjan, prefix=van der, useprefix=true and Maschke, Bernhard and Escobar, Gerardo},
  date = {2002-04-01},
  journaltitle = {Automatica},
  shortjournal = {Automatica},
  volume = {38},
  number = {4},
  pages = {585--596},
  issn = {0005-1098},
  doi = {10.1016/S0005-1098(01)00278-3},
  url = {https://www.sciencedirect.com/science/article/pii/S0005109801002783},
  urldate = {2022-05-19},
  abstract = {Passivity-based control (PBC) is a well-established technique that has shown to be very powerful to design robust controllers for physical systems described by Euler–Lagrange (EL) equations of motion. For regulation problems of mechanical systems, which can be stabilized “shaping” only the potential energy, PBC preserves the EL structure and furthermore assigns a closed-loop energy function equal to the difference between the energy of the system and the energy supplied by the controller. Thus, we say that stabilization is achieved via energy balancing. Unfortunately, these nice properties of EL–PBC are lost when used in other applications which require shaping of the total energy, for instance, in electrical or electromechanical systems, or even some underactuated mechanical devices. Our main objective in this paper is to develop a new PBC theory which extends to a broader class of systems the aforementioned energy-balancing stabilization mechanism and the structure invariance. Towards this end, we depart from the EL description of the systems and consider instead port-controlled Hamiltonian models, which result from the network modelling of energy-conserving lumped-parameter physical systems with independent storage elements, and strictly contain the class of EL models.},
  langid = {english},
  keywords = {Hamiltonian systems,Nonlinear control,Passivity,Stabilization of nonlinear systems},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ortega_et_al_2002_interconnection_and_damping_assignment_passivity-based_control_of.pdf}
}

@article{ortenziHybridMotionForce2017,
  title = {Hybrid Motion/Force Control: A Review},
  shorttitle = {Hybrid Motion/Force Control},
  author = {Ortenzi, V. and Stolkin, R. and Kuo, J. and Mistry, M.},
  date = {2017-10-18},
  journaltitle = {Advanced Robotics},
  volume = {31},
  number = {19-20},
  pages = {1102--1113},
  publisher = {{Taylor \& Francis}},
  issn = {0169-1864},
  doi = {10.1080/01691864.2017.1364168},
  url = {https://doi.org/10.1080/01691864.2017.1364168},
  urldate = {2022-06-22},
  abstract = {This paper reviews hybrid motion/force control, a control scheme which enables robots to perform tasks involving both motion, in the free space, and interactive force, at the contacts. Motivated by the large amount of literature on this topic, we facilitate comparison and elucidate the key differences among different approaches. An emphasis is placed on the study of the decoupling of motion control and force control. And we conclude that it is indeed possible to achieve a complete decoupling; however, this feature can be relaxed or sacrificed to reduce the robot’s joint torques while still completing the task.},
  keywords = {Hybrid motion/force control,motion/force decoupling},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ortenzi_et_al_2017_hybrid_motion-force_control.pdf}
}

@article{osaAlgorithmicPerspectiveImitation2018,
  title = {An {{Algorithmic Perspective}} on {{Imitation Learning}}},
  author = {Osa, Takayuki and Pajarinen, Joni and Neumann, Gerhard and Bagnell, J. Andrew and Abbeel, Pieter and Peters, Jan},
  date = {2018-03-26},
  journaltitle = {Foundations and Trends® in Robotics},
  shortjournal = {ROB},
  volume = {7},
  number = {1-2},
  pages = {1--179},
  publisher = {{Now Publishers, Inc.}},
  issn = {1935-8253, 1935-8261},
  doi = {10.1561/2300000053},
  url = {http://www.nowpublishers.com/article/Details/ROB-053},
  urldate = {2022-09-09},
  abstract = {An Algorithmic Perspective on Imitation Learning},
  langid = {english},
  keywords = {IMPORTANT,REVIEW},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/osa_et_al_2018_an_algorithmic_perspective_on_imitation_learning.pdf}
}

@article{osinenkoReinforcementLearningGuarantees2022,
  title = {Reinforcement Learning with Guarantees: A Review},
  shorttitle = {Reinforcement Learning with Guarantees},
  author = {Osinenko, Pavel and Dobriborsci, Dmitrii and Aumer, Wolfgang},
  date = {2022-01-01},
  journaltitle = {IFAC-PapersOnLine},
  shortjournal = {IFAC-PapersOnLine},
  series = {6th {{IFAC Conference}} on {{Intelligent Control}} and {{Automation Sciences ICONS}} 2022},
  volume = {55},
  number = {15},
  pages = {123--128},
  issn = {2405-8963},
  doi = {10.1016/j.ifacol.2022.07.619},
  url = {https://www.sciencedirect.com/science/article/pii/S2405896322010308},
  urldate = {2023-05-02},
  abstract = {Reinforcement learning is concerned with a generic concept of an agent acting in an environment. From the control theory standpoint, reinforcement learning may be considered as an adaptive optimal control scheme. Despite accumulating evidence of effectiveness of reinforcement learning in various applications, which range from video games to robotics, this control scheme in its bare-bones version provides no guarantees on the performance of the agent-environment closed loop. Measures have to be taken to provide the said guarantees. This survey gives a brief picture of the current progress in this direction. Three major groups of approaches are overviewed: supervisor-based, Lyapunov reinforcement learning and fusion with model-predictive control. The central message of this survey is that a synergy with classical model-based control seems the most promising direction of research in reinforcement learning, as long as it is to become an industry standard.},
  langid = {english},
  keywords = {READ,Reinforcement learning,REVIEW,safe control,stability},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/osinenko_et_al_2022_reinforcement_learning_with_guarantees.pdf}
}

@unpublished{ostrovskiCountBasedExplorationNeural2017,
  title = {Count-{{Based Exploration}} with {{Neural Density Models}}},
  author = {Ostrovski, Georg and Bellemare, Marc G. and family=Oord, given=Aaron, prefix=van den, useprefix=false and Munos, Remi},
  date = {2017-06-14},
  eprint = {1703.01310},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1703.01310},
  urldate = {2020-07-02},
  abstract = {Bellemare et al. (2016) introduced the notion of a pseudo-count, derived from a density model, to generalize count-based exploration to non-tabular reinforcement learning. This pseudo-count was used to generate an exploration bonus for a DQN agent and combined with a mixed Monte Carlo update was sufficient to achieve state of the art on the Atari 2600 game Montezuma's Revenge. We consider two questions left open by their work: First, how important is the quality of the density model for exploration? Second, what role does the Monte Carlo update play in exploration? We answer the first question by demonstrating the use of PixelCNN, an advanced neural density model for images, to supply a pseudo-count. In particular, we examine the intrinsic difficulties in adapting Bellemare et al.'s approach when assumptions about the model are violated. The result is a more practical and general algorithm requiring no special apparatus. We combine PixelCNN pseudo-counts with different agent architectures to dramatically improve the state of the art on several hard Atari games. One surprising finding is that the mixed Monte Carlo update is a powerful facilitator of exploration in the sparsest of settings, including Montezuma's Revenge.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,exploration,intrinsic motivation,reinforcement learning}
}

@book{ottCartesianImpedanceControl2008,
  title = {Cartesian Impedance Control of Redundant and Flexible-Joint Robots},
  author = {Ott, Christian},
  date = {2008},
  publisher = {{Springer}},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/cartesian_impedance_control_of_redundant_and_flexible-joint_robots.pdf}
}

@article{ottPrioritizedMultitaskCompliance2015,
  title = {Prioritized Multi-Task Compliance Control of Redundant Manipulators},
  author = {Ott, Christian and Dietrich, Alexander and Albu-Schäffer, Alin},
  date = {2015},
  journaltitle = {Automatica},
  volume = {53},
  pages = {416--423},
  publisher = {{Elsevier}},
  keywords = {STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ott_et_al_2015_prioritized_multi-task_compliance_control_of_redundant_manipulators.pdf}
}

@inproceedings{ottSubspaceorientedEnergyDistribution2011,
  title = {Subspace-Oriented Energy Distribution for the {{Time Domain Passivity Approach}}},
  booktitle = {2011 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Ott, Christian and Artigas, Jordi and Preusche, Carsten},
  date = {2011-09},
  pages = {665--671},
  issn = {2153-0866},
  doi = {10.1109/IROS.2011.6094697},
  abstract = {The Time Domain Passivity Control Approach (TDPA) is a powerful tool to guarantee passive interaction between a robot and its environment. Rather than establishing fixed control parameters to keep the system stable in any possible environment, the TDPA observes the energy flow due to the interaction and applies a dissipative term in the case the interaction becomes active. In a robot manipulator the rationale behind the Passivity Controller requires a criterion on how to distribute the energy to be dissipated among the multiple joints and must be adjusted according to the general control goal of the application. This paper presents a method for distributing the dissipation between decoupled subspaces of a redundant manipulator, prioritizing dissipation in the null-space. The method allows to preserve passivity while avoiding disturbance of the general control goal defined in the task-space. Thus the general control goal can, to some extent, be as well decoupled from passivity considerations and thus a less conservative controller can be achieved. The approach is sustained with a numerical simulation.},
  eventtitle = {2011 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  keywords = {Aerospace electronics,Damping,Joints,Kinetic energy,Manipulators,Robot kinematics,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ott_et_al_2011_subspace-oriented_energy_distribution_for_the_time_domain_passivity_approach.pdf}
}

@unpublished{paineOneShotHighFidelityImitation2018,
  title = {One-{{Shot High-Fidelity Imitation}}: {{Training Large-Scale Deep Nets}} with {{RL}}},
  shorttitle = {One-{{Shot High-Fidelity Imitation}}},
  author = {Paine, Tom Le and Colmenarejo, Sergio Gómez and Wang, Ziyu and Reed, Scott and Aytar, Yusuf and Pfaff, Tobias and Hoffman, Matt W. and Barth-Maron, Gabriel and Cabi, Serkan and Budden, David and family=Freitas, given=Nando, prefix=de, useprefix=true},
  date = {2018-10-11},
  eprint = {1810.05017},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1810.05017},
  urldate = {2020-07-02},
  abstract = {Humans are experts at high-fidelity imitation -- closely mimicking a demonstration, often in one attempt. Humans use this ability to quickly solve a task instance, and to bootstrap learning of new tasks. Achieving these abilities in autonomous agents is an open problem. In this paper, we introduce an off-policy RL algorithm (MetaMimic) to narrow this gap. MetaMimic can learn both (i) policies for high-fidelity one-shot imitation of diverse novel skills, and (ii) policies that enable the agent to solve tasks more efficiently than the demonstrators. MetaMimic relies on the principle of storing all experiences in a memory and replaying these to learn massive deep neural network policies by off-policy RL. This paper introduces, to the best of our knowledge, the largest existing neural networks for deep RL and shows that larger networks with normalization are needed to achieve one-shot high-fidelity imitation on a challenging manipulation task. The results also show that both types of policy can be learned from vision, in spite of the task rewards being sparse, and without access to demonstrator actions.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,imitation learning,inverse RL learning,reinforcement learning},
  annotation = {00009}
}

@article{panBayesMeetsLyapunov,
  title = {Bayes {{Meets Lyapunov}}: {{A Gaussian Process Approach}} to {{Guarantee Stability}} in {{Reinforcement Learning}} for {{Continuous Control}}},
  author = {Pan, Wei},
  pages = {6},
  langid = {english},
  keywords = {#nosource}
}

@article{panzirschDeflectionDomainPassivityControl2022,
  title = {Deflection-{{Domain Passivity Control}} of {{Variable Stiffnesses Based}} on {{Potential Energy Reference}}},
  author = {Panzirsch, Michael and Sierotowicz, Marek and Prakash, Revanth and Singh, Harsimran and Ott, Christian},
  date = {2022-04},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {2},
  pages = {4440--4447},
  issn = {2377-3766},
  doi = {10.1109/LRA.2022.3147566},
  abstract = {With emerging capabilities, robots will advance gradually into human environments in the near future. Thereby, safety and robustness is currently tackled through intrinsically soft robotics or variable impedances, mainly stiffnesses. In tele-operation, for instance, the control stiffness can be adapted to a measured arm impedance of the operator to stiffen the robot only when required for a manipulation task. Thus, humans or moving objects in the robot’s environment are protected from hard collisions. Independent from its realization through hardware or software, the stability of the variation needs to be ensured through control strategies since energy is potentially introduced into the robotic system. This work presents a novel gradient-based passivity control concept for variable stiffnesses. In contrast to state-of-the-art methods, the approach is based on a potential energy storage reference and prevents phases of zero stiffness through deflection-domain control. I.e., according to the energy storage, the stiffness variation over the spring deflection is controlled to ensure passivity. Experiments confirm the functionality of the approach and its robustness against delayed communication and active environments.},
  eventtitle = {{{IEEE Robotics}} and {{Automation Letters}}},
  keywords = {Attenuation,Couplings,deflection-domain control,Force,Impedance,Potential energy,Robots,Safety in HRI,Springs,STABILITY,telerobotics and teleoperation,variable impedance},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/panzirsch_et_al_2022_deflection-domain_passivity_control_of_variable_stiffnesses_based_on_potential.pdf}
}

@article{papageorgiouKinestheticGuidanceUtilizing2020,
  title = {Kinesthetic {{Guidance Utilizing DMP Synchronization}} and {{Assistive Virtual Fixtures}} for {{Progressive Automation}}},
  author = {Papageorgiou, Dimitrios and Dimeas, Fotios and Kastritsi, Theodora and Doulgeri, Zoe},
  date = {2020-10},
  journaltitle = {Robotica},
  shortjournal = {Robotica},
  volume = {38},
  number = {10},
  pages = {1824--1841},
  issn = {0263-5747, 1469-8668},
  doi = {10.1017/S0263574719001437},
  url = {https://www.cambridge.org/core/product/identifier/S0263574719001437/type/journal_article},
  urldate = {2022-08-09},
  abstract = {The progressive automation framework allows the seamless transition of a robot from kinesthetic guidance to autonomous operation mode during programming by demonstration of discrete motion tasks. This is achieved by the synergetic action of dynamic movement primitives (DMPs), virtual fixtures, and variable impedance control. The proposed DMPs encode the demonstrated trajectory and synchronize with the current demonstration from the user so that the reference generated motion follows the human’s demonstration. The proposed virtual fixtures assist the user in repeating the learned kinematic behavior but allow penetration so that the user can make modifications to the learned trajectory if needed. The tracking error in combination with the interaction forces and torques is used by a variable stiffness strategy to adjust the progressive automation level and transition the leading role between the human and the robot. An energy tank approach is utilized to apply the designed controller and to prove the passivity of the overall control method. An experimental evaluation of the proposed framework is presented for a pick and place task and results show that the transition to autonomous mode is achieved in few demonstrations.},
  langid = {english},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/papageorgiou_et_al_2020_kinesthetic_guidance_utilizing_dmp_synchronization_and_assistive_virtual.pdf}
}

@inproceedings{paraschosModelfreeProbabilisticMovement2015,
  title = {Model-Free Probabilistic Movement Primitives for Physical Interaction},
  booktitle = {2015 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Paraschos, Alexandros and Rueckert, Elmar and Peters, Jan and Neumann, Gerhard},
  date = {2015},
  pages = {2860--2866},
  publisher = {{IEEE}},
  keywords = {STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/paraschos_et_al_2015_model-free_probabilistic_movement_primitives_for_physical_interaction.pdf}
}

@article{paraschosProbabilisticMovementPrimitives2013,
  title = {Probabilistic Movement Primitives},
  author = {Paraschos, Alexandros and Daniel, Christian and Peters, Jan R. and Neumann, Gerhard},
  date = {2013},
  journaltitle = {Advances in neural information processing systems},
  volume = {26},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/paraschos_et_al_2013_probabilistic_movement_primitives.pdf}
}

@article{paraschosUsingProbabilisticMovement2018,
  title = {Using Probabilistic Movement Primitives in Robotics},
  author = {Paraschos, Alexandros and Daniel, Christian and Peters, Jan and Neumann, Gerhard},
  date = {2018},
  journaltitle = {Autonomous Robots},
  volume = {42},
  number = {3},
  pages = {529--551},
  publisher = {{Springer}},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/paraschos_et_al_2018_using_probabilistic_movement_primitives_in_robotics.pdf}
}

@unpublished{parisottoNeuralMapStructured2017,
  title = {Neural {{Map}}: {{Structured Memory}} for {{Deep Reinforcement Learning}}},
  shorttitle = {Neural {{Map}}},
  author = {Parisotto, Emilio and Salakhutdinov, Ruslan},
  date = {2017-02-27},
  eprint = {1702.08360},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1702.08360},
  urldate = {2020-07-02},
  abstract = {A critical component to enabling intelligent reasoning in partially observable environments is memory. Despite this importance, Deep Reinforcement Learning (DRL) agents have so far used relatively simple memory architectures, with the main methods to overcome partial observability being either a temporal convolution over the past k frames or an LSTM layer. More recent work (Oh et al., 2016) has went beyond these architectures by using memory networks which can allow more sophisticated addressing schemes over the past k frames. But even these architectures are unsatisfactory due to the reason that they are limited to only remembering information from the last k frames. In this paper, we develop a memory system with an adaptable write operator that is customized to the sorts of 3D environments that DRL agents typically interact with. This architecture, called the Neural Map, uses a spatially structured 2D memory image to learn to store arbitrary information about the environment over long time lags. We demonstrate empirically that the Neural Map surpasses previous DRL memories on a set of challenging 2D and 3D maze environments and show that it is capable of generalizing to environments that were not seen during training.},
  keywords = {#nosource,Computer Science - Machine Learning,memory,reinforcement learning},
  annotation = {00130}
}

@inproceedings{parkImpedanceControlVarying1998,
  title = {Impedance Control with Varying Stiffness for Parallel-Link Manipulators},
  booktitle = {Proceedings of the 1998 {{American Control Conference}}. {{ACC}} ({{IEEE Cat}}. {{No}}.{{98CH36207}})},
  author = {Park, J.H. and Cho, H.C.},
  date = {1998-06},
  volume = {1},
  pages = {478-482 vol.1},
  issn = {0743-1619},
  doi = {10.1109/ACC.1998.694714},
  abstract = {This paper proposes a new impedance control algorithm based on a variable stiffness-matrix, for parallel-link manipulators without the measurement of the tip position. It assumes that only the link lengths and velocities as well as the contact force at the tip are measured, and that the motion range is small around an operating point so that the nonlinear dynamics can be linearized. One of the key ideas is to change the stiffness matrix of the impedance model in order to compensate dynamics model parameter errors. In simulations, it is shown that despite the simplicity of the algorithm, its performance is similar to that of the control law based on the nonlinear dynamics with the correct parameters estimates, and that it is robust to estimation errors of some parameters such as the platform mass and the leg masses.},
  eventtitle = {Proceedings of the 1998 {{American Control Conference}}. {{ACC}} ({{IEEE Cat}}. {{No}}.{{98CH36207}})},
  keywords = {Force measurement,Impedance measurement,Length measurement,Manipulator dynamics,Motion measurement,Parameter estimation,Position measurement,Robust control,Velocity measurement,Weight control},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/park_cho_1998_impedance_control_with_varying_stiffness_for_parallel-link_manipulators.pdf}
}

@article{parkInputtoStateStabilityVariable2020,
  title = {Input-to-{{State Stability}} of {{Variable Impedance Control}} for {{Robotic Manipulator}}},
  author = {Park, Junho and Choi, Youngjin},
  date = {2020-01},
  journaltitle = {Applied Sciences},
  volume = {10},
  number = {4},
  pages = {1271},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2076-3417},
  doi = {10.3390/app10041271},
  url = {https://www.mdpi.com/2076-3417/10/4/1271},
  urldate = {2022-04-29},
  abstract = {Variable impedance control has been required to perform a variety of interactive tasks in contact with environments. In some cases, the time-varying stiffness matrix of the impedance model can be used to achieve high performance for uneven contact tasks. In the paper, two sufficient conditions are proposed to ensure the input-to-state stability (ISS) irrespective of time-varying stiffness. Furthermore, the update rule of the stiffness is also suggested in such a way that the asymptotic stability is guaranteed under certain region conditions. Even when the update rule is not applied, the ISS is at least assured. In other words, the error is always bounded only if the external force/torque is bounded. In detail, two sufficient conditions offer the lower bound of stiffness and the upper bound of its time derivative. Simulation results show that the ISS of variable impedance control is achieved if the proposed sufficient conditions are satisfied. Also, we can confirm the asymptotic behavior in the simulation when the stiffness is updated according to the given rule.},
  issue = {4},
  langid = {english},
  keywords = {IMPORTANT,ISS (input-to-state stability),READ,STABILITY,time-varying stiffness,update rule,variable impedance control},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/park_choi_2020_input-to-state_stability_of_variable_impedance_control_for_robotic_manipulator.pdf}
}

@unpublished{pashevichLearningAugmentSynthetic2019,
  title = {Learning to {{Augment Synthetic Images}} for {{Sim2Real Policy Transfer}}},
  author = {Pashevich, Alexander and Strudel, Robin A. M. and Kalevatykh, Igor and Laptev, Ivan and Schmid, Cordelia},
  date = {2019-03-18},
  eprint = {1903.07740},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1903.07740},
  urldate = {2019-05-09},
  abstract = {Vision and learning have made significant progress that could improve robotics policies for complex tasks and environments. Learning deep neural networks for image understanding, however, requires large amounts of domain-specific visual data. While collecting such data from real robots is possible, such an approach limits the scalability as learning policies typically requires thousands of trials. In this work we attempt to learn manipulation policies in simulated environments. Simulators enable scalability and provide access to the underlying world state during training. Policies learned in simulators, however, do not transfer well to real scenes given the domain gap between real and synthetic data. We follow recent work on domain randomization and augment synthetic images with sequences of random transformations. Our main contribution is to optimize the augmentation strategy for sim2real transfer and to enable domain-independent policy learning. We design an efficient search for depth image augmentations using object localization as a proxy task. Given the resulting sequence of random transformations, we use it to augment synthetic depth images during policy learning. Our augmentation strategy is policy-independent and enables policy learning with no real images. We demonstrate our approach to significantly improve accuracy on three manipulation tasks evaluated on a real robot.},
  keywords = {#nosource,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,robotic grasping,sim-to-real,Statistics - Machine Learning,tno internship},
  annotation = {00000}
}

@inproceedings{pastorLearningGeneralizationMotor2009,
  title = {Learning and Generalization of Motor Skills by Learning from Demonstration},
  booktitle = {2009 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Pastor, Peter and Hoffmann, Heiko and Asfour, Tamim and Schaal, Stefan},
  date = {2009-05},
  pages = {763--768},
  issn = {1050-4729},
  doi = {10.1109/ROBOT.2009.5152385},
  abstract = {We provide a general approach for learning robotic motor skills from human demonstration. To represent an observed movement, a non-linear differential equation is learned such that it reproduces this movement. Based on this representation, we build a library of movements by labeling each recorded movement according to task and context (e.g., grasping, placing, and releasing). Our differential equation is formulated such that generalization can be achieved simply by adapting a start and a goal parameter in the equation to the desired position values of a movement. For object manipulation, we present how our framework extends to the control of gripper orientation and finger position. The feasibility of our approach is demonstrated in simulation as well as on the Sarcos dextrous robot arm. The robot learned a pick-and-place operation and a water-serving task and could generalize these tasks to novel situations.},
  eventtitle = {2009 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  keywords = {Anthropomorphism,Differential equations,Fingers,Grippers,Humans,Labeling,Libraries,READ,Robotics and automation,Robots,Robustness},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/pastor_et_al_2009_learning_and_generalization_of_motor_skills_by_learning_from_demonstration2.pdf}
}

@inproceedings{pastorOnlineMovementAdaptation2011,
  title = {Online Movement Adaptation Based on Previous Sensor Experiences},
  booktitle = {2011 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Pastor, Peter and Righetti, Ludovic and Kalakrishnan, Mrinal and Schaal, Stefan},
  date = {2011-09},
  pages = {365--371},
  issn = {2153-0866},
  doi = {10.1109/IROS.2011.6095059},
  abstract = {Personal robots can only become widespread if they are capable of safely operating among humans. In uncertain and highly dynamic environments such as human households, robots need to be able to instantly adapt their behavior to unforseen events. In this paper, we propose a general framework to achieve very contact-reactive motions for robotic grasping and manipulation. Associating stereotypical movements to particular tasks enables our system to use previous sensor experiences as a predictive model for subsequent task executions. We use dynamical systems, named Dynamic Movement Primitives (DMPs), to learn goal-directed behaviors from demonstration. We exploit their dynamic properties by coupling them with the measured and predicted sensor traces. This feedback loop allows for online adaptation of the movement plan. Our system can create a rich set of possible motions that account for external perturbations and perception uncertainty to generate truly robust behaviors. As an example, we present an application to grasping with the WAM robot arm.},
  eventtitle = {2011 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  keywords = {Dynamics,Force,Grasping,Quaternions,Robot sensing systems,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/pastor_et_al_2011_online_movement_adaptation_based_on_previous_sensor_experiences.pdf}
}

@unpublished{pasUsingGeometryDetect2015,
  title = {Using {{Geometry}} to {{Detect Grasps}} in {{3D Point Clouds}}},
  author = {family=Pas, given=Andreas, prefix=ten, useprefix=false and Platt, Robert},
  date = {2015-01-13},
  eprint = {1501.03100},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1501.03100},
  urldate = {2019-04-17},
  abstract = {This paper proposes a new approach to detecting grasp points on novel objects presented in clutter. The input to our algorithm is a point cloud and the geometric parameters of the robot hand. The output is a set of hand configurations that are expected to be good grasps. Our key idea is to use knowledge of the geometry of a good grasp to improve detection. First, we use a geometrically necessary condition to sample a large set of high quality grasp hypotheses. We were surprised to find that using simple geometric conditions for detection can result in a relatively high grasp success rate. Second, we use the notion of an antipodal grasp (a standard characterization of a good two fingered grasp) to help us classify these grasp hypotheses. In particular, we generate a large automatically labeled training set that gives us high classification accuracy. Overall, our method achieves an average grasp success rate of 88\% when grasping novels objects presented in isolation and an average success rate of 73\% when grasping novel objects presented in dense clutter. This system is available as a ROS package at http://wiki.ros.org/agile\_grasp.},
  keywords = {#nosource,Computer Science - Robotics,gras,machine learning control,object grasping,robotic grasping,tno internship}
}

@unpublished{pathakCuriositydrivenExplorationSelfsupervised2017,
  title = {Curiosity-Driven {{Exploration}} by {{Self-supervised Prediction}}},
  author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
  date = {2017-05-15},
  eprint = {1705.05363},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1705.05363},
  urldate = {2020-07-02},
  abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at https://pathak22.github.io/noreward-rl/},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,exploration,intrinsic motivation,reinforcement learning,Statistics - Machine Learning}
}

@article{pengDeepMimicExampleguidedDeep2018,
  title = {{{DeepMimic}}: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills},
  shorttitle = {{{DeepMimic}}},
  author = {Peng, Xue Bin and Abbeel, Pieter and Levine, Sergey and family=Panne, given=Michiel, prefix=van de, useprefix=true},
  date = {2018-08-10},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {37},
  number = {4},
  pages = {1--14},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3197517.3201311},
  url = {https://dl.acm.org/doi/10.1145/3197517.3201311},
  urldate = {2020-07-02},
  langid = {english},
  keywords = {#nosource,imitation learning inverse RL learning,reinforcement learning},
  annotation = {00232}
}

@unpublished{pengVariationalDiscriminatorBottleneck2018,
  title = {Variational {{Discriminator Bottleneck}}: {{Improving Imitation Learning}}, {{Inverse RL}}, and {{GANs}} by {{Constraining Information Flow}}},
  shorttitle = {Variational {{Discriminator Bottleneck}}},
  author = {Peng, Xue Bin and Kanazawa, Angjoo and Toyer, Sam and Abbeel, Pieter and Levine, Sergey},
  date = {2018-12-28},
  eprint = {1810.00821},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1810.00821},
  urldate = {2020-07-02},
  abstract = {Adversarial learning methods have been proposed for a wide range of applications, but the training of adversarial models can be notoriously unstable. Effectively balancing the performance of the generator and discriminator is critical, since a discriminator that achieves very high accuracy will produce relatively uninformative gradients. In this work, we propose a simple and general technique to constrain information flow in the discriminator by means of an information bottleneck. By enforcing a constraint on the mutual information between the observations and the discriminator's internal representation, we can effectively modulate the discriminator's accuracy and maintain useful and informative gradients. We demonstrate that our proposed variational discriminator bottleneck (VDB) leads to significant improvements across three distinct application areas for adversarial learning algorithms. Our primary evaluation studies the applicability of the VDB to imitation learning of dynamic continuous control skills, such as running. We show that our method can learn such skills directly from \textbackslash emph\{raw\} video demonstrations, substantially outperforming prior adversarial imitation learning methods. The VDB can also be combined with adversarial inverse reinforcement learning to learn parsimonious reward functions that can be transferred and re-optimized in new settings. Finally, we demonstrate that VDB can train GANs more effectively for image generation, improving upon a number of prior stabilization methods.},
  keywords = {#nosource,Computer Science - Machine Learning,imitation learning,inverse RL learning,reinforcement learning,Statistics - Machine Learning},
  annotation = {00054}
}

@article{perrinFastDiffeomorphicMatching2016,
  title = {Fast Diffeomorphic Matching to Learn Globally Asymptotically Stable Nonlinear Dynamical Systems},
  author = {Perrin, Nicolas and Schlehuber-Caissier, Philipp},
  date = {2016-10},
  journaltitle = {Systems \& Control Letters},
  shortjournal = {Syst. Control Lett.},
  volume = {96},
  pages = {51--59},
  publisher = {{Elsevier Science Bv}},
  location = {{Amsterdam}},
  issn = {0167-6911},
  doi = {10.1016/j.sysconle.2016.06.018},
  url = {https://www.sciencedirect.com/science/article/abs/pii/S0167691116300846},
  urldate = {2022-09-11},
  abstract = {We propose a new diffeomorphic matching algorithm and use it to learn nonlinear dynamical systems with the guarantee that the learned systems have global asymptotic stability. For a given set of demonstration trajectories, and a reference globally asymptotically stable time-invariant system, we compute a diffeomorphism that maps forward orbits of the reference system onto the demonstrations. The same diffeomorphism deforms the whole reference system into one that reproduces the demonstrations, and is still globally asymptotically stable. (C) 2016 Elsevier B.V. All rights reserved.},
  langid = {english},
  keywords = {Diffeomorphic mapping,Dynamical movement primitives,Imitation learning,Lyapunov stability,motions,Nonlinear dynamical systems,READ},
  annotation = {WOS:000384788100009},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/perrin_schlehuber-caissier_2016_fast_diffeomorphic_matching_to_learn_globally_asymptotically_stable_nonlinear.pdf}
}

@article{pervezLearningTaskparameterizedDynamic2018,
  title = {Learning Task-Parameterized Dynamic Movement Primitives Using Mixture of {{GMMs}}},
  author = {Pervez, Affan and Lee, Dongheui},
  date = {2018},
  journaltitle = {Intelligent Service Robotics},
  volume = {11},
  number = {1},
  pages = {61--78},
  publisher = {{Springer}},
  keywords = {READ,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/pervez_lee_2018_learning_task-parameterized_dynamic_movement_primitives_using_mixture_of_gmms.pdf}
}

@article{petersReinforcementLearningMotor2008,
  title = {Reinforcement Learning of Motor Skills with Policy Gradients},
  author = {Peters, Jan and Schaal, Stefan},
  date = {2008-05},
  journaltitle = {Neural Networks: The Official Journal of the International Neural Network Society},
  shortjournal = {Neural Netw},
  volume = {21},
  number = {4},
  eprint = {18482830},
  eprinttype = {pmid},
  pages = {682--697},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2008.02.003},
  abstract = {Autonomous learning is one of the hallmarks of human and animal behavior, and understanding the principles of learning will be crucial in order to achieve true autonomy in advanced machines like humanoid robots. In this paper, we examine learning of complex motor skills with human-like limbs. While supervised learning can offer useful tools for bootstrapping behavior, e.g., by learning from demonstration, it is only reinforcement learning that offers a general approach to the final trial-and-error improvement that is needed by each individual acquiring a skill. Neither neurobiological nor machine learning studies have, so far, offered compelling results on how reinforcement learning can be scaled to the high-dimensional continuous state and action spaces of humans or humanoids. Here, we combine two recent research developments on learning motor control in order to achieve this scaling. First, we interpret the idea of modular motor control by means of motor primitives as a suitable way to generate parameterized control policies for reinforcement learning. Second, we combine motor primitives with the theory of stochastic policy gradient learning, which currently seems to be the only feasible framework for reinforcement learning for humanoids. We evaluate different policy gradient methods with a focus on their applicability to parameterized motor primitives. We compare these algorithms in the context of motor primitive learning, and show that our most modern algorithm, the Episodic Natural Actor-Critic outperforms previous algorithms by at least an order of magnitude. We demonstrate the efficiency of this reinforcement learning method in the application of learning to hit a baseball with an anthropomorphic robot arm.},
  langid = {english},
  keywords = {#nosource,Algorithms,Animals,Artificial Intelligence,classical RL,Computer Simulation,Extremities,Feedback,Forelimb,Humans,Locomotion,Motor Skills,Movement,policy gradients,reinforcement learning,Reinforcement Psychology,Robotics,Stochastic Processes},
  annotation = {00815}
}

@article{petricAcceleratedSensorimotorLearning2018,
  title = {Accelerated {{Sensorimotor Learning}} of {{Compliant Movement Primitives}}},
  author = {Petric, Tadej and Gams, Andrej and Colasanto, Luca and Ijspeert, Auke J. and Ude, Ales},
  date = {2018-12},
  journaltitle = {Ieee Transactions on Robotics},
  shortjournal = {IEEE Trans. Robot.},
  volume = {34},
  number = {6},
  pages = {1636--1642},
  publisher = {{Ieee-Inst Electrical Electronics Engineers Inc}},
  location = {{Piscataway}},
  issn = {1552-3098},
  doi = {10.1109/TRO.2018.2861921},
  url = {https://ieeexplore.ieee.org/document/8437179},
  urldate = {2022-12-15},
  abstract = {Autonomous trajectory generation through generalization requires a database of motion, which can be difficult and time consuming to obtain. In this paper, we propose a method for autonomous expansion of a database for the generation of compliant and accurate motion, achieved through the framework of compliant movement primitives (CMPs). These combine task-specific kinematic and corresponding feed-forward dynamic trajectories. The framework allows for generalization and modulation of dynamic behavior. Inspired by human sensorimotor learning abilities, we propose a novel method that can autonomously learn task-specific torque primitives (TPs) associated to given kinematic trajectories, encoded as dynamic movement primitives. The proposed algorithm is completely autonomous, and can be used to rapidly generate and expand the CMP database. Since CMPs are parameterized, statistical generalization can be used to obtain an initial TP estimate of a new CMP. Thereby, the learning rate of new CMPs can be significantly improved. The evaluation of the proposed approach on a Kuka LWR-4 robot performing a peg-in-hole task shows fast TP acquisition and accurate generalization estimates in real-world scenarios.},
  langid = {english},
  keywords = {Autonomous learning,compliant movement primitives (CMPs),internal   dynamic models,models,motion,READ},
  annotation = {WOS:000453564100016},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/petric_et_al_2018_accelerated_sensorimotor_learning_of_compliant_movement_primitives.pdf}
}

@inproceedings{piccinelliPassivityBasedBilateralTeleoperation2020,
  title = {A {{Passivity-Based Bilateral Teleoperation Architecture}} Using {{Distributed Nonlinear Model Predictive Control}}},
  booktitle = {2020 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Piccinelli, Nicola and Muradore, Riccardo},
  date = {2020-10},
  pages = {11466--11472},
  issn = {2153-0866},
  doi = {10.1109/IROS45743.2020.9341048},
  abstract = {Bilateral teleoperation systems allow the telepresence of an operator while working remotely. Such ability becomes crucial when dealing with critical environments like space, nuclear plants, rescue, and surgery. The main properties of a teleoperation system are the stability and the transparency which, in general, are in contrast and they cannot be fully achieved at the same time. In this paper, we will present a novel model predictive controller that implements a passivity-based bilateral teleoperation algorithm. Our solution mitigates the chattering issue arising when resorting to the energy tank (or reservoir) mechanism by forcing the passivity as a hard constraint on the system evolution.},
  eventtitle = {2020 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  keywords = {Heuristic algorithms,Prediction algorithms,Predictive models,STABILITY,Telepresence,Torque,Trajectory,Upper bound},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/piccinelli_muradore_2020_a_passivity-based_bilateral_teleoperation_architecture_using_distributed.pdf}
}

@unpublished{pintoAsymmetricActorCritic2017,
  title = {Asymmetric Actor Critic for Image-Based Robot Learning},
  author = {Pinto, Lerrel and Andrychowicz, Marcin and Welinder, Peter and Zaremba, Wojciech and Abbeel, Pieter},
  date = {2017},
  eprint = {1710.06542},
  eprinttype = {arxiv},
  keywords = {#nosource,object grasping,reinforcement learning,robotic grasping,tno internship}
}

@inproceedings{pintoLearningPushGrasping2017,
  title = {Learning to Push by Grasping: {{Using}} Multiple Tasks for Effective Learning},
  shorttitle = {Learning to Push by Grasping},
  booktitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Pinto, L. and Gupta, A.},
  date = {2017-05},
  pages = {2161--2168},
  doi = {10.1109/ICRA.2017.7989249},
  abstract = {Recently, end-to-end learning frameworks are gaining prevalence in the field of robot control. These frameworks input states/images and directly predict the torques or the action parameters. However, these approaches are often critiqued due to their huge data requirements for learning a task. The argument of the difficulty in scalability to multiple tasks is well founded, since training these tasks often require hundreds or thousands of examples. But do end-to-end approaches need to learn a unique model for every task? Intuitively, it seems that sharing across tasks should help since all tasks require some common understanding of the environment. In this paper, we attempt to take the next step in data-driven end-to-end learning frameworks: move from the realm of task-specific models to joint learning of multiple robot tasks. In an astonishing result we show that models with multi-task learning tend to perform better than task-specific models trained with same amounts of data. For example, a deep-network learned with 2.5K grasp and 2.5K push examples performs better on grasping than a network trained on 5K grasp examples.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  keywords = {#nosource,Data models,data-driven end-to-end learning frameworks,effective learning,Force,grasping,Grasping,grippers,learning (artificial intelligence),multiple robot tasks,multitask learning,object manipulation,robot control,Robot sensing systems,robotic grasping,task-specific models,tno internship,torques,Training,Visualization},
  annotation = {00037}
}

@unpublished{pintoSupersizingSelfsupervisionLearning2015,
  title = {Supersizing {{Self-supervision}}: {{Learning}} to {{Grasp}} from {{50K Tries}} and 700 {{Robot Hours}}},
  shorttitle = {Supersizing {{Self-supervision}}},
  author = {Pinto, Lerrel and Gupta, Abhinav},
  date = {2015-09-22},
  eprint = {1509.06825},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1509.06825},
  urldate = {2019-04-17},
  abstract = {Current learning-based robot grasping approaches exploit human-labeled datasets for training the models. However, there are two problems with such a methodology: (a) since each object can be grasped in multiple ways, manually labeling grasp locations is not a trivial task; (b) human labeling is biased by semantics. While there have been attempts to train robots using trial-and-error experiments, the amount of data used in such experiments remains substantially low and hence makes the learner prone to over-fitting. In this paper, we take the leap of increasing the available training data to 40 times more than prior work, leading to a dataset size of 50K data points collected over 700 hours of robot grasping attempts. This allows us to train a Convolutional Neural Network (CNN) for the task of predicting grasp locations without severe overfitting. In our formulation, we recast the regression problem to an 18-way binary classification over image patches. We also present a multi-stage learning approach where a CNN trained in one stage is used to collect hard negatives in subsequent stages. Our experiments clearly show the benefit of using large-scale datasets (and multi-stage training) for the task of grasping. We also compare to several baselines and show state-of-the-art performance on generalization to unseen objects for grasping.},
  keywords = {#nosource,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,machine learning control,object grasping,robotic grasping,supervised learning,tno internship}
}

@online{PlanningImagesDeep2020,
  title = {Planning from {{Images}} with {{Deep Latent Gaussian Process Dynamics}}},
  date = {2020-05-07T21:29:45+00:00},
  url = {https://deepai.org/publication/planning-from-images-with-deep-latent-gaussian-process-dynamics},
  urldate = {2021-05-07},
  abstract = {05/07/20 - Planning is a powerful approach to control problems with known environment dynamics. In unknown environments the agent needs to le...},
  organization = {{DeepAI}},
  keywords = {#nosource,dgp}
}

@book{plappertMultiGoalReinforcementLearning2018,
  title = {Multi-{{Goal Reinforcement Learning}}: {{Challenging Robotics Environments}} and {{Request}} for {{Research}}},
  author = {Plappert, Matthias and Andrychowicz, Marcin and Ray, Alex and McGrew, Bob and Baker, Bowen and Powell, Glenn and Schneider, Jonas and Tobin, Josh and Chociej, Maciek and Welinder, Peter and Kumar, Vikash and Zaremba, Wojciech},
  date = {2018},
  keywords = {#nosource,object grasping,reinforcement learning,robotic grasping,tno internship}
}

@article{pollayilChoosingStiffnessDamping2023,
  title = {Choosing {{Stiffness}} and {{Damping}} for {{Optimal Impedance Planning}}},
  author = {Pollayil, Mathew Jose and Angelini, Franco and Xin, Guiyang and Mistry, Michael and Vijayakumar, Sethu and Bicchi, Antonio and Garabini, Manolo},
  date = {2023-04},
  journaltitle = {IEEE Transactions on Robotics},
  volume = {39},
  number = {2},
  pages = {1281--1300},
  issn = {1941-0468},
  doi = {10.1109/TRO.2022.3216078},
  abstract = {The attention given to impedance control in recent years does not match a similar focus on the choice of impedance values that the controller should execute. Current methods are hardly general and often compute fixed controller gains relying on the use of expensive sensors. In this article, we address the problem of online impedance planning for Cartesian impedance controllers that do not assign the closed-loop inertia. We propose an optimization-based algorithm that, given the Cartesian inertia, computes the stiffness and damping gains without relying on force/torque measurements and so that the effects of perturbations are less than a maximum acceptable value. By doing so, we increase robot resilience to unexpected external disturbances while guaranteeing performance and robustness. The algorithm provides an analytical solution in the case of impedance-controlled robots with diagonally dominant inertia matrix. Instead, established numerical methods are employed to deal with the more common case of nondiagonally dominant inertia. Our work attempts to create a general impedance planning framework, which needs no additional hardware and is easily applicable to any robotic system. Through experiments on real robots, including a quadruped and a robotic arm, our method is shown to be employable in real time and to lead to satisfactory behaviors.},
  eventtitle = {{{IEEE Transactions}} on {{Robotics}}},
  keywords = {Control,End effectors,impedance,Impedance,Jacobian matrices,planning,Planning,Robots,Sensors,Task analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/pollayil_et_al_2023_choosing_stiffness_and_damping_for_optimal_impedance_planning.pdf}
}

@book{popovHyperstabilityControlSystems1973,
  title = {Hyperstability of Control Systems},
  author = {Popov, Vasile-Mihai and Georgescu, Radu},
  date = {1973},
  publisher = {{Springer}}
}

@article{prakashDynamicTrajectoryGeneration2020,
  title = {Dynamic {{Trajectory Generation}} and a {{Robust Controller}} to {{Intercept}} a {{Moving Ball}} in a {{Game Setting}}},
  author = {Prakash, Ravi and Behera, Laxmidhar and Mohan, Santhakumar and Jagannathan, Sarangapani},
  date = {2020-07},
  journaltitle = {IEEE Transactions on Control Systems Technology},
  volume = {28},
  number = {4},
  pages = {1418--1432},
  issn = {1558-0865},
  doi = {10.1109/TCST.2019.2913129},
  abstract = {Complex and interactive robot manipulation skills, such as playing a game of table tennis against a human opponent, are a novel problem with multifaceted challenges. Accurate dynamic trajectory generation in order to respond to the tennis ball from the opponent and a novel control scheme for robust and high-performance tracking of the ball in such dynamic situations is a prerequisite to winning the game. In this paper, the dynamic movement primitives (DMPs) are employed for the stable generation of dynamic trajectories in the presence of environmental uncertainties such as ball position and velocity, opponent position and velocity and so on. In DMP, kernels parameters need to be found from expert demonstrations using a learning technique. To mitigate environmental uncertainties by accurate reconstruction of novel dynamic trajectories by combining multiple DMPs, we propose a piecewise linear canonical system (PLCS)-based modified DMP in contrast to the standard exponential canonical system (ECS)-based DMP. To enhance effectiveness and performance, a novel learning technique based on Lyapunov stability is embedded with the above-developed formulation. We show that the proposed learning technique is faster, has the best steady state error performance, and requires almost half the kernels than the state of the art. Next, to intercept the dynamic moving ball in real-time using the DMP, a novel control scheme is needed and designed using fuzzy fractional order sliding mode control. The control scheme results in chatter free tracking of the desired trajectories because the control law is free from sgn(·) function. The tracking control performance is guaranteed theoretically via the Lyapunov approach and also verified through simulations and experiments. Finally, we have developed a complete system (including a vision system for tracking the ball) using a real 4 degrees-of-freedom (DOFs) Barrett Wam robotic arm and show that the proposed overall framework is able to respond to a hanging ball with high accuracy.},
  eventtitle = {{{IEEE Transactions}} on {{Control Systems Technology}}},
  keywords = {Dynamic trajectory generation,fractional calculus (FC),fuzzy logic,Games,Manipulators,READ,sliding mode control,Sports,Task analysis,Trajectory,transfer learning,Uncertainty},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/prakash_et_al_2020_dynamic_trajectory_generation_and_a_robust_controller_to_intercept_a_moving.pdf}
}

@article{prasslerShortHistoryCleaning2000,
  title = {A {{Short History}} of {{Cleaning Robots}}},
  author = {Prassler, Erwin and Ritter, Arno and Schaeffer, Christoph and Fiorini, Paolo},
  date = {2000-12-01},
  journaltitle = {Autonomous Robots},
  shortjournal = {Autonomous Robots},
  volume = {9},
  number = {3},
  pages = {211--226},
  issn = {1573-7527},
  doi = {10.1023/A:1008974515925},
  url = {https://doi.org/10.1023/A:1008974515925},
  urldate = {2019-11-28},
  abstract = {The definition of the desired functions and the design of an ultimate versatile personal robot is an ongoing debate. Meanwhile, however, precursors of this yet to evolve species are well on their way to become commercial products. Cleaning robots for public environments as well as for private households seem to be able to provide the breakthrough which the designers of non-industrial robot systems have long awaited.This survey describes a selection of 30 different cleaning robots, with the first developments reaching back more than 15 years. With a few exceptions we have focused on floor cleaning, in particular indoor floor cleaning. We describe a variety of scrubbing and vacuuming robots which were developed for this task. The described systems range from heavy, large, and expensive industrial cleaning vehicles to small-size, light-weight, low-cost household devices. Thesurvey does not include, however, systems for cleaning facades of buildings, or windows, or production tools.Although not all of the 30 cleaning robots abovementioned have yet reached the state of commercial products, their number alone certainly reflects the expectations regarding the economic value associated with the automation of cleaning tasks. In Europe only the estimates for the market for cleaning services range up to the order of US\$ 100 billion per year. It is therefore not surprising that the cleaning industry and the manufacturers of cleaning devices arerather enthusiastic with respect to the automation of cleaning tasks using (semi-)autonomous mobile robot systems.},
  langid = {english},
  keywords = {#nosource,automatic cleaning,autonomous cleaning devices,autonomous pool cleaner,autonomous vacuum cleaner,cleaning robots,robotic floor scrubber,robotic grasping,robotic road sweeper,tno internship}
}

@incollection{prattichizzoGrasping2008,
  title = {Grasping},
  booktitle = {Springer {{Handbook}} of {{Robotics}}},
  author = {Prattichizzo, Domenico and Trinkle, Jeffrey C.},
  editor = {Siciliano, Bruno and Khatib, Oussama},
  date = {2008},
  pages = {671--700},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-30301-5_29},
  url = {https://doi.org/10.1007/978-3-540-30301-5_29},
  urldate = {2019-04-16},
  abstract = {This chapter introduces fundamental models of grasp analysis. The overall model is a coupling of models that define contact behavior with widely used models of rigid-body kinematics and dynamics. The contact model essentially boils down to the selection of components of contact force and moment that are transmitted through each contact. Mathematical properties of the complete model naturally give rise to five primary grasp types whose physical interpretations provide insight for grasp and manipulation planning. After introducing the basic model and types of grasps, this chapter focuses on the most important grasp characteristic: complete restraint. A grasp with complete restraint prevents loss of contact and thus is very secure. Two primary restraint properties are form closure and force closure. A form closure grasp guarantees maintenance of contact as long as the links of the hand and the object are well approximated as rigid and as long as the joint actuators are sufficiently strong. As will be seen, the primary difference between form closure and force closure grasps is the latterʼs reliance on contact friction. This translates into requiring fewer contacts to achieve force closure than form closure.},
  isbn = {978-3-540-30301-5},
  langid = {english},
  keywords = {#nosource,Contact Force,Contact Model,Force Closure,Form Closure,Friction Cone,machine learning,object grasping,robotic grasping,tno internship}
}

@unpublished{pritzelNeuralEpisodicControl2017,
  title = {Neural {{Episodic Control}}},
  author = {Pritzel, Alexander and Uria, Benigno and Srinivasan, Sriram and Puigdomènech, Adrià and Vinyals, Oriol and Hassabis, Demis and Wierstra, Daan and Blundell, Charles},
  date = {2017-03-06},
  eprint = {1703.01988},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1703.01988},
  urldate = {2020-07-02},
  abstract = {Deep reinforcement learning methods attain super-human performance in a wide range of environments. Such methods are grossly inefficient, often taking orders of magnitudes more data than humans to achieve reasonable performance. We propose Neural Episodic Control: a deep reinforcement learning agent that is able to rapidly assimilate new experiences and act upon them. Our agent uses a semi-tabular representation of the value function: a buffer of past experience containing slowly changing state representations and rapidly updated estimates of the value function. We show across a wide range of environments that our agent learns significantly faster than other state-of-the-art, general purpose deep reinforcement learning agents.},
  keywords = {#nosource,Computer Science - Machine Learning,memory,reinforcement learning,Statistics - Machine Learning},
  annotation = {00127}
}

@unpublished{quillenDeepReinforcementLearning2018,
  title = {Deep {{Reinforcement Learning}} for {{Vision-Based Robotic Grasping}}: {{A Simulated Comparative Evaluation}} of {{Off-Policy Methods}}},
  shorttitle = {Deep {{Reinforcement Learning}} for {{Vision-Based Robotic Grasping}}},
  author = {Quillen, Deirdre and Jang, Eric and Nachum, Ofir and Finn, Chelsea and Ibarz, Julian and Levine, Sergey},
  date = {2018-03-28},
  eprint = {1802.10264},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1802.10264},
  urldate = {2019-11-27},
  abstract = {In this paper, we explore deep reinforcement learning algorithms for vision-based robotic grasping. Model-free deep reinforcement learning (RL) has been successfully applied to a range of challenging environments, but the proliferation of algorithms makes it difficult to discern which particular approach would be best suited for a rich, diverse task like grasping. To answer this question, we propose a simulated benchmark for robotic grasping that emphasizes off-policy learning and generalization to unseen objects. Off-policy learning enables utilization of grasping data over a wide variety of objects, and diversity is important to enable the method to generalize to new objects that were not seen during training. We evaluate the benchmark tasks against a variety of Q-function estimation methods, a method previously proposed for robotic grasping with deep neural network models, and a novel approach based on a combination of Monte Carlo return estimation and an off-policy correction. Our results indicate that several simple methods provide a surprisingly strong competitor to popular algorithms such as double Q-learning, and our analysis of stability sheds light on the relative tradeoffs between the algorithms.},
  keywords = {#nosource,Computer Science - Machine Learning,Computer Science - Robotics,object grasping,reinforcement learning,robotic grasping,Statistics - Machine Learning,tno internship}
}

@article{raibertHybridPositionForce1981,
  title = {Hybrid Position/Force Control of Manipulators},
  author = {Raibert, Marc H. and Craig, John J.},
  date = {1981},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/raibert_craig_1981_hybrid_position-force_control_of_manipulators.pdf}
}

@article{raiolaDevelopmentSafetyEnergyAware2018,
  title = {Development of a {{Safety-}} and {{Energy-Aware Impedance Controller}} for {{Collaborative Robots}}},
  author = {Raiola, Gennaro and Cardenas, Carlos Alberto and Tadele, Tadele Shiferaw and family=Vries, given=Theo, prefix=de, useprefix=true and Stramigioli, Stefano},
  date = {2018-04},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {3},
  number = {2},
  pages = {1237--1244},
  issn = {2377-3766},
  doi = {10.1109/LRA.2018.2795639},
  abstract = {In contexts where robots share their workspace with humans, safety is of utmost importance. Consequently, in recent years, a big impulse has been given to the design of human-friendly robots by involving both mechanical and control design aspects. Regarding controller design, this often involves introducing compliance and ensuring asymptotic stability using an interaction control scheme and passivity theory. Moreover, when human operators physically interact with the robot during work, strict safety measures become necessary with some of these including power and force limitations [1]. In this letter, a novel impedance control technique for collaborative robots is presented. The featured controller allows a safe human-robot interaction through energy and power limitations, assuring passivity through energy tanks. The proposed controller is evaluated with a KUKA LWR 4+ arm in a comanipulation environment.},
  eventtitle = {{{IEEE Robotics}} and {{Automation Letters}}},
  keywords = {Asymptotic stability,compliance and impedance control,Damping,Impedance,Measurement,physical human–robot interaction,Robot safety,Robots,Safety,Symmetric matrices},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/raiola_et_al_2018_development_of_a_safety-_and_energy-aware_impedance_controller_for.pdf;/home/ricks/Insync/OneDrive/Documents/Zotero/raiola_et_al_2018_development_of_a_safety-_and_energy-aware_impedance_controller_for2.pdf}
}

@article{ramirez-veraImpedanceControlBounded2022,
  title = {Impedance {{Control}} with {{Bounded Actions}} for {{Human}}–{{Robot Interaction}}},
  author = {Ramírez-Vera, Victor I. and Mendoza-Gutiérrez, Marco O. and Bonilla-Gutiérrez, Isela},
  date = {2022-02-19},
  journaltitle = {Arabian Journal for Science and Engineering},
  shortjournal = {Arab J Sci Eng},
  issn = {2191-4281},
  doi = {10.1007/s13369-022-06638-3},
  url = {https://doi.org/10.1007/s13369-022-06638-3},
  urldate = {2022-08-11},
  abstract = {Human–robot interaction tasks have seen an increased interest in recent years, leading to the need for new proposals both for the design of new robotic systems and for their control and security schemes. In this regard, this work proposes a first approach to impedance control for robot manipulators with bounded inputs which aims to achieve safe human–robot interaction. The proposed scheme has a nonlinear proportional–derivative structure with compensation (PD\$\$+\$\$) based on the robot model, makes use of generalized saturation functions to generate bounded control actions, and includes an external torque compensation term based on the user’s electromyographic information. One of the main advantages of this proposal is that the human–robot interaction is defined in the joint space, which avoids singularities, since the robot works within its natural coordinates and the torque applied by the user is estimated at a joint level. The advantage of the novel control scheme can be demonstrated by the stability analysis of the closed-loop system equilibrium point, as well as by comparative analysis of the simulation results.},
  langid = {english},
  keywords = {Bounded inputs,Impedance control,Lyapunov stability,Robot manipulator},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ramírez-vera_et_al_2022_impedance_control_with_bounded_actions_for_human–robot_interaction.pdf}
}

@inproceedings{ranaCoordinatedRobotMotions2021,
  title = {Towards {{Coordinated Robot Motions}}: {{End-to-End Learning}} of {{Motion Policies}} on {{Transform Trees}}},
  shorttitle = {Towards {{Coordinated Robot Motions}}},
  booktitle = {2021 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Rana, M. Asif and Li, Anqi and Fox, Dieter and Chernova, Sonia and Boots, Byron and Ratliff, Nathan},
  date = {2021-09},
  pages = {7792--7799},
  issn = {2153-0866},
  doi = {10.1109/IROS51168.2021.9636097},
  abstract = {Generating robot motion that fulfills multiple tasks simultaneously is challenging due to the geometric constraints imposed on the robot. In this paper, we propose to solve multi-task problems through learning structured policies from human demonstrations. Our structured policy is inspired by RMPflow, a framework for combining subtask policies on different spaces. The policy structure provides the user an interface to 1) specifying the spaces that are directly relevant to the completion of the tasks, and 2) designing policies for certain tasks that do not need to be learned. We derive an end-to-end learning objective that is suitable for the multi-task problem, emphasizing the distance between generated motions and demonstrations measured on task spaces. Furthermore, the motion generated from the learned policy class is guaranteed to be stable. We validate the effectiveness of our proposed learning framework through qualitative and quantitative evaluations on three robotic tasks on a 7-DOF Rethink Sawyer robot.},
  eventtitle = {2021 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  keywords = {Extraterrestrial measurements,Intelligent robots,Multitasking,READ,Robot kinematics,Robot motion,Task analysis,Transforms},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/rana_et_al_2021_towards_coordinated_robot_motions.pdf}
}

@inproceedings{ranaEuclideanizingFlowsDiffeomorphic2020,
  title = {Euclideanizing {{Flows}}: {{Diffeomorphic Reduction}} for {{Learning Stable Dynamical Systems}}},
  shorttitle = {Euclideanizing {{Flows}}},
  booktitle = {Proceedings of the 2nd {{Conference}} on {{Learning}} for {{Dynamics}} and {{Control}}},
  author = {Rana, Muhammad Asif and Li, Anqi and Fox, Dieter and Boots, Byron and Ramos, Fabio and Ratliff, Nathan},
  date = {2020-07-31},
  pages = {630--639},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v120/rana20a.html},
  urldate = {2022-05-30},
  abstract = {Execution of complex tasks in robotics requires motions that have complex geometric structure. We present an approach which allows robots to learn such motions from a few human demonstrations. The motions are encoded as rollouts of a dynamical system on a Riemannian manifold. Additional structure is imposed which guarantees smooth convergent motions to a goal location. The aforementioned structure involves viewing motions on an observed Riemannian manifold as deformations of straight lines on a latent Euclidean space. The observed and latent spaces are related through a diffeomorphism. Thus, this paper presents an approach for learning flexible diffeomorphisms, resulting in a stable dynamical system.  The efficacy of this approach is demonstrated through validation on an established benchmark as well demonstrations collected on a real-world robotic system.},
  eventtitle = {Learning for {{Dynamics}} and {{Control}}},
  langid = {english},
  keywords = {IMPORTANT,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/rana_et_al_2020_euclideanizing_flows.pdf}
}

@inproceedings{ranaLearningReactiveMotion2020,
  title = {Learning {{Reactive Motion Policies}} in {{Multiple Task Spaces}} from {{Human Demonstrations}}},
  booktitle = {Proceedings of the {{Conference}} on {{Robot Learning}}},
  author = {Rana, M. Asif and Li, Anqi and Ravichandar, Harish and Mukadam, Mustafa and Chernova, Sonia and Fox, Dieter and Boots, Byron and Ratliff, Nathan},
  date = {2020-05-12},
  pages = {1457--1468},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v100/rana20a.html},
  urldate = {2023-01-23},
  abstract = {Complex manipulation tasks often require non-trivial and coordinated movements of different parts of a robot. In this work, we address the challenges associated with learning and reproducing the skills required to execute such complex tasks. Specifically, we decompose a task into multiple subtasks and learn to reproduce the subtasks by learning stable policies from demonstrations. By leveraging the RMPflow framework for motion generation, our approach finds a stable global policy in the configuration space that enables simultaneous execution of various learned subtasks. The resulting global policy is a weighted combination of the learned policies such that the motions are coordinated and feasible under the robot’s kinematic and environmental constraints. We demonstrate the necessity and efficacy of the proposed approach in the context of multiple constrained manipulation tasks performed by a Franka Emika robot.},
  eventtitle = {Conference on {{Robot Learning}}},
  langid = {english},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/rana_et_al_2020_learning_reactive_motion_policies_in_multiple_task_spaces_from_human.pdf}
}

@inproceedings{ravanbakhshFormalPolicyLearning2019,
  title = {Formal {{Policy Learning}} from {{Demonstrations}} for {{Reachability Properties}}},
  booktitle = {2019 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Ravanbakhsh, Hadi and Sankaranarayanan, Sriram and Seshia, Sanjit A.},
  date = {2019},
  pages = {6037--6043},
  publisher = {{IEEE}},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ravanbakhsh_et_al_formal_policy_learning_from_demonstrations_for_reachability.pdf}
}

@article{ravanbakhshLearningControlLyapunov2019,
  title = {Learning Control Lyapunov Functions from Counterexamples and Demonstrations},
  author = {Ravanbakhsh, Hadi and Sankaranarayanan, Sriram},
  date = {2019-02-01},
  journaltitle = {Autonomous Robots},
  shortjournal = {Auton Robot},
  volume = {43},
  number = {2},
  pages = {275--307},
  issn = {1573-7527},
  doi = {10.1007/s10514-018-9791-9},
  url = {https://doi.org/10.1007/s10514-018-9791-9},
  urldate = {2022-09-11},
  abstract = {We present a technique for learning control Lyapunov-like functions, which are used in turn to synthesize controllers for nonlinear dynamical systems that can stabilize the system, or satisfy specifications such as remaining inside a safe set, or eventually reaching a target set while remaining inside a safe set. The learning framework uses a demonstrator that implements a black-box, untrusted strategy presumed to solve the problem of interest, a learner that poses finitely many queries to the demonstrator to infer a candidate function, and a verifier that checks whether the current candidate is a valid control Lyapunov-like function. The overall learning framework is iterative, eliminating a set of candidates on each iteration using the counterexamples discovered by the verifier and the demonstrations over these counterexamples. We prove its convergence using ellipsoidal approximation techniques from convex optimization. We also implement this scheme using nonlinear MPC controllers to serve as demonstrators for a set of state and trajectory stabilization problems for nonlinear dynamical systems. We show how the verifier can be constructed efficiently using convex relaxations of the verification problem for polynomial systems to semi-definite programming problem instances. Our approach is able to synthesize relatively simple polynomial control Lyapunov-like functions, and in that process replace the MPC using a guaranteed and computationally less expensive controller.},
  langid = {english},
  keywords = {Concept learning,Controller synthesis,Learning from demonstrations,Lyapunov functions,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ravanbakhsh_sankaranarayanan_2019_learning_control_lyapunov_functions_from_counterexamples_and_demonstrations.pdf}
}

@inproceedings{ravichandarLearningContractingNonlinear2016,
  title = {Learning {{Contracting Nonlinear Dynamics From Human Demonstration}} for {{Robot Motion Planning}}},
  author = {Ravichandar, Harish and Dani, Ashwin},
  date = {2016-01-12},
  publisher = {{American Society of Mechanical Engineers Digital Collection}},
  doi = {10.1115/DSCC2015-9870},
  url = {https://asmedigitalcollection.asme.org/DSCC/proceedings/DSCC2015/57250/V002T27A008/230481},
  urldate = {2022-09-30},
  eventtitle = {{{ASME}} 2015 {{Dynamic Systems}} and {{Control Conference}}},
  langid = {english},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ravichandar_dani_2016_learning_contracting_nonlinear_dynamics_from_human_demonstration_for_robot.pdf}
}

@inproceedings{ravichandarLearningPartiallyContracting2017,
  title = {Learning {{Partially Contracting Dynamical Systems}} from {{Demonstrations}}.},
  booktitle = {{{CoRL}}},
  author = {Ravichandar, Harish Chaandar and Salehi, Iman and Dani, Ashwin P.},
  date = {2017},
  pages = {369--378},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ravichandar_et_al_2017_learning_partially_contracting_dynamical_systems_from_demonstrations.pdf}
}

@article{ravichandarLearningPositionOrientation2019,
  title = {Learning Position and Orientation Dynamics from Demonstrations via Contraction Analysis},
  author = {family=Ravichandar, given=Harish, prefix=chaandar, useprefix=false and Dani, Ashwin},
  date = {2019-04-01},
  journaltitle = {Autonomous Robots},
  shortjournal = {Auton Robot},
  volume = {43},
  number = {4},
  pages = {897--912},
  issn = {1573-7527},
  doi = {10.1007/s10514-018-9758-x},
  url = {https://doi.org/10.1007/s10514-018-9758-x},
  urldate = {2022-11-02},
  abstract = {This paper presents a unified framework of model-learning algorithms, called contracting dynamical system primitives (CDSP), that can be used to learn pose (i.e., position and orientation) dynamics of point-to-point motions from demonstrations. The position and the orientation (represented using quaternions) trajectories are modeled as two separate autonomous nonlinear dynamical systems. The special constraints of the \$\$\{\textbackslash mathbb \{S\}\}\^{}\{3\}\$\$manifold are enforced in the formulation of the system that models the orientation dynamics. To capture the variability in the demonstrations, the dynamical systems are estimated using Gaussian mixture models (GMMs). The parameters of the GMMs are learned subject to the constraints derived using partial contraction analysis. The learned models’ reproductions are shown to accurately reproduce the demonstrations and are guaranteed to converge to the desired goal location. Experimental results illustrate the CDSP algorithm’s ability to accurately learn position and orientation dynamics and the utility of the learned models in path generation for a Baxter robot arm. The CDSP algorithm is evaluated on a publicly available dataset and a synthetic dataset, and is shown to have the lowest and comparable average reproduction errors when compared to state-of-the-art imitation learning algorithms.},
  langid = {english},
  keywords = {Contraction analysis,Gaussian mixture models,Learning from demonstration,Model learning,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ravichandar_dani_2019_learning_position_and_orientation_dynamics_from_demonstrations_via_contraction.pdf}
}

@inproceedings{ravichandarLearningStableNonlinear2018,
  title = {Learning {{Stable Nonlinear Dynamical Systems}} with {{External Inputs}} Using {{Gaussian Mixture Models}}},
  booktitle = {2018 {{Annual American Control Conference}} ({{ACC}})},
  author = {family=Ravichandar, given=Harish, prefix=chaandar, useprefix=false and Salehi, Iman and Baillie, Brian P. and Bollas, George M. and Dani, Ashwin},
  date = {2018-06},
  pages = {4825--4830},
  issn = {2378-5861},
  doi = {10.23919/ACC.2018.8431461},
  abstract = {Learning nonlinear dynamics from data is particularly useful in control design for complex systems since developing analytical models might be intractable. This paper presents an algorithm to learn a class of stable nonlinear dynamical systems from data. To accommodate for the variability in data, Gaussian mixture models (GMM) are used to represent the nonlinear dynamics in the state space. The algorithm can learn dynamical systems with time-varying external inputs and multiple equilibria. To ensure that the trajectories generated by the learned model converge to the desired equilibrium point, the parameters of the GMMs are learned subject to the constraints derived from partial contraction analysis. Given the initial conditions and the steady-state inputs, a separate GMM is trained to predict the equilibrium point at which the trajectories converge. Experimental results illustrate the proposed algorithm's ability to accurately learn the nonlinear dynamics of a counter-flow heat exchanger model.},
  eventtitle = {2018 {{Annual American Control Conference}} ({{ACC}})},
  keywords = {Gaussian mixture model,Heuristic algorithms,Modeling,Nonlinear dynamical systems,READ,Steady-state,Training data,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ravichandar_et_al_2018_learning_stable_nonlinear_dynamical_systems_with_external_inputs_using_gaussian.pdf}
}

@article{ravichandarRecentAdvancesRobot2020,
  title = {Recent Advances in Robot Learning from Demonstration},
  author = {Ravichandar, Harish and Polydoros, Athanasios S. and Chernova, Sonia and Billard, Aude},
  date = {2020},
  journaltitle = {Annual review of control, robotics, and autonomous systems},
  volume = {3},
  pages = {297--330},
  publisher = {{Annual Reviews}},
  keywords = {REVIEW},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ravichandar_et_al_2020_recent_advances_in_robot_learning_from_demonstration.pdf}
}

@article{rebeloTimeDomainPassivity2015,
  title = {Time {{Domain Passivity Controller}} for 4-{{Channel Time-Delay Bilateral Teleoperation}}},
  author = {Rebelo, João and Schiele, André},
  date = {2015-01},
  journaltitle = {IEEE Transactions on Haptics},
  volume = {8},
  number = {1},
  pages = {79--89},
  issn = {2329-4051},
  doi = {10.1109/TOH.2014.2363466},
  abstract = {This paper presents an extension of the time-domain passivity control approach to a four-channel bilateral controller under the effects of time delays. Time-domain passivity control has been used successfully to stabilize teleoperation systems with position-force and position-position controllers; however, the performance with such control architectures is sub-optimal both with and without time delays. This work extends the network representation of the time-domain passivity controller to the four-channel architecture, which provides perfect transparency to the user without time delay. The proposed architecture is based on modelling the controllers as dependent voltage sources and using only series passivity controllers. The obtained results are shown on a one degree-of-freedom setup and illustrate the stabilization behaviour of the proposed controller when time delay is present in the communication channel.},
  eventtitle = {{{IEEE Transactions}} on {{Haptics}}},
  keywords = {Communication channels,Delay effects,Force,force feedback,Force feedback,haptic rendering,Haptic rendering,Monitoring,Observers,Telemanipulation,Time-domain analysis,Voltage control},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/rebelo_schiele_2015_time_domain_passivity_controller_for_4-channel_time-delay_bilateral.pdf}
}

@unpublished{redmonRealTimeGraspDetection2014,
  title = {Real-{{Time Grasp Detection Using Convolutional Neural Networks}}},
  author = {Redmon, Joseph and Angelova, Anelia},
  date = {2014-12-09},
  eprint = {1412.3128},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1412.3128},
  urldate = {2019-05-08},
  abstract = {We present an accurate, real-time approach to robotic grasp detection based on convolutional neural networks. Our network performs single-stage regression to graspable bounding boxes without using standard sliding window or region proposal techniques. The model outperforms state-of-the-art approaches by 14 percentage points and runs at 13 frames per second on a GPU. Our network can simultaneously perform classification so that in a single step it recognizes the object and finds a good grasp rectangle. A modification to this model predicts multiple grasps per object by using a locally constrained prediction mechanism. The locally constrained model performs significantly better, especially on objects that can be grasped in a variety of ways.},
  keywords = {#nosource,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics,grasp pose detection,neural network,object grasping,robotic grasping,tno internship},
  annotation = {00183}
}

@unpublished{redmonYouOnlyLook2016,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  date = {2016-05-09},
  eprint = {1506.02640},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1506.02640},
  urldate = {2019-11-27},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  keywords = {#nosource,Computer Science - Computer Vision and Pattern Recognition,machine learning control}
}

@article{regoLearningbasedRobustNeurocontrol2022,
  title = {Learning-Based Robust Neuro-Control: {{A}} Method to Compute Control {{Lyapunov}} Functions},
  shorttitle = {Learning-Based Robust Neuro-Control},
  author = {Rego, Rosana C. B. and family=Araújo, given=Fábio M. U., prefix=de, useprefix=true},
  date = {2022},
  journaltitle = {International Journal of Robust and Nonlinear Control},
  volume = {32},
  number = {5},
  pages = {2644--2661},
  issn = {1099-1239},
  doi = {10.1002/rnc.5399},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/rnc.5399},
  urldate = {2022-12-06},
  abstract = {Nonlinear dynamical systems play a crucial role in control systems because, in practice, all the plants are nonlinear, and they are also a hopeful description of complex robot movements. To perform a control and stability analysis of a nonlinear system, usually, a Lyapunov function is used. In this article, we proposed a method to compute a control Lyapunov function (CLF) for nonlinear dynamics based on a learning robust neuro-control strategy. The procedure uses a deep neural network architecture to generate control functions supported by the Lyapunov stability theory. An estimation of the region of attraction is produced for advanced stability analysis. We implemented two numerical examples to compare the performance of the proposed technique with some existing methods. The proposed method computes a CLF that provides the stabilizability of the systems and produced better solutions to nonlinear systems in the design of stable controls without linear approximations and in the presence of disturbances.},
  langid = {english},
  keywords = {control Lyapunov function,deep neural network,intelligent control,nonlinear systems,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/rego_de_araújo_2022_learning-based_robust_neuro-control.pdf}
}

@article{revayConvexParameterizationRobust2020,
  title = {A Convex Parameterization of Robust Recurrent Neural Networks},
  author = {Revay, Max and Wang, Ruigang and Manchester, Ian R.},
  date = {2020},
  journaltitle = {IEEE Control Systems Letters},
  volume = {5},
  number = {4},
  pages = {1363--1368},
  publisher = {{IEEE}},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/revay_et_al_2020_a_convex_parameterization_of_robust_recurrent_neural_networks.pdf}
}

@inproceedings{revayRecurrentEquilibriumNetworks2021,
  title = {Recurrent {{Equilibrium Networks}}: {{Unconstrained Learning}} of {{Stable}} and {{Robust Dynamical Models}}},
  shorttitle = {Recurrent {{Equilibrium Networks}}},
  booktitle = {2021 60th {{IEEE Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Revay, Max and Wang, Ruigang and Manchester, Ian R.},
  date = {2021-12},
  pages = {2282--2287},
  issn = {2576-2370},
  doi = {10.1109/CDC45484.2021.9683054},
  abstract = {This paper introduces recurrent equilibrium networks (RENs), a new class of nonlinear dynamical models for applications in machine learning and system identification. The new model class has "built in" guarantees of stability and robustness: all models in the class are contracting – a strong form of nonlinear stability – and models can have prescribed Lipschitz bounds. RENs are otherwise very flexible: they can represent all stable linear systems, all previously-known sets of contracting recurrent neural networks, all deep feedforward neural networks, and all stable Wiener/Hammerstein models. RENs are parameterized directly by a vector in ℝN, i.e. stability and robustness are ensured without parameter constraints, which simplifies learning since generic methods for unconstrained optimization can be used. The performance of the robustness of the new model set is evaluated on benchmark nonlinear system identification problems.},
  eventtitle = {2021 60th {{IEEE Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  keywords = {Conferences,Linear systems,Machine learning,READ,Recurrent neural networks,Robustness,Stability analysis,System identification},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/revay_et_al_2021_recurrent_equilibrium_networks.pdf}
}

@article{reyLearningMotionsDemonstrations2018,
  title = {Learning Motions from Demonstrations and Rewards with Time-Invariant Dynamical Systems Based Policies},
  author = {Rey, Joel and Kronander, Klas and Farshidian, Farbod and Buchli, Jonas and Billard, Aude},
  date = {2018-01},
  journaltitle = {Autonomous Robots},
  shortjournal = {Auton. Robot.},
  volume = {42},
  number = {1},
  pages = {45--64},
  publisher = {{Springer}},
  location = {{Dordrecht}},
  issn = {0929-5593},
  doi = {10.1007/s10514-017-9636-y},
  url = {https://link.springer.com/article/10.1007/s10514-017-9636-y},
  urldate = {2022-09-11},
  abstract = {An important challenge when using reinforcement learning for learning motions in robotics is the choice of parameterization for the policy. We use Gaussian Mixture Regression to extract a parameterization with relevant non-linear features from a set of demonstrations of a motion following the paradigm of learning from demonstration. The resulting parameterization takes the form of a non-linear time-invariant dynamical system (DS). We use this time-invariant DS as a parameterized policy for a variant of the PI2 policy search algorithm. This paper contributes by adapting PI2 for our time-invariant motion representation. We introduce two novel parameter exploration schemes that can be used to (1) sample model parameters to achieve a uniform exploration in state space and (2) explore while ensuring stability of the resulting motion model. Additionally, a state dependent stiffness profile is learned simultaneously to the reference trajectory and both are used together in a variable impedance control architecture. This learning architecture is validated in a hardware experiment consisting of a digging task using a KUKA LWR platform.},
  langid = {english},
  keywords = {Dynamical systems,imitation,impedance control,IMPORTANT,Manipulation,READ,reinforcement,Reinforcement learning,skills,STABILITY},
  annotation = {WOS:000419487700003},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/rey_et_al_2018_learning_motions_from_demonstrations_and_rewards_with_time-invariant_dynamical.pdf}
}

@article{rezazadehLearningContractionPolicies2022,
  title = {Learning {{Contraction Policies From Offline Data}}},
  author = {Rezazadeh, Navid and Kolarich, Maxwell and Kia, Solmaz S. and Mehr, Negar},
  date = {2022-04},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {2},
  pages = {2905--2912},
  issn = {2377-3766},
  doi = {10.1109/LRA.2022.3145100},
  abstract = {This letter proposes a data-driven method for learning convergent control policies from offline data using Contraction theory. Contraction theory enables constructing a policy that makes the closed-loop system trajectories inherently convergent towards a unique trajectory. At the technical level, identifying the contraction metric, which is the distance metric with respect to which a robot’s trajectories exhibit contraction is often non-trivial. We propose to jointly learn the control policy and its corresponding contraction metric while enforcing contraction. To achieve this, we learn an implicit dynamics model of the robotic system from an offline data set consisting of the robot’s state and input trajectories. We propose a data augmentation algorithm for learning contraction policies using this learned dynamics model. We randomly generate samples in the state space and propagate them forward in time through the learned dynamics model to generate auxiliary sample trajectories. We then learn both the control policy and the contraction metric such that the distance between the trajectories from the offline data set and our generated auxiliary sample trajectories decreases over time. We evaluate the performance of our proposed framework on simulated robotic goal-reaching tasks and demonstrate that enforcing contraction results in faster convergence and greater robustness of the learned policy.},
  eventtitle = {{{IEEE Robotics}} and {{Automation Letters}}},
  keywords = {Convergence,Deep learning methods,Heuristic algorithms,machine learning for robot control,Measurement,READ,reinforcement learning,Robots,Robustness,System dynamics,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/rezazadeh_et_al_2022_learning_contraction_policies_from_offline_data.pdf}
}

@inproceedings{rombokasTendondrivenVariableImpedance2013,
  title = {Tendon-Driven Variable Impedance Control Using Reinforcement Learning},
  booktitle = {Robotics: {{Science}} and {{Systems}}},
  author = {Rombokas, Eric and Malhotra, Mark and Theodorou, Evangelos and Todorov, Emanuel and Matsuoka, Yoky},
  date = {2013},
  volume = {8},
  pages = {369--376},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/rombokas_et_al_2013_tendon-driven_variable_impedance_control_using_reinforcement_learning.pdf}
}

@article{rossaRoboticRehabilitationAssistance2021,
  title = {Robotic {{Rehabilitation}} and {{Assistance}} for {{Individuals With Movement Disorders Based}} on a {{Kinematic Model}} of the {{Upper Limb}}},
  author = {Rossa, Carlos and Najafi, Mohammad and Tavakoli, Mahdi and Adams, Kim},
  date = {2021-02},
  journaltitle = {IEEE Transactions on Medical Robotics and Bionics},
  volume = {3},
  number = {1},
  pages = {190--203},
  issn = {2576-3202},
  doi = {10.1109/TMRB.2021.3050512},
  abstract = {Design and development of robotic-assistance must consider the abilities of individuals with disabilities. In this article, a 8-DOF kinematic model of the upper limb complex is derived to evaluate the reachable workspace of the arm during interaction with a planar robot and to serve as the basis for rehabilitation strategies and assistive robotics. Through inverse differential kinematics and by taking account the physical limits of each arm joint, the model determines workspaces where the individual is able to perform tasks and those regions where robotic assistance is required. Next, a learning-from-demonstration strategy via a nonparametric potential field function is derived to teach the robot the required assistance based on demonstrations of functional tasks. This article investigates two applications. First, in the context of rehabilitation, robotic assistance is only provided if the individual is required to move her arm in regions that are not reachable via voluntary motion. Second, in the context of assistive robotics, the demonstrated trajectory is scaled down to match the individual's voluntary range of motion through a nonlinear workspace mapping. Assistance is provided within that workspace only. Experimental results in 5 different experimental scenarios with a person with cerebral palsy confirm the suitability of the proposed framework.},
  eventtitle = {{{IEEE Transactions}} on {{Medical Robotics}} and {{Bionics}}},
  keywords = {assistive robotics,cerebral palsy,Kinematic model,Kinematics,learning-from-demonstration,Manipulators,non-linear mapping,READ,rehabilitation robotics,Rehabilitation robotics,Robot kinematics,Robots,Shoulder,STABILITY,Task analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/rossa_et_al_2021_robotic_rehabilitation_and_assistance_for_individuals_with_movement_disorders.pdf}
}

@article{rovedaInteractionControllerFormulation2016,
  title = {An Interaction Controller Formulation to Systematically Avoid Force Overshoots through Impedance Shaping Method with Compliant Robot Base},
  author = {Roveda, Loris and Pedrocchi, Nicola and Vicentini, Federico and Molinari Tosatti, Lorenzo},
  date = {2016-11},
  journaltitle = {Mechatronics},
  shortjournal = {Mechatronics},
  volume = {39},
  pages = {42--53},
  issn = {09574158},
  doi = {10.1016/j.mechatronics.2016.08.001},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957415816300769},
  urldate = {2022-08-09},
  langid = {english},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/roveda_et_al_2016_an_interaction_controller_formulation_to_systematically_avoid_force_overshoots.pdf}
}

@online{rozanecNeuralDynamicMovement2022,
  title = {Neural {{Dynamic Movement Primitives}} -- a Survey},
  author = {Rožanec, Jože M. and Nemec, Bojan},
  date = {2022-08-03},
  eprint = {2208.01903},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2208.01903},
  url = {http://arxiv.org/abs/2208.01903},
  urldate = {2022-10-30},
  abstract = {One of the most important challenges in robotics is producing accurate trajectories and controlling their dynamic parameters so that the robots can perform different tasks. The ability to provide such motion control is closely related to how such movements are encoded. Advances on deep learning have had a strong repercussion in the development of novel approaches for Dynamic Movement Primitives. In this work, we survey scientific literature related to Neural Dynamic Movement Primitives, to complement existing surveys on Dynamic Movement Primitives.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/rožanec_nemec_2022_neural_dynamic_movement_primitives_--_a_survey.pdf}
}

@inproceedings{ruohanwangDynamicMovementPrimitives2016,
  title = {Dynamic {{Movement Primitives Plus}}: {{For}} Enhanced Reproduction Quality and Efficient Trajectory Modification Using Truncated Kernels and {{Local Biases}}},
  shorttitle = {Dynamic {{Movement Primitives Plus}}},
  booktitle = {2016 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {{Ruohan Wang} and Wu, Yan and {Wei Liang Chan} and {Keng Peng Tee}},
  date = {2016-10},
  pages = {3765--3771},
  publisher = {{IEEE}},
  location = {{Daejeon, South Korea}},
  doi = {10.1109/IROS.2016.7759554},
  url = {http://ieeexplore.ieee.org/document/7759554/},
  urldate = {2022-09-12},
  eventtitle = {2016 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  isbn = {978-1-5090-3762-9},
  keywords = {IMPORTANT,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ruohan_wang_et_al_2016_dynamic_movement_primitives_plus.pdf}
}

@unpublished{rusuProgressiveNeuralNetworks2016,
  title = {Progressive {{Neural Networks}}},
  author = {Rusu, Andrei A. and Rabinowitz, Neil C. and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
  date = {2016-09-07},
  eprint = {1606.04671},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1606.04671},
  urldate = {2020-07-02},
  abstract = {Learning to solve complex sequences of tasks--while both leveraging transfer and avoiding catastrophic forgetting--remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.},
  keywords = {#nosource,Computer Science - Machine Learning,multitask RL,reinforcement learning,transfer learning},
  annotation = {00693}
}

@article{sadanandananandSafeLearningControl2021,
  title = {Safe {{Learning}} for {{Control}} Using {{Control Lyapunov Functions}} and {{Control Barrier Functions}}: {{A Review}}},
  shorttitle = {Safe {{Learning}} for {{Control}} Using {{Control Lyapunov Functions}} and {{Control Barrier Functions}}},
  author = {Sadanandan Anand, Akhil and Seel, Katrine and Gjærum, Vilde Benoni and Håkansson, Anne and Robinson, Haakon and Saad, Aya},
  date = {2021},
  journaltitle = {3987-3997},
  publisher = {{Elsevier Science}},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2021.09.173},
  url = {https://ntnuopen.ntnu.no/ntnu-xmlui/handle/11250/2811583},
  urldate = {2022-10-31},
  abstract = {Real-world autonomous systems are often controlled using conventional model-based control methods. But if accurate models of a system are not available, these methods may be unsuitable. For many safety-critical systems, such as robotic systems, a model of the system and a control strategy may be learned using data. When applying learning to safety-critical systems, guaranteeing safety during learning as well as testing/deployment is paramount. A variety of different approaches for ensuring safety exists, but the published works are cluttered and there are few reviews that compare the latest approaches. This paper reviews two promising approaches on guaranteeing safety for learning-based robust control of uncertain dynamical systems, which are based on control barrier functions and control Lyapunov functions. While control barrier functions provide an option to incorporate safety in terms of constraint satisfaction, control Lyapunov functions are used to define safety in terms of stability. This review categorises learning-based methods that use control barrier functions and control Lyapunov functions into three groups, namely reinforcement learning, online and offline supervised learning. Finally, the paper presents a discussion of the suitability of the different methods for different applications.},
  langid = {english},
  keywords = {READ,REVIEW},
  annotation = {Accepted: 2021-10-14T08:36:42Z},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/sadanandan_anand_et_al_2021_safe_learning_for_control_using_control_lyapunov_functions_and_control_barrier.pdf;/home/ricks/Zotero/storage/HGRTC45S/Sadanandan Anand et al. - 2021 - Safe Learning for Control using Control Lyapunov F.pdf}
}

@article{sahbaniOverview3DObject2012,
  title = {An Overview of {{3D}} Object Grasp Synthesis Algorithms},
  author = {Sahbani, A. and El-Khoury, S. and Bidaud, P.},
  date = {2012-03},
  journaltitle = {Robotics and Autonomous Systems},
  volume = {60},
  number = {3},
  pages = {326--336},
  issn = {09218890},
  doi = {10.1016/j.robot.2011.07.016},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0921889011001485},
  urldate = {2019-05-06},
  abstract = {This overview presents computational algorithms for generating 3D object grasps with autonomous multi-fingered robotic hands. Robotic grasping has been an active research subject for decades, and a great deal of effort has been spent on grasp synthesis algorithms. Existing papers focus on reviewing the mechanics of grasping and the finger–object contact interactions Bicchi and Kumar (2000) [12] or robot hand design and their control Al-Gallaf et al. (1993) [70]. Robot grasp synthesis algorithms have been reviewed in Shimoga (1996) [71], but since then an important progress has been made toward applying learning techniques to the grasping problem. This overview focuses on analytical as well as empirical grasp synthesis approaches.},
  langid = {english},
  keywords = {#nosource,robotic grasping,tno internship},
  annotation = {00229}
}

@inproceedings{salehiActiveSamplingBased2019,
  title = {Active {{Sampling}} Based {{Safe Identification}} of {{Dynamical Systems}} Using {{Extreme Learning Machines}} and {{Barrier Certificates}}},
  booktitle = {2019 {{International Conference}} on {{Robotics}} and {{Automation}} (Icra)},
  author = {Salehi, Iman and Yao, Gang and Dani, Ashwin P.},
  editor = {Howard, A. and Althoefer, K. and Arai, F. and Arrichiello, F. and Caputo, B. and Castellanos, J. and Hauser, K. and Isler, V. and Kim, J. and Liu, H. and Oh, P. and Santos, V. and Scaramuzza, D. and Ude, A. and Voyles, R. and Yamane, K. and Okamura, A.},
  date = {2019},
  pages = {22--28},
  publisher = {{Ieee}},
  location = {{New York}},
  issn = {1050-4729},
  url = {https://www.sciencedirect.com/science/article/pii/S0167691116300846},
  urldate = {2022-12-02},
  abstract = {Learning the dynamical system (DS) model from data that preserves dynamical system properties is an important problem in many robot learning applications. Typically, the joint data coming from cyber-physical systems, such as robots have some underlying DS properties associated with it, e.g., convergence, invariance to a set, etc. In this paper, a model learning method is developed such that the trajectories of the DS are invariant in a given compact set. Such invariant DS models can be used to generate trajectories of the robot that will always remain in a prescribed set. In order to achieve invariance to a set, Barrier certificates are employed. The DS is approximated using Extreme Learning Machine (ELM), and a parameter learning problem subject to Barrier certificates enforced at all the points in the prescribed set is solved. To solve an infinite constraint problem for enforcing Barrier Certificates at every point in a given compact set, a modified constraint is developed that is sufficient to hold the Barrier certificates in the entire set. An active sampling strategy is formulated to minimize the number of constraints in learning. Simulation results of ELM learning with and without Barrier certificates are presented which show the invariance property being preserved in the ELM learning when learning procedure involves Barrier constraints. The method is validated using experiments conducted on a robot arm recreating invariant trajectories inside a prescribed set.},
  isbn = {978-1-5386-6026-3},
  langid = {english},
  keywords = {models,motion,READ},
  annotation = {WOS:000494942300004},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/salehi_et_al_2019_active_sampling_based_safe_identification_of_dynamical_systems_using_extreme.pdf}
}

@article{salehianDynamicalSystemApproach2016,
  title = {A {{Dynamical System Approach}} for {{Softly Catching}} a {{Flying Object}}: {{Theory}} and {{Experiment}}},
  shorttitle = {A {{Dynamical System Approach}} for {{Softly Catching}} a {{Flying Object}}},
  author = {Salehian, Seyed Sina Mirrazavi and Khoramshahi, Mahdi and Billard, Aude},
  date = {2016-04},
  journaltitle = {IEEE Transactions on Robotics},
  volume = {32},
  number = {2},
  pages = {462--471},
  issn = {1941-0468},
  doi = {10.1109/TRO.2016.2536749},
  abstract = {Catching a fast flying object is particularly challenging as it consists of two tasks: extremely precise estimation of the object's motion and control of the robot's motion. Any small imprecision may lead the fingers to close too abruptly and let the object fly away from the hand before closing. We present a strategy to overcome for sensorimotor imprecision by introducing softness in the catching approach. Soft catching consists of having the robot moves with the object for a short period of time, so as to leave more time for the fingers to close on the object. We use a dynamic system-based control law to generate the appropriate reach and follow motion, which is expressed as a linear parameter varying (LPV) system. We propose a method to approximate the parameters of LPV systems using Gaussian mixture models, based on a set of kinematically feasible demonstrations generated by an offline optimal control framework. We show theoretically that the resulting DS will intercept the object at the intercept point, at the right time with the desired velocity direction. Stability and convergence of the approach are assessed through Lyapunov stability theory. The proposed method is validated systematically to catch three objects that generate elastic contacts and demonstrate important improvement over a hard catching approach.},
  eventtitle = {{{IEEE Transactions}} on {{Robotics}}},
  keywords = {Catching,dynamical system,manipulation planning,Optimal control,Planning,READ,Robot sensing systems,STABILITY,Stability analysis,Tracking,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/salehian_et_al_2016_a_dynamical_system_approach_for_softly_catching_a_flying_object.pdf}
}

@article{salehianDynamicalSystemBasedApproachControlling2018,
  title = {A {{Dynamical-System-Based Approach}} for {{Controlling Robotic Manipulators During Noncontact}}/{{Contact Transitions}}},
  author = {Salehian, Seyed Sina Mirrazavi and Billard, Aude},
  date = {2018-10},
  journaltitle = {Ieee Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {3},
  number = {4},
  pages = {2738--2745},
  publisher = {{Ieee-Inst Electrical Electronics Engineers Inc}},
  location = {{Piscataway}},
  issn = {2377-3766},
  doi = {10.1109/LRA.2018.2833142},
  url = {https://ieeexplore.ieee.org/document/8354896},
  urldate = {2022-05-02},
  abstract = {Many daily life tasks require precise control when making contact with surfaces. Ensuring a smooth transition from free motion to contact is crucial as incurring a large impact force may lead to unstable contact with the robot bouncing on the surface, i.e., chattering. Stabilizing the forces at contact is not possible as the impact lasts for less than a millisecond, leaving no time for the robot to react to the impact force. We present a strategy in which the robot adapts its dynamic before entering into contact. The speed is modulated so as to align with the surface. We leverage the properties of autonomous dynamical systems for immediate re-planning and handling unforeseen perturbations and exploit local modulations of the dynamics to control for the smooth transitions at contact. We show theoretically and empirically that by using the modulation framework, the robot can stably touch the contact surface, even when the surface's location is uncertain, at a desired location, and finally, leave the surface or stop on the surface at a desired point.},
  langid = {english},
  keywords = {bang impact control,manipulation planning,mechanical systems,Motion control,obstacle avoidance,READ,tasks},
  annotation = {WOS:000435511900001},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/salehian_billard_2018_a_dynamical-system-based_approach_for_controlling_robotic_manipulators_during.pdf}
}

@inproceedings{salehiChanceConstrainedSystemIdentification2022,
  title = {Chance-{{Constrained System Identification}} of {{Nonlinear Discrete Systems}} with {{Safety}} and {{Stability Guarantees}}},
  booktitle = {2022 {{American Control Conference}} ({{ACC}})},
  author = {Salehi, Iman and Taplin, Tyler and Dani, Ashwin P.},
  date = {2022-06},
  pages = {1497--1502},
  issn = {2378-5861},
  doi = {10.23919/ACC53348.2022.9867712},
  abstract = {This paper presents a discrete-time nonlinear system identification method while satisfying the stability and safety properties of the system with high probability. An Extreme Learning Machine (ELM) is used with a Gaussian assumption on the function reconstruction error. A quadratically constrained quadratic program (QCQP) is developed with probabilistic safety and stability constraints that are only required to be satisfied at sampled points inside the invariant region. The proposed method is validated using two simulation examples: a two degrees-of-freedom (DoF) robot manipulator with constraints on joint angles whose trajectories are guaranteed to remain inside a safe set and on motion trajectories data of a hand-drawn shape.},
  eventtitle = {2022 {{American Control Conference}} ({{ACC}})},
  keywords = {Gaussian noise,Learning systems,Manipulators,Probabilistic logic,READ,Safety,Shape,Stability analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/salehi_et_al_2022_chance-constrained_system_identification_of_nonlinear_discrete_systems_with.pdf}
}

@online{salehiDiscreteTimeNonlinearSystems2022,
  title = {Discrete-{{Time Nonlinear Systems Identification}} with {{Probabilistic Safety}} and {{Stability Constraints}}},
  author = {Salehi, Iman and Taplin, Tyler and Dani, Ashwin P.},
  date = {2022-10-03},
  eprint = {2111.07466},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  doi = {10.48550/arXiv.2111.07466},
  url = {http://arxiv.org/abs/2111.07466},
  urldate = {2022-11-02},
  abstract = {This paper presents a discrete-time nonlinear system identification method while satisfying the stability and safety properties of the system with high probability. An Extreme Learning Machine (ELM) is used with a Gaussian assumption on the function reconstruction error. A quadratically constrained quadratic program (QCQP) is developed with probabilistic safety and stability constraints that are only required to be satisfied at sampled points inside the invariant region. The proposed method is validated using two simulation examples: a two degrees-of-freedom (DoF) robot manipulator with constraints on joint angles whose trajectories are guaranteed to remain inside a safe set and on motion trajectories data of a hand-drawn shape.},
  pubstate = {preprint},
  keywords = {Electrical Engineering and Systems Science - Systems and Control,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/salehi_et_al_2022_discrete-time_nonlinear_systems_identification_with_probabilistic_safety_and.pdf}
}

@article{salehiDynamicalSystemLearning2021,
  title = {Dynamical System Learning Using Extreme Learning Machines with Safety and Stability Guarantees},
  author = {Salehi, Iman and Rotithor, Ghananeel and Yao, Gang and Dani, Ashwin P.},
  date = {2021},
  journaltitle = {International Journal of Adaptive Control and Signal Processing},
  volume = {35},
  number = {6},
  pages = {894--914},
  issn = {1099-1115},
  doi = {10.1002/acs.3237},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/acs.3237},
  urldate = {2022-10-31},
  abstract = {This article presents a continuous dynamical system model learning methodology that can be used to generate reference trajectories for the autonomous systems to follow, such that these trajectories are invariant to a given closed set and uniformly ultimately bounded with respect to an equilibrium point inside the closed set. The autonomous system dynamics are approximated using extreme learning machines (ELM), the parameters of which are learned subject to the safety constraints expressed using a reciprocal barrier function, and the stability constraints derived using a Lyapunov analysis in the presence of the ELM reconstruction error. This formulation leads to solving a constrained quadratic program (QP) that includes a finite number of decision variables with an infinite number of constraints. Theorems are developed to relax the QP with infinite number of constraints to a QP with a finite number of constraints which can be practically implemented using a QP solver. In addition, an active sampling methodology is developed that further reduced the number of required constraints for the QP by only evaluating the constraints at a smaller subset of points. The proposed method is validated using a motion reproduction task on a seven degree-of-freedom Baxter robot, where the task space position and velocity dynamics are learned using the presented methodology.},
  langid = {english},
  keywords = {barrier certificates,model learning,READ,robustness,safety},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/salehi_et_al_2021_dynamical_system_learning_using_extreme_learning_machines_with_safety_and.pdf}
}

@article{salehiLearningDiscreteTimeUncertain2022,
  title = {Learning {{Discrete-Time Uncertain Nonlinear Systems}} with {{Probabilistic Safety}} and {{Stability Constraints}}},
  author = {Salehi, I. and Taplin, T. and Dani, A. P.},
  date = {2022},
  journaltitle = {IEEE Open Journal of Control Systems},
  pages = {1--12},
  issn = {2694-085X},
  doi = {10.1109/OJCSYS.2022.3216545},
  abstract = {This paper presents a discrete-time dynamical system model learning method from demonstration while providing probabilistic guarantees on the safety and stability of the learned model. The controlled dynamic model of a discrete-time system with a zero-mean Gaussian process noise is approximated using an Extreme Learning Machine (ELM) whose parameters are learned subject to chance constraints derived using a discrete-time control barrier function and discrete-time control Lyapunov function in the presence of the ELM reconstruction error. To estimate the ELM parameters a quadratically constrained quadratic program (QCQP) is developed subject to the constraints that are only required to be evaluated at sampled points. Simulations validate that the system model learned using the proposed method can reproduce the demonstrations inside a prescribed safe set while converging to the desired goal location starting from various different initial conditions inside the safe set. Furthermore, it is shown that the learned model can adapt to changes in goal location during reproductions without violating the stability and safety constraints.},
  eventtitle = {{{IEEE Open Journal}} of {{Control Systems}}},
  keywords = {Adaptation models,Control systems,Discrete-time Control Barrier Function,Discrete-time systems,Extreme Learning Machine,Nonlinear dynamical systems,Probabilistic logic,READ,Safe Model Learning,Safety,Stability criteria},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/salehi_et_al_2022_learning_discrete-time_uncertain_nonlinear_systems_with_probabilistic_safety.pdf}
}

@online{salehiRobustLearningNonlinear2022,
  title = {Robust {{Learning}} of {{Nonlinear Dynamical Systems}} with {{Safety}} and {{Stability Properties}}},
  author = {Salehi, Iman and Rotithor, Ghananeel and Dani, Ashwin P.},
  date = {2022-12-08},
  eprint = {2211.16676},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2211.16676},
  urldate = {2023-01-23},
  abstract = {The paper presents a robust parameter learning methodology for identification of nonlinear dynamical system from data while satisfying safety and stability constraints in the context of learning from demonstration (LfD) methods. Extreme Learning Machines (ELM) is used to approximate the system model, whose parameters are learned subject to the safety and stability constraints obtained using zeroing barrier and Lyapunov-based stability analysis in the presence of model uncertainties and external disturbances. A constrained Quadratic Program (QP) is developed, which accounts for the ELM function reconstruction error, to estimate the ELM parameters. Furthermore, a robustness lemma is presented, which proves that the learned system model guarantees safety and stability in the presence of disturbances. The method is tested in simulations. Trajectory reconstruction accuracy of the method is compared against state-of-the-art LfD methods using swept error area (SEA) metric. Robustness of the learned model is tested by conducting Monte Carlo tests. The proposed method is implemented on a Baxter robot for a pick-and-place task where the robot is constrained to an ellipsoidal safety region.},
  pubstate = {preprint},
  keywords = {Electrical Engineering and Systems Science - Systems and Control,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/salehi_et_al_2022_robust_learning_of_nonlinear_dynamical_systems_with_safety_and_stability.pdf}
}

@unpublished{salimansEvolutionStrategiesScalable2017,
  title = {Evolution {{Strategies}} as a {{Scalable Alternative}} to {{Reinforcement Learning}}},
  author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
  date = {2017-09-07},
  eprint = {1703.03864},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1703.03864},
  urldate = {2020-07-02},
  abstract = {We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,evolutionary algorithms,model-free,reinforcement learning,Statistics - Machine Learning}
}

@article{san-miguelAutomatedOffLineGeneration2022,
  title = {Automated {{Off-Line Generation}} of {{Stable Variable Impedance Controllers According}} to {{Performance Specifications}}},
  author = {San-Miguel, Alberto and Alenyà, Guillem and Puig, Vicenç},
  date = {2022-07},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {3},
  pages = {5874--5881},
  issn = {2377-3766},
  doi = {10.1109/LRA.2022.3160593},
  abstract = {In this letter, we propose a novel methodology for off-line generating stable Variable Impedance Controllers considering any parameter modulation law in function of exogenous signals to the robot, as e.g. the exerted force by the human in a collaborative task. The aim is to find the optimal controller according to a desired trade-off between accuracy and control effort. Each controller is formulated as a polytopic Linear Parameter Varying system consisting in a set of vertex systems at the limit operation points. Then, the stability and operating properties can be assessed through Linear Matrix Inequalities, from which an optimality index can be obtained. This index is used by a genetic optimisation algorithm to iteratively generate new controller solutions towards the best one. To exemplify our method we choose a case study of modulation laws for tasks that require a physical interaction between human and robot. Generated solutions for different trade-offs are evaluated on a object handover scenario using a 7-DoF WAM robotic manipulator.},
  eventtitle = {{{IEEE Robotics}} and {{Automation Letters}}},
  keywords = {compliance and impedance control,discrete systems,Force,Impedance,Indexes,Modulation,optimization and optimal control,Physical human-robot interaction,Robots,stability,STABILITY,Stability criteria,Task analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/san-miguel_et_al_2022_automated_off-line_generation_of_stable_variable_impedance_controllers.pdf}
}

@unpublished{santoroRelationalRecurrentNeural2018,
  title = {Relational Recurrent Neural Networks},
  author = {Santoro, Adam and Faulkner, Ryan and Raposo, David and Rae, Jack and Chrzanowski, Mike and Weber, Theophane and Wierstra, Daan and Vinyals, Oriol and Pascanu, Razvan and Lillicrap, Timothy},
  date = {2018-06-28},
  eprint = {1806.01822},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1806.01822},
  urldate = {2020-07-02},
  abstract = {Memory-based neural networks model temporal data by leveraging an ability to remember information for long periods. It is unclear, however, whether they also have an ability to perform complex relational reasoning with the information they remember. Here, we first confirm our intuitions that standard memory architectures may struggle at tasks that heavily involve an understanding of the ways in which entities are connected -- i.e., tasks involving relational reasoning. We then improve upon these deficits by using a new memory module -- a \textbackslash textit\{Relational Memory Core\} (RMC) -- which employs multi-head dot product attention to allow memories to interact. Finally, we test the RMC on a suite of tasks that may profit from more capable relational reasoning across sequential information, and show large gains in RL domains (e.g. Mini PacMan), program evaluation, and language modeling, achieving state-of-the-art results on the WikiText-103, Project Gutenberg, and GigaWord datasets.},
  keywords = {#nosource,Computer Science - Machine Learning,memory,meta-RL,reinforcement learning,Statistics - Machine Learning},
  annotation = {00079}
}

@article{santosLearningRobotReaching2018,
  title = {Learning Robot Reaching Motions by Demonstration Using Nonlinear Autoregressive Models},
  author = {Santos, Rafael F. and Pereira, Guilherme A. S. and Aguirre, L. A.},
  date = {2018-09-01},
  journaltitle = {Robotics and Autonomous Systems},
  shortjournal = {Robotics and Autonomous Systems},
  volume = {107},
  pages = {182--195},
  issn = {0921-8890},
  doi = {10.1016/j.robot.2018.06.006},
  url = {https://www.sciencedirect.com/science/article/pii/S092188901730814X},
  urldate = {2022-09-11},
  abstract = {This paper presents NAR-RM, a method for learning robot reaching motions from a set of demonstrations using Nonlinear AutoRegressive (NAR) polynomial models. Reaching motions are modeled as solutions to autonomous discrete-time nonlinear dynamical systems, so that the movements started near the data of the demonstrations follow the trained trajectories and always reach and stop at the target. Since NAR models obtained using standard system identification techniques do not always adequately model the reaching motions, in this paper we present a method that uses a least-squares estimator with constraints to impose the location of fixed points in the model. With the imposition of new fixed points it is possible to change the location of the original fixed points of the model, thus allowing the learning of stable reaching motions. We evaluate our method using a library of human handwriting motions, a mobile robot and an industrial manipulator.},
  langid = {english},
  keywords = {Dynamical systems,Fixed point,Learning by demonstration,Nonlinear autoregressive models,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/santos_et_al_2018_learning_robot_reaching_motions_by_demonstration_using_nonlinear_autoregressive.pdf}
}

@article{satishOnPolicyDatasetSynthesis2019,
  title = {On-{{Policy Dataset Synthesis}} for {{Learning Robot Grasping Policies Using Fully Convolutional Deep Networks}}},
  author = {Satish, Vishal and Mahler, Jeffrey and Goldberg, Ken},
  date = {2019-04},
  journaltitle = {IEEE Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {4},
  number = {2},
  pages = {1357--1364},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2019.2895878},
  url = {https://ieeexplore.ieee.org/document/8629272/},
  urldate = {2019-11-27},
  abstract = {Rapid and reliable robot grasping for a diverse set of objects has applications from warehouse automation to home de-cluttering. One promising approach is to learn deep policies from synthetic training datasets of point clouds, grasps, and rewards sampled using analytic models with stochastic noise models for domain randomization. In this paper, we explore how the distribution of synthetic training examples affects the rate and reliability of the learned robot policy. We propose a synthetic data sampling distribution that combines grasps sampled from the policy action set with guiding samples from a robust grasping supervisor that has full state knowledge. We use this to train a robot policy based on a fully convolutional network architecture that evaluates millions of grasp candidates in 4-DOF (3D position and planar orientation). Physical robot experiments suggest that a policy based on Fully Convolutional Grasp Quality CNNs (FC-GQ-CNNs) can plan grasps in 0.625s, considering 5000x more grasps than our prior policy based on iterative grasp sampling and evaluation. This computational efficiency improves rate and reliability, achieving 296 mean picks per hour (MPPH) compared to 250 MPPH for iterative policies. Sensitivity experiments explore the effect of supervisor guidance level and granularity of the policy action space. Code, datasets, videos, and supplementary material can be found at http://berkeleyautomation.github.io/fcgqcnn.},
  langid = {english},
  keywords = {#nosource,deep learning,neural network,robotic grasping,tno internship}
}

@inproceedings{saudraisRateModeBilateral2021,
  title = {Rate {{Mode Bilateral Teleoperation Based}} on {{Passivity Tanks}} and {{Variable Admittance Control}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Saudrais, Charlélie and Barbé, Laurent and Bayle, Bernard},
  date = {2021-05},
  pages = {3942--3948},
  issn = {2577-087X},
  doi = {10.1109/ICRA48506.2021.9561813},
  abstract = {Bilateral teleoperation under rate mode is known to be a difficult problem in terms of stability, especially when the slave manipulator interacts with a time-varying environment. This paper presents an energy based variable admittance control approach, whose principle combines the monitoring and the regulation of the energy exchanges with a passivity tank. It allows stable interactions with force feedback for any desired inertia, damping and stiffness parameters. Experiments are conducted to assess the efficiency of the proposed approach using an experimental setup with a variable stiffness environment. The obtained results illustrate the ability of the proposed strategy to stabilize a system otherwise unstable, with little effect on the transparency of the teleoperation system.},
  eventtitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  keywords = {Automation,Conferences,Damping,Force feedback,Manipulators,Regulation,STABILITY,Stability analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/saudrais_et_al_2021_rate_mode_bilateral_teleoperation_based_on_passivity_tanks_and_variable.pdf}
}

@unpublished{saundersTrialErrorSafe2017,
  title = {Trial without {{Error}}: {{Towards Safe Reinforcement Learning}} via {{Human Intervention}}},
  shorttitle = {Trial without {{Error}}},
  author = {Saunders, William and Sastry, Girish and Stuhlmueller, Andreas and Evans, Owain},
  date = {2017-07-17},
  eprint = {1707.05173},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1707.05173},
  urldate = {2020-07-02},
  abstract = {AI systems are increasingly applied to complex tasks that involve interaction with humans. During training, such systems are potentially dangerous, as they haven't yet learned to avoid actions that could cause serious harm. How can an AI system explore and learn without making a single mistake that harms humans or otherwise causes serious damage? For model-free reinforcement learning, having a human "in the loop" and ready to intervene is currently the only way to prevent all catastrophes. We formalize human intervention for RL and show how to reduce the human labor required by training a supervised learner to imitate the human's intervention decisions. We evaluate this scheme on Atari games, with a Deep RL agent being overseen by a human for four hours. When the class of catastrophes is simple, we are able to prevent all catastrophes without affecting the agent's learning (whereas an RL baseline fails due to catastrophic forgetting). However, this scheme is less successful when catastrophes are more complex: it reduces but does not eliminate catastrophes and the supervised learner fails on adversarial examples found by the agent. Extrapolating to more challenging environments, we show that our implementation would not scale (due to the infeasible amount of human labor required). We outline extensions of the scheme that are necessary if we are to train model-free agents without a single catastrophe.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,reinforcement learning,safety}
}

@article{saverianoCombiningDecisionMaking2020,
  title = {Combining Decision Making and Dynamical Systems for Monitoring and Executing Manipulation Tasks},
  author = {Saveriano, Matteo and Piater, Justus},
  date = {2020-10-01},
  journaltitle = {e \& i Elektrotechnik und Informationstechnik},
  shortjournal = {Elektrotech. Inftech.},
  volume = {137},
  number = {6},
  pages = {309--315},
  issn = {1613-7620},
  doi = {10.1007/s00502-020-00816-7},
  url = {https://doi.org/10.1007/s00502-020-00816-7},
  urldate = {2022-12-15},
  abstract = {In this paper, we propose a unified framework for online task scheduling, monitoring, and execution that integrates reconfigurable behavior trees, a decision-making framework with integrated low-level control functionalities, and reactive motion generation with stable dynamical systems. In this way, we realize a flexible and reactive system capable of coping with unexpected variations in the executive context without penalizing modularity, expressiveness, and readability of humans. The framework is evaluated in a simulated sorting task showing promising results in terms of flexibility regarding task scheduling and robustness to external disturbances.},
  langid = {english},
  keywords = {Ausführung und Überwachung von Tätigkeiten,decision making,Entscheidungsfindung,learning from demonstration,Nachahmungslernen,reactive motion planning,READ,reaktive Bewegungsplanung,task execution monitoring},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/saveriano_piater_2020_combining_decision_making_and_dynamical_systems_for_monitoring_and_executing.pdf}
}

@online{saverianoDynamicMovementPrimitives2021,
  title = {Dynamic {{Movement Primitives}} in {{Robotics}}: {{A Tutorial Survey}}},
  shorttitle = {Dynamic {{Movement Primitives}} in {{Robotics}}},
  author = {Saveriano, Matteo and Abu-Dakka, Fares J. and Kramberger, Aljaz and Peternel, Luka},
  date = {2021-02-07},
  eprint = {2102.03861},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2102.03861},
  urldate = {2022-08-23},
  abstract = {Biological systems, including human beings, have the innate ability to perform complex tasks in versatile and agile manner. Researchers in sensorimotor control have tried to understand and formally define this innate property. The idea, supported by several experimental findings, that biological systems are able to combine and adapt basic units of motion into complex tasks finally lead to the formulation of the motor primitives theory. In this respect, Dynamic Movement Primitives (DMPs) represent an elegant mathematical formulation of the motor primitives as stable dynamical systems, and are well suited to generate motor commands for artificial systems like robots. In the last decades, DMPs have inspired researchers in different robotic fields including imitation and reinforcement learning, optimal control,physical interaction, and human-robot co-working, resulting a considerable amount of published papers. The goal of this tutorial survey is two-fold. On one side, we present the existing DMPs formulations in rigorous mathematical terms,and discuss advantages and limitations of each approach as well as practical implementation details. In the tutorial vein, we also search for existing implementations of presented approaches and release several others. On the other side, we provide a systematic and comprehensive review of existing literature and categorize state of the art work on DMP. The paper concludes with a discussion on the limitations of DMPs and an outline of possible research directions.},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics,READ,REVIEW},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/saveriano_et_al_2021_dynamic_movement_primitives_in_robotics.pdf}
}

@unpublished{saverianoEnergybasedApproachEnsure2020,
  title = {An {{Energy-based Approach}} to {{Ensure}} the {{Stability}} of {{Learned Dynamical Systems}}},
  author = {Saveriano, Matteo},
  date = {2020-05-29},
  eprint = {2003.11290},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2003.11290},
  urldate = {2022-05-10},
  abstract = {Non-linear dynamical systems represent a compact, flexible, and robust tool for reactive motion generation. The effectiveness of dynamical systems relies on their ability to accurately represent stable motions. Several approaches have been proposed to learn stable and accurate motions from demonstration. Some approaches work by separating accuracy and stability into two learning problems, which increases the number of open parameters and the overall training time. Alternative solutions exploit single-step learning but restrict the applicability to one regression technique. This paper presents a single-step approach to learn stable and accurate motions that work with any regression technique. The approach makes energy considerations on the learned dynamics to stabilize the system at run-time while introducing small deviations from the demonstrated motion. Since the initial value of the energy injected into the system affects the reproduction accuracy, it is estimated from training data using an efficient procedure. Experiments on a real robot and a comparison on a public benchmark shows the effectiveness of the proposed approach.},
  keywords = {Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control,IMPORTANT,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/saveriano_2020_an_energy-based_approach_to_ensure_the_stability_of_learned_dynamical_systems.pdf}
}

@inproceedings{saverianoIncrementalMotionReshaping2020,
  title = {Incremental {{Motion Reshaping}} of {{Autonomous Dynamical Systems}}},
  booktitle = {Human-{{Friendly Robotics}} 2019},
  author = {Saveriano, Matteo and Lee, Dongheui},
  editor = {Ferraguti, F. and Villani, V. and Sabattini, L. and Bonfe, M.},
  date = {2020},
  volume = {12},
  pages = {43--57},
  publisher = {{Springer International Publishing Ag}},
  location = {{Cham}},
  issn = {2511-1256},
  doi = {10.1007/978-3-030-42026-0_4},
  url = {https://ieeexplore.ieee.org/document/8594474},
  urldate = {2022-12-15},
  abstract = {This paper presents an approach to incrementally learn a reshaping term that modifies the trajectories of an autonomous dynamical system without affecting its stability properties. The reshaping term is considered as an additive control input and it is incrementally learned from human demonstrations using Gaussian process regression. We propose a novel parametrization of this control input that preserves the time-independence and the stability of the reshaped system, as analytically proved in the performed Lyapunov stability analysis. The effectiveness of the proposed approach is demonstrated with simulations and experiments on a real robot.},
  isbn = {978-3-030-42026-0 978-3-030-42025-3},
  langid = {english},
  keywords = {Dynamical systems for motion   planning,Incremental learning of stable motions,READ,strategy,task},
  annotation = {WOS:000659339900004},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/saveriano_lee_2020_incremental_motion_reshaping_of_autonomous_dynamical_systems.pdf}
}

@inproceedings{saverianoIncrementalSkillLearning2018,
  title = {Incremental {{Skill Learning}} of {{Stable Dynamical Systems}}},
  booktitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Saveriano, Matteo and Lee, Dongheui},
  date = {2018-10},
  pages = {6574--6581},
  publisher = {{IEEE}},
  location = {{Madrid}},
  doi = {10.1109/IROS.2018.8594474},
  url = {https://ieeexplore.ieee.org/document/8594474/},
  urldate = {2022-09-12},
  eventtitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  isbn = {978-1-5386-8094-0},
  langid = {english},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/saveriano_lee_2018_incremental_skill_learning_of_stable_dynamical_systems.pdf}
}

@inproceedings{saverianoLearningBarrierFunctions2019,
  title = {Learning {{Barrier Functions}} for {{Constrained Motion Planning}} with {{Dynamical Systems}}},
  booktitle = {2019 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Saveriano, Matteo and Lee, Dongheui},
  date = {2019-11},
  pages = {112--119},
  issn = {2153-0866},
  doi = {10.1109/IROS40897.2019.8967981},
  abstract = {Stable dynamical systems are a flexible tool to plan robotic motions in real-time. In the robotic literature, dynamical system motions are typically planned without considering possible limitations in the robot's workspace. This work presents a novel approach to learn workspace constraints from human demonstrations and to generate motion trajectories for the robot that lie in the constrained workspace. Training data are incrementally clustered into different linear subspaces and used to fit a low dimensional representation of each subspace. By considering the learned constraint subspaces as zeroing barrier functions, we are able to design a control input that keeps the system trajectory within the learned bounds. This control input is effectively combined with the original system dynamics preserving eventual asymptotic properties of the unconstrained system. Simulations and experiments on a real robot show the effectiveness of the proposed approach.},
  eventtitle = {2019 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/saveriano_lee_2019_learning_barrier_functions_for_constrained_motion_planning_with_dynamical.pdf}
}

@online{saverianoLearningStableRobotic2022,
  title = {Learning {{Stable Robotic Skills}} on {{Riemannian Manifolds}}},
  author = {Saveriano, Matteo and Abu-Dakka, Fares J. and Kyrki, Ville},
  date = {2022-08-31},
  eprint = {2208.13267},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2208.13267},
  url = {http://arxiv.org/abs/2208.13267},
  urldate = {2022-10-30},
  abstract = {In this paper, we propose an approach to learn stable dynamical systems evolving on Riemannian manifolds. The approach leverages a data-efficient procedure to learn a diffeomorphic transformation that maps simple stable dynamical systems onto complex robotic skills. By exploiting mathematical tools from differential geometry, the method ensures that the learned skills fulfill the geometric constraints imposed by the underlying manifolds, such as unit quaternion (UQ) for orientation and symmetric positive definite (SPD) matrices for impedance, while preserving the convergence to a given target. The proposed approach is firstly tested in simulation on a public benchmark, obtained by projecting Cartesian data into UQ and SPD manifolds, and compared with existing approaches. Apart from evaluating the approach on a public benchmark, several experiments were performed on a real robot performing bottle stacking in different conditions and a drilling task in cooperation with a human operator. The evaluation shows promising results in terms of learning accuracy and task adaptation capabilities.},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics,IMPORTANT,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/saveriano_et_al_2022_learning_stable_robotic_skills_on_riemannian_manifolds.pdf}
}

@inproceedings{saverianoMergingPositionOrientation2019,
  title = {Merging Position and Orientation Motion Primitives},
  booktitle = {2019 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Saveriano, Matteo and Franzel, Felix and Lee, Dongheui},
  date = {2019},
  pages = {7041--7047},
  publisher = {{IEEE}},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/saveriano_et_al_2019_merging_position_and_orientation_motion_primitives.pdf}
}

@article{schaalControlPlanningLearning2003,
  title = {Control, Planning, Learning, and Imitation with Dynamic Movement Primitives},
  author = {Schaal, Stefan and Peters, Jan and Nakanishi, J. and Ijspeert, A.J.},
  date = {2003-01-01},
  journaltitle = {Workshop on Bilateral Paradigms on Humans and Humanoids, IEEE Int. Conf. on Intelligent Robots and Systems, Las Vegas, NV},
  shortjournal = {Workshop on Bilateral Paradigms on Humans and Humanoids, IEEE Int. Conf. on Intelligent Robots and Systems, Las Vegas, NV},
  abstract = {This paper discusses a comprehensive framework for modular motor control based on a recently developed theory of dynamic movement primitives (DMP). DMPs are a formulation of movement primitives with autonomous nonlinear differential equations, whose time evolution creates smooth kinematic control policies. Model-based control theory is used to convert the outputs of these policies into motor commands. By means of coupling terms, on-line modifications can be incorporated into the time evolution of the differential equations, thus providing a rather flexible and reactive framework for motor planning and execution. The linear parameterization of DMPs lends itself naturally to supervised learning from demonstration. Moreover, the temporal, scale, and translation invariance of the differential equations with respect to these parameters provides a useful means for movement recognition. A novel reinforcement learning technique based on natural stochastic policy gradients allows a general approach of improving DMPs by trial and error learning with respect to almost arbitrary optimization criteria. We demonstrate the different ingredients of the DMP approach in various examples, involving skill learning from demonstration on the humanoid robot DB, and learning biped walking from demonstration in simulation, including self-improvement of the movement patterns towards energy efficiency through resonance tuning.},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/schaal_et_al_2003_control,_planning,_learning,_and_imitation_with_dynamic_movement_primitives.pdf}
}

@unpublished{schaulPrioritizedExperienceReplay2016,
  title = {Prioritized {{Experience Replay}}},
  author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  date = {2016-02-25},
  eprint = {1511.05952},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1511.05952},
  urldate = {2020-07-02},
  abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
  keywords = {#nosource,Computer Science - Machine Learning,DQL,model-free,reinforcement learning},
  annotation = {00000}
}

@article{schaulUniversalValueFunction,
  title = {Universal {{Value Function Approximators}}},
  author = {Schaul, Tom and Horgan, Dan and Gregor, Karol and Silver, David},
  pages = {9},
  abstract = {Value functions are a core component of reinforcement learning systems. The main idea is to to construct a single function approximator V (s; θ) that estimates the long-term reward from any state s, using parameters θ. In this paper we introduce universal value function approximators (UVFAs) V (s, g; θ) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals.},
  langid = {english},
  keywords = {#nosource,multitask RL,reinforcement learning,transfer learning}
}

@inproceedings{schindlbeckUnifiedPassivitybasedCartesian2015,
  title = {Unified Passivity-Based {{Cartesian}} Force/Impedance Control for Rigid and Flexible Joint Robots via Task-Energy Tanks},
  booktitle = {2015 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Schindlbeck, Christopher and Haddadin, Sami},
  date = {2015-05},
  pages = {440--447},
  issn = {1050-4729},
  doi = {10.1109/ICRA.2015.7139036},
  abstract = {In this paper we propose a novel hybrid Cartesian force/impedance controller that is equipped with energy tanks to preserve passivity. Our approach overcomes the problems of (hybrid) force control, impedance control, and set-point based indirect force control. It allows accurate force tracking, full compliant impedance behavior, and safe contact resemblance simultaneously by introducing a controller shaping function that robustly handles unexpected contact loss and avoids chattering behavior that switching based approaches suffer from. Furthermore, we propose a constructive way of initiating the energy tanks via the concept of task energy. This represents an estimate of the energy consumption of a given force control task prior to execution. The controller can be applied to both rigid body and flexible joint dynamics. To show the validity of our approach, several simulations and experiments with the KUKA/DLR LWR-III are carried out.},
  eventtitle = {2015 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  keywords = {Force,Force control,Impedance,Joints,READ,Robot kinematics,STABILITY,Stability analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/schindlbeck_haddadin_2015_unified_passivity-based_cartesian_force-impedance_control_for_rigid_and.pdf}
}

@unpublished{schulmanEquivalencePolicyGradients2018,
  title = {Equivalence {{Between Policy Gradients}} and {{Soft Q-Learning}}},
  author = {Schulman, John and Chen, Xi and Abbeel, Pieter},
  date = {2018-10-14},
  eprint = {1704.06440},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1704.06440},
  urldate = {2020-07-02},
  abstract = {Two of the leading approaches for model-free reinforcement learning are policy gradient methods and \$Q\$-learning methods. \$Q\$-learning methods can be effective and sample-efficient when they work, however, it is not well-understood why they work, since empirically, the \$Q\$-values they estimate are very inaccurate. A partial explanation may be that \$Q\$-learning methods are secretly implementing policy gradient updates: we show that there is a precise equivalence between \$Q\$-learning and policy gradient methods in the setting of entropy-regularized reinforcement learning, that "soft" (entropy-regularized) \$Q\$-learning is exactly equivalent to a policy gradient method. We also point out a connection between \$Q\$-learning methods and natural policy gradient methods. Experimentally, we explore the entropy-regularized versions of \$Q\$-learning and policy gradients, and we find them to perform as well as (or slightly better than) the standard variants on the Atari benchmark. We also show that the equivalence holds in practical settings by constructing a \$Q\$-learning method that closely matches the learning dynamics of A3C without using a target network or \$\textbackslash epsilon\$-greedy exploration schedule.},
  keywords = {#nosource,Computer Science - Machine Learning,model-free,policy gradients,QL,reinforcement learning}
}

@unpublished{schulmanHighDimensionalContinuousControl2018,
  title = {High-{{Dimensional Continuous Control Using Generalized Advantage Estimation}}},
  author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  date = {2018-10-20},
  eprint = {1506.02438},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1506.02438},
  urldate = {2020-07-02},
  abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
  keywords = {#nosource,Computer Science - Machine Learning,Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control,model-free,policy gradients,policy optimization,reinforcement learning},
  annotation = {00884}
}

@unpublished{schulmanProximalPolicyOptimization2017,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  date = {2017-08-28},
  eprint = {1707.06347},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1707.06347},
  urldate = {2020-07-02},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  keywords = {#nosource,Computer Science - Machine Learning,model-free,policy gradients,policy optimization,reinforcement learning},
  annotation = {00000}
}

@unpublished{schulmanTrustRegionPolicy2017,
  title = {Trust {{Region Policy Optimization}}},
  author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
  date = {2017-04-20},
  eprint = {1502.05477},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1502.05477},
  urldate = {2020-07-02},
  abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  keywords = {#nosource,Computer Science - Machine Learning,model-free,policy gradients,policy optimization,reinforcement learning},
  annotation = {00000}
}

@book{schwaba.l.MultibodyDynamics2018,
  title = {Multibody Dynamics {{B}}},
  author = {Schwab,A.L.},
  date = {2018},
  keywords = {#nosource,robotic grasping,tno internship},
  annotation = {00000}
}

@article{schwarzRGBDObjectDetection2018,
  title = {{{RGB-D}} Object Detection and Semantic Segmentation for Autonomous Manipulation in Clutter},
  author = {Schwarz, Max and Milan, Anton and Periyasamy, Arul Selvam and Behnke, Sven},
  date = {2018-04-01},
  journaltitle = {The International Journal of Robotics Research},
  shortjournal = {The International Journal of Robotics Research},
  volume = {37},
  number = {4-5},
  pages = {437--451},
  issn = {0278-3649},
  doi = {10.1177/0278364917713117},
  url = {https://doi.org/10.1177/0278364917713117},
  urldate = {2019-05-09},
  abstract = {Autonomous robotic manipulation in clutter is challenging. A large variety of objects must be perceived in complex scenes, where they are partially occluded and embedded among many distractors, often in restricted spaces. To tackle these challenges, we developed a deep-learning approach that combines object detection and semantic segmentation. The manipulation scenes are captured with RGB-D cameras, for which we developed a depth fusion method. Employing pretrained features makes learning from small annotated robotic datasets possible. We evaluate our approach on two challenging datasets: one captured for the Amazon Picking Challenge 2016, where our team NimbRo came in second in the Stowing and third in the Picking task; and one captured in disaster-response scenarios. The experiments show that object detection and semantic segmentation complement each other and can be combined to yield reliable object perception.},
  langid = {english},
  keywords = {#nosource,object grasping,robotic grasping,tno internship},
  annotation = {00032}
}

@inproceedings{scibiliaSelfAdaptiveRobotControl2018,
  title = {A {{Self-Adaptive Robot Control Framework}} for {{Improved Tracking}} and {{Interaction Performances}} in {{Low-Stiffness Teleoperation}}},
  booktitle = {2018 {{Ieee-Ras}} 18th {{International Conference}} on {{Humanoid Robots}} (Humanoids)},
  author = {Scibilia, Adriano and Laghi, Marco and De Momi, Elena and Peternel, Luka and Ajoudani, Arash},
  editor = {Asfour, T.},
  date = {2018},
  pages = {628--634},
  publisher = {{Ieee}},
  location = {{New York}},
  issn = {2164-0572},
  url = {https://ieeexplore.ieee.org/document/8625062},
  urldate = {2022-05-02},
  abstract = {The improved adaptability of a robotic teleoperation system to unexpected disturbances in remote environments can be achieved by compliance control. Nevertheless, complying with all types of interaction forces while performing realistic manipulation tasks may deteriorate the teleoperation performance. For instance, the loading effect of the objects and tools that are held and manipulated by the robot can introduce undesired deviations from the reference trajectories in case of low-stiffness (or high payload) teleoperation. Although this can be addressed by updating the robot dynamics with the external loading effect, a sudden loss of the object may also generate undesired and potentially dangerous robot behaviours. To address this problem, we propose a novel and self-adaptive teleoperation framework. The method uses the feedback from robot's force sensors to recognize the interaction aspects that must be compensated by robot dynamics. Thanks to this online compensation, the slave robot reduces the tracking error with respect to the commanded motion by the human operator, while performing complex interactive tasks without the haptic feedback. The robot local controller also includes an energy tank based passivity paradigm to be able to manage unexpected collisions or a contact loss without resulting in an unsafe behaviour. We validate the proposed approach by experiments on a torque-controlled robotic arm performing manipulation tasks that require both object manipulation and environment interaction.},
  isbn = {978-1-5386-7283-9},
  langid = {english},
  keywords = {manipulation,STABILITY},
  annotation = {WOS:000458689700090},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/scibilia_et_al_2018_a_self-adaptive_robot_control_framework_for_improved_tracking_and_interaction.pdf}
}

@book{secchiControlInteractiveRobotic2007,
  title = {Control of Interactive Robotic Interfaces: {{A}} Port-{{Hamiltonian}} Approach},
  shorttitle = {Control of Interactive Robotic Interfaces},
  author = {Secchi, Cristian and Stramigioli, Stefano and Fantuzzi, Cesare},
  date = {2007},
  volume = {29},
  publisher = {{Springer}},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/secchi_et_al_2007_control_of_interactive_robotic_interfaces.pdf}
}

@inproceedings{secchiEnergyOptimizationRobust2019,
  title = {Energy Optimization for a {{Robust}} and {{Flexible Interaction Control}}},
  booktitle = {2019 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Secchi, Cristian and Ferraguti, Federica},
  date = {2019-05},
  pages = {1919--1925},
  issn = {2577-087X},
  doi = {10.1109/ICRA.2019.8794055},
  abstract = {The possibility of adapting online the way a robot interacts with the environment is becoming more and more important. In this paper we introduce the tank based admittance controller. We show that all the admittance controllers can be modeled as an energy optimization problem and then we introduce a novel admittance control strategy that allows to change online the interactive behavior while preserving a stable interaction with the environment. The effectiveness of the proposed architecture is experimentally validated.},
  eventtitle = {2019 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  keywords = {Admittance,Buildings,Collaboration,Dynamics,Manipulators,Optimization,READ,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/secchi_ferraguti_2019_energy_optimization_for_a_robust_and_flexible_interaction_control.pdf}
}

@inproceedings{secchiPositionDriftCompensation2006,
  title = {Position {{Drift Compensation}} in {{Port-Hamiltonian Based Telemanipulation}}},
  booktitle = {2006 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Secchi, Cristian and Stramigioli, Stefano and Fantuzzi, Cesare},
  date = {2006-10},
  pages = {4211--4216},
  issn = {2153-0866},
  doi = {10.1109/IROS.2006.281915},
  abstract = {Passivity based bilateral telemanipulation schemes are often subject to a position drift between master and slave if the communication channel is implemented using scattering variables. The magnitude of this position mismatch can be significant during interaction tasks. In this paper we propose a passivity preserving scheme for compensating the position drift arising during contact tasks in port-Hamiltonian based telemanipulation improving the kinematic perception of the remote environment felt by the human operator},
  eventtitle = {2006 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  keywords = {Communication channels,Communication system control,Control systems,Delay,Humans,Kinematics,Master-slave,Robots,Scattering,Shape control,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/secchi_et_al_2006_position_drift_compensation_in_port-hamiltonian_based_telemanipulation.pdf}
}

@inproceedings{selvaggioPassiveTaskPrioritizedSharedControl2019,
  title = {Passive {{Task-Prioritized Shared-Control Teleoperation}} with {{Haptic Guidance}}},
  booktitle = {2019 {{International Conference}} on {{Robotics}} and {{Automation}} (Icra)},
  author = {Selvaggio, M. and Giordano, P. Robuffo and Ficuciello, F. and Siciliano, B.},
  editor = {Howard, A. and Althoefer, K. and Arai, F. and Arrichiello, F. and Caputo, B. and Castellanos, J. and Hauser, K. and Isler, V. and Kim, J. and Liu, H. and Oh, P. and Santos, V. and Scaramuzza, D. and Ude, A. and Voyles, R. and Yamane, K. and Okamura, A.},
  date = {2019},
  pages = {430--436},
  publisher = {{Ieee}},
  location = {{New York}},
  issn = {1050-4729},
  url = {https://hal.inria.fr/hal-02051476/file/ICRA19_2568_FI.pdf},
  urldate = {2022-08-09},
  abstract = {Robot teleoperation is widely used for several hazardous applications. To increase teleoperator capabilities shared-control methods can be employed. In this paper, we present a passive task-prioritized shared-control method for remote telemanipulation of redundant robots. The proposed method fuses the task-prioritized control architecture with haptic guidance techniques to realize a shared-control framework for teleoperation systems. To preserve the semi-autonomous telerobotic system safety, passivity is analyzed and an energy-tanks passivity-based controller is developed. The proposed theoretical results are validated through experiments involving a real haptic device and a simulated slave robot.},
  isbn = {978-1-5386-6026-3},
  langid = {english},
  keywords = {autonomy,robot manipulators,semiautonomous teleoperation,space},
  annotation = {WOS:000494942300052},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/selvaggio_et_al_2019_passive_task-prioritized_shared-control_teleoperation_with_haptic_guidance.pdf}
}

@article{selvaggioPassiveVirtualFixtures2018,
  title = {Passive {{Virtual Fixtures Adaptation}} in {{Minimally Invasive Robotic Surgery}}},
  author = {Selvaggio, Mario and Fontanelli, Giuseppe Andrea and Ficuciello, Fanny and Villani, Luigi and Siciliano, Bruno},
  date = {2018-10},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {3},
  number = {4},
  pages = {3129--3136},
  issn = {2377-3766},
  doi = {10.1109/LRA.2018.2849876},
  abstract = {During robot-aided surgical interventions, the surgeon can be benefitted from the application of virtual fixtures (VFs). Though very effective, this technique is very often not practicable in unstructured surgical environments. In order to comply with the environmental deformation, both the VF geometry and the constraint enforcement parameters need to be online defined/adapted. This letter proposes a strategy for an effective use of VF assistance in minimally invasive robotic surgical tasks. An online VF generation technique based on the interaction force measurements is presented. Pose and geometry adaptations of the VF are considered. Passivity of the overall system is guaranteed by using energy tanks passivity-based control. The proposed method is validated through experiments on the da Vinci Research Kit.},
  eventtitle = {{{IEEE Robotics}} and {{Automation Letters}}},
  keywords = {compliance and impedance control,Force,Geometry,Impedance,laparoscopy,physical human-robot interaction,Robots,Splines (mathematics),STABILITY,Surgery,Surgical robotics,Task analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/selvaggio_et_al_2018_passive_virtual_fixtures_adaptation_in_minimally_invasive_robotic_surgery.pdf}
}

@inproceedings{selvaggioVisionBasedVirtual2016,
  title = {Vision Based Virtual Fixture Generation for Teleoperated Robotic Manipulation},
  booktitle = {2016 {{International Conference}} on {{Advanced Robotics}} and {{Mechatronics}} ({{ICARM}})},
  author = {Selvaggio, Mario and Chen, Fei and Gao, Boyang and Notomista, Gennaro and Trapani, Francesco and Caldwell, Darwin},
  date = {2016-08},
  pages = {190--195},
  doi = {10.1109/ICARM.2016.7606917},
  abstract = {In this paper we present a vision-based system for online virtual fixture generation suitable for manipulation tasks using remote controlled robots. This system makes use of a stereo camera system which provides accurate pose estimation of parts within the surrounding environment of the robot using features detection algorithms. The proposed approach is suitable for fast adaptation of the teleoperation system to different manipulation tasks without the need of tedious reimplementation of virtual constraints. Our main goal is to improve the efficiency of bilateral teleoperation systems by reducing the human operator effort in programming the system. In fact, using this method virtual guidances do not need to be programmed a priori but they can be instead dynamically generated on-the-fly and updated at any time making, in the end, the system suitable for any unstructured environment. In addition, this methodology is easily adaptable to any kind of teleoperation system since it is independent from the used master/slave robots. In order to validate our approach we performed a series of experiments in an emulated industrial scenario. We show how through the use of our approach a generic telemanipulation task can be easily accomplished without influencing the transparency of the system.},
  eventtitle = {2016 {{International Conference}} on {{Advanced Robotics}} and {{Mechatronics}} ({{ICARM}})},
  keywords = {enhanced telemanipulation,Feature extraction,Force,Haptic interfaces,human-in-the-loop,Robot kinematics,Robot sensing systems,shared control,Three-dimensional displays,vision-based virtual fixtures generation},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/selvaggio_et_al_2016_vision_based_virtual_fixture_generation_for_teleoperated_robotic_manipulation.pdf}
}

@inproceedings{senaImprovingTaskParameterisedMovement2019,
  title = {Improving {{Task-Parameterised Movement Learning Generalisation}} with {{Frame-Weighted Trajectory Generation}}},
  booktitle = {2019 {{Ieee}}/{{Rsj International Conference}} on {{Intelligent Robots}} and {{Systems}} (Iros)},
  author = {Sena, Aran and Michael, Brendan and Howard, Matthew},
  date = {2019},
  pages = {4281--4287},
  publisher = {{Ieee}},
  location = {{New York}},
  issn = {2153-0858},
  url = {http://arxiv.org/pdf/1903.01240},
  urldate = {2022-12-02},
  abstract = {Learning from Demonstration depends on a robot learner generalising its learned model to unseen conditions. Generally, it is not feasible for a person to provide demonstrations that account for all possible variations in non-trivial tasks. While there are many learning methods that can handle interpolation of observed data effectively, extrapolation from observed data offers a much greater challenge. To address this problem, this paper proposes a modified Task-Parameterised Gaussian Mixture Regression method that considers the relevance of task parameters during trajectory generation, as determined by variance in the data. The benefits of the proposed method are first explored using a simulated reaching task data set. Here it is shown that the proposed method offers far-reaching, low-error extrapolation abilities that are different in nature to existing learning methods. Data collected from novice users for a real-world manipulation task is then considered, where it is shown that the proposed method is able to effectively reduce grasping performance errors by similar to 40\% and extrapolate to unseen grasp targets under real-world conditions. These results indicate the proposed method can benefit novice users by placing less reliance on the user to provide high quality demonstration data sets.},
  isbn = {978-1-72814-004-9},
  langid = {english},
  keywords = {READ},
  annotation = {WOS:000544658403072},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/sena_et_al_2019_improving_task-parameterised_movement_learning_generalisation_with.pdf}
}

@book{sepulchreConstructiveNonlinearControl2012,
  title = {Constructive Nonlinear Control},
  author = {Sepulchre, Rodolphe and Jankovic, Mrdjan and Kokotovic, Petar V.},
  date = {2012},
  publisher = {{Springer Science \& Business Media}},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/sepulchre_et_al_2012_constructive_nonlinear_control.pdf}
}

@inproceedings{serajiForceTrackingImpedance1993,
  title = {Force Tracking in Impedance Control},
  booktitle = {[1993] {{Proceedings IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Seraji, H. and Colbaugh, R.},
  date = {1993-05},
  pages = {499-506 vol.2},
  doi = {10.1109/ROBOT.1993.291908},
  abstract = {Two simple and computationally efficient schemes for force tracking using impedance control are presented. The schemes generate the reference position trajectory required to produce a desired contact force despite lack of knowledge of the environmental stiffness and location. The first scheme uses direct adaptive control to generate the reference position on-line as a function of the force tracking-error. The second scheme utilizes an indirect adaptive strategy in which the environmental parameters are estimated on-line, and the required reference position is computed based on these estimates. Simulation studies are presented for a 7-degrees-of-freedom (DOF) robotics research arm using full arm dynamics. It is shown that the adaptive schemes are able to compensate for uncertainties in both the environmental stiffness and location, so that the end-effector applies the desired contact force while exhibiting the specified impedance dynamics.{$<>$}},
  eventtitle = {[1993] {{Proceedings IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  keywords = {Automatic control,Computational modeling,Control systems,Force control,Force measurement,Impedance,Manipulator dynamics,Motion control,Robots,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/seraji_colbaugh_1993_force_tracking_in_impedance_control.pdf}
}

@inproceedings{shahriariAdaptingContactsEnergy2017,
  title = {Adapting to Contacts: {{Energy}} Tanks and Task Energy for Passivity-Based Dynamic Movement Primitives},
  shorttitle = {Adapting to Contacts},
  booktitle = {2017 {{IEEE-RAS}} 17th {{International Conference}} on {{Humanoid Robotics}} ({{Humanoids}})},
  author = {Shahriari, Erfan and Kramberger, Aljaž and Gams, Andrej and Ude, Aleš and Haddadin, Sami},
  date = {2017-11},
  pages = {136--142},
  issn = {2164-0580},
  doi = {10.1109/HUMANOIDS.2017.8239548},
  abstract = {In this paper, we develop a framework to encode demonstrated trajectories as periodic dynamic motion primitives (DMP) for an impedance-controlled robot and their modification to fulfil the task objective, i. e. to adapt based on the force feedback and encoded desired wrench profile via an admittance controller. This behavior by itself can violate stability. Therefore, a passivity analysis for the whole system is presented, and based on input power ports and the demonstrated reference power, a passivity observer (PO) is designed. Subsequently, a DMP phase altering law is introduced according to the passivity criterion in order to adjust the phase based on the passivity criterion. However, since this does not necessarily guarantee passivity, a suitable virtual energy tank is used. Experimental results on a Kuka LWR-4 robot polishing an unknown surface underline the real world applicability the suggested controller.},
  eventtitle = {2017 {{IEEE-RAS}} 17th {{International Conference}} on {{Humanoid Robotics}} ({{Humanoids}})},
  keywords = {adaptation,Admittance,Encoding,framework,Impedance,impedance control,IMPORTANT,Joint impedance,manipulators,motion,READ,robot,Robots,stability,STABILITY,Stability analysis,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/shahriari_et_al_2017_adapting_to_contacts.pdf}
}

@article{shahriariPassivityBasedAdaptiveForceImpedance2022,
  title = {Passivity-{{Based Adaptive Force-Impedance Control}} for {{Modular Multi-Manual Object Manipulation}}},
  author = {Shahriari, Erfan and Birjandi, Seyed Ali Baradaran and Haddadin, Sami},
  date = {2022-04},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {2},
  pages = {2194--2201},
  issn = {2377-3766},
  doi = {10.1109/LRA.2022.3142903},
  abstract = {There exist robotic tasks such as cumbersome object manipulation for which one arm alone is not sufficient and a team of robots should be employed. For such a multi-manual system, a control policy must be defined to meet the task objective. This letter presents a modular uni-arm control law for a multi-manual object manipulation task. Following the concept of force and impedance control, a novel adaptation policy is introduced which mitigates the intrinsic limitations of conventional methods such as the coupled impedance and hybrid position/force control approaches. To ensure the stability of the proposed control algorithm, passivity analysis is performed and a virtual energy tank is augmented to the multi-manual system. Finally, a set of experiments with a bi-manual setup manipulating an object is carried out to evaluate the performance of the proposed control approach.},
  eventtitle = {{{IEEE Robotics}} and {{Automation Letters}}},
  keywords = {Couplings,End effectors,Force,Impedance,impedance control,Multi-robot systems,object manipulation,READ,Robots,Task analysis,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/shahriari_et_al_2022_passivity-based_adaptive_force-impedance_control_for_modular_multi-manual.pdf}
}

@article{shahriariPowerFlowRegulation2020,
  title = {Power {{Flow Regulation}}, {{Adaptation}}, and {{Learning}} for {{Intrinsically Robust Virtual Energy Tanks}}},
  author = {Shahriari, Erfan and Johannsmeier, Lars and Jensen, Elisabeth and Haddadin, Sami},
  date = {2020-01},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {5},
  number = {1},
  pages = {211--218},
  issn = {2377-3766},
  doi = {10.1109/LRA.2019.2953662},
  abstract = {Ideally, a robot controller should not only be designed to exhibit a given interaction behavior under controlled conditions, but also to be robust to changes e.g. in the environment. Within the paradigm of virtual energy tanks for passivity-based controls, robustness may be provided by setting absolute limits on the tank energy. However, an energy limit alone does not prevent a sudden drain of the tank, which may result in a sudden increase of potentially problematic, passivity-violating energy somewhere in the system. In this letter, we tackle this problem by regulating the exchanged power between the energy tank and the system according to a reference power trajectory. We propose a method to encode this trajectory and to conservatively learn the corresponding parameters. The resulting system is adaptable and robust to both predicted and unpredicted changes, either in the environment or the system. Experimental results with a Franka Emika Panda robot performing an exemplary force-based interaction task validate the performance improvement with our method.},
  eventtitle = {{{IEEE Robotics}} and {{Automation Letters}}},
  keywords = {Cartesian impedance,Convergence,force control,Force control,Load flow,motion control,Motion control,READ,Robot control,STABILITY,stability analysis,Stability analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/shahriari_et_al_2020_power_flow_regulation,_adaptation,_and_learning_for_intrinsically_robust.pdf}
}

@inproceedings{shahriariValvebasedVirtualEnergy2018,
  title = {Valve-Based {{Virtual Energy Tanks}}: {{A Framework}} to {{Simultaneously Passify Controls}} and {{Embed Control Objectives}}},
  shorttitle = {Valve-Based {{Virtual Energy Tanks}}},
  booktitle = {2018 {{Annual American Control Conference}} ({{ACC}})},
  author = {Shahriari, Erfan and Johannsmeier, Lars and Haddadin, Sami},
  date = {2018-06},
  pages = {3634--3641},
  issn = {2378-5861},
  doi = {10.23919/ACC.2018.8431718},
  abstract = {In this paper, we show how virtual energy tanks can be used for passivity-based control schemes beyond ensuring passivity of the system, but respecting power-related control objectives. For this, we introduce the concept of valve-based virtual energy tanks. We show how these can be designed to handle several potentially non-passive ports based on their associated task priorities. Moreover, several practically useful policies are introduced for tank design. Finally, as a case study, a common potentially non-passive robotics system is elaborated to which a thorough passivity analysis as well as valve-based tank augmentation is applied. Several experiments with a force-impedance controlled robot in interaction with the environment underline the real world applicability of our approach.},
  eventtitle = {2018 {{Annual American Control Conference}} ({{ACC}})},
  keywords = {Cartesian impedance,Control design,Pins,READ,Robots,STABILITY,Stability analysis,Task analysis,Valves},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/shahriari_et_al_2018_valve-based_virtual_energy_tanks.pdf}
}

@article{sharifiImpedanceLearningBasedAdaptive2021,
  title = {Impedance {{Learning-Based Adaptive Control}} for {{Human-Robot Interaction}}},
  author = {Sharifi, Mojtaba and Azimi, Vahid and Mushahwar, Vivian K. and Tavakoli, Mahdi},
  date = {2021},
  journaltitle = {Ieee Transactions on Control Systems Technology},
  shortjournal = {IEEE Trans. Control Syst. Technol.},
  publisher = {{Ieee-Inst Electrical Electronics Engineers Inc}},
  location = {{Piscataway}},
  issn = {1063-6536},
  doi = {10.1109/TCST.2021.3107483},
  url = {https://ieeexplore.ieee.org/document/9531394},
  urldate = {2022-05-02},
  abstract = {In this article, a new learning-based time-varying impedance controller is proposed and tested to facilitate an autonomous physical human-robot interaction (pHRI). Novel adaptation laws are formulated for online adjustment of robot impedance based on human behavior. Two other sets of update rules are defined for intelligent coping with the robot's structured and unstructured uncertainties. These rules ensure stability via Lyapunov's theorem and provide uniform ultimate boundedness (UUB) of the closed-loop system's response, without a need for HRI force/torque measurement. Accordingly, the convergence of response signals, including errors in tracking, online impedance learning, robot parameter adaptation, and controller gain variation, is proven to operate in a bounded region (compact set) in the presence of robot and human uncertainties and bounded disturbances. The performance of the developed intelligent impedance-varying control strategy is investigated through comprehensive experimental studies in a repetitive following task with a moving target.},
  langid = {english},
  keywords = {adaptation,adaptive control,Autonomous impedance variation,design,Dynamics,force,Force,framework,Impedance,manipulators,motion,nonlinear   adaptive control,nonlinear adaptive control,physical human-robot interaction (pHRI),physical human–robot interaction (pHRI),robot   stability,Robot sensing systems,robot stability,Robots,stability,STABILITY,Stability analysis,systems,Task analysis,teleoperation,uniform ultimate boundedness (UUB),uniform ultimate boundedness (UUB).},
  annotation = {WOS:000732363200001},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/sharifi_et_al_2021_impedance_learning-based_adaptive_control_for_human-robot_interaction.pdf;/home/ricks/Zotero/storage/4WGZ2RMW/Sharifi et al. - 2022 - Impedance Learning-Based Adaptive Control for Huma.pdf}
}

@article{sharifiImpedanceVariationLearning2021,
  title = {Impedance {{Variation}} and {{Learning Strategies}} in {{Human-Robot Interaction}}},
  author = {Sharifi, Mojtaba and Zakerimanesh, Amir and Mehr, Javad K. and Torabi, Ali and Mushahwar, Vivian K. and Tavakoli, Mahdi},
  date = {2021},
  journaltitle = {IEEE Transactions on Cybernetics},
  pages = {1--14},
  issn = {2168-2275},
  doi = {10.1109/TCYB.2020.3043798},
  abstract = {In this survey, various concepts and methodologies developed over the past two decades for varying and learning the impedance or admittance of robotic systems that physically interact with humans are explored. For this purpose, the assumptions and mathematical formulations for the online adjustment of impedance models and controllers for physical human-robot interaction (HRI) are categorized and compared. In this systematic review, studies on: 1) variation and 2) learning of appropriate impedance elements are taken into account. These strategies are classified and described in terms of their objectives, points of view (approaches), and signal requirements (including position, HRI force, and electromyography activity). Different methods involving linear/nonlinear analyses (e.g., optimal control design and nonlinear Lyapunov-based stability guarantee) and the Gaussian approximation algorithms (e.g., Gaussian mixture model-based and dynamic movement primitives-based strategies) are reviewed. Current challenges and research trends in physical HRI are finally discussed.},
  eventtitle = {{{IEEE Transactions}} on {{Cybernetics}}},
  keywords = {Damping,End effectors,Force,Human-robot interaction (HRI),Human–robot interaction (HRI),Impedance,impedance and admittance models,impedance control,impedance learning,impedance variation,REVIEW,Robot kinematics,robot learning,robot stability,Robots,Task analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/sharifi_et_al_2021_impedance_variation_and_learning_strategies_in_human-robot_interaction.pdf;/home/ricks/Zotero/storage/Q5LF3BPB/Sharifi et al. - 2022 - Impedance Variation and Learning Strategies in Hum.pdf}
}

@article{sharkawyHumanRobotInteraction2022,
  title = {Human–{{Robot Interaction}}: {{A Review}} and {{Analysis}} on {{Variable Admittance Control}}, {{Safety}}, and {{Perspectives}}},
  shorttitle = {Human–{{Robot Interaction}}},
  author = {Sharkawy, Abdel-Nasser and Koustoumpardis, Panagiotis N.},
  date = {2022-07},
  journaltitle = {Machines},
  volume = {10},
  number = {7},
  pages = {591},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2075-1702},
  doi = {10.3390/machines10070591},
  url = {https://www.mdpi.com/2075-1702/10/7/591},
  urldate = {2022-08-11},
  abstract = {Human–robot interaction (HRI) is a broad research topic, which is defined as understanding, designing, developing, and evaluating the robotic system to be used with or by humans. This paper presents a survey on the control, safety, and perspectives for HRI systems. The first part of this paper reviews the variable admittance (VA) control for human–robot co-manipulation tasks, where the virtual damping, inertia, or both are adjusted. An overview of the published research for the VA control approaches, their methods, the accomplished collaborative co-manipulation tasks and applications, and the criteria for evaluating them are presented and compared. Then, the performance of various VA controllers is compared and investigated. In the second part, the safety of HRI systems is discussed. The various methods for detection of human–robot collisions (model-based and data-based) are investigated and compared. Furthermore, the criteria, the main aspects, and the requirements for the determination of the collision and their thresholds are discussed. The performance measure and the effectiveness of each method are analyzed and compared. The third and final part of the paper discusses the perspectives, necessity, influences, and expectations of the HRI for future robotic systems.},
  issue = {7},
  langid = {english},
  keywords = {human–robot interaction,review,REVIEW,robot control,robot safety,safety methods,variable admittance control},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/sharkawy_koustoumpardis_2022_human–robot_interaction.pdf}
}

@article{sharkawySurveyApplicationsHumanRobot2021,
  title = {A {{Survey}} on {{Applications}} of {{Human-Robot Interaction}}},
  author = {Sharkawy, Abdel-Nasser},
  date = {2021},
  volume = {251},
  number = {4},
  pages = {9},
  abstract = {Human-robot interaction (HRI) is the extensive research topic which aims at the complementary combination between the robot capabilities and human skills. The robots assist humans in terms of precision, speed, and force. The humans contribute in terms of the experience, knowledge of executing the task, intuition, and easy adaptation and learning, and understanding of control strategies. In this manuscript, a survey on the applications of HRI is presented. These applications could be industrial, medical, agricultural, servical, and educational. HRI can be found in industrial applications in handling of the workpieces in the production lines, comanipulation tasks, welding processes, parts assembly, and painting. Assistive robotics are one from the highest profile areas in HRI. For people with the physical and the mental challenges, the robots can provide the opportunity of interaction and therapy. Furthermore, HRI can be widely applied in hospitals. Nowadays, HRI is very important to fight against the new coronavirus (COVID-19) pandemic. In agriculture, the robot is able to help humans with many tasks such as harvesting, seeding, fertilizing, spraying, weed detection, hauling, and mowing. HRI is quite new in the field of education. However, the robot can help children in classrooms for learning processes. In addition, the robots can help young children empathy and social skills. HRI can be found in other applications such as home use, inventory management, mining, Space exploration, and UAVs.},
  langid = {english},
  keywords = {REVIEW},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/sharkawy_2021_a_survey_on_applications_of_human-robot_interaction.pdf}
}

@article{shavitLearningAugmentedJointSpace2018,
  title = {Learning {{Augmented Joint-Space Task-Oriented Dynamical Systems}}: {{A Linear Parameter Varying}} and {{Synergetic Control Approach}}},
  shorttitle = {Learning {{Augmented Joint-Space Task-Oriented Dynamical Systems}}},
  author = {Shavit, Yonadav and Figueroa, Nadia and Salehian, Seyed Sina Mirrazavi and Billard, Aude},
  date = {2018-07},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {3},
  number = {3},
  pages = {2718--2725},
  issn = {2377-3766},
  doi = {10.1109/LRA.2018.2833497},
  abstract = {In this letter, we propose an asymptotically stable joint-space dynamical system (DS) that captures desired behaviors in joint-space while converging toward a task-space attractor in both position and orientation. To encode joint-space behaviors while meeting the stability criteria, we propose a DS constructed as a linear parameter varying system combining different behavior synergies and provide a method for learning these synergy matrices from demonstrations. Specifically, we use dimensionality reduction to find a low-dimensional embedding space for modulating joint synergies, and then estimate the parameters of the corresponding synergies by solving a convex semidefinite optimization problem that minimizes the joint velocity prediction error from the demonstrations. Our proposed approach is empirically validated on a variety of motions that reach a target in position and orientation, while following a desired joint-space behavior.},
  eventtitle = {{{IEEE Robotics}} and {{Automation Letters}}},
  keywords = {Aerospace electronics,Asymptotic stability,Gaussian mixture models,Jacobian matrices,joint-space control,kinematics,Kinematics,learning from demonstration,Motion control,READ,Robots,STABILITY,Task analysis,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/shavit_et_al_2018_learning_augmented_joint-space_task-oriented_dynamical_systems.pdf}
}

@online{shawConstrainedDynamicMovement2022,
  title = {Constrained {{Dynamic Movement Primitives}} for {{Safe Learning}} of {{Motor Skills}}},
  author = {Shaw, Seiji and Jha, Devesh K. and Raghunathan, Arvind and Corcodel, Radu and Romeres, Diego and Konidaris, George and Nikovski, Daniel},
  date = {2022-09-28},
  eprint = {2209.14461},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2209.14461},
  url = {http://arxiv.org/abs/2209.14461},
  urldate = {2022-10-30},
  abstract = {Dynamic movement primitives are widely used for learning skills which can be demonstrated to a robot by a skilled human or controller. While their generalization capabilities and simple formulation make them very appealing to use, they possess no strong guarantees to satisfy operational safety constraints for a task. In this paper, we present constrained dynamic movement primitives (CDMP) which can allow for constraint satisfaction in the robot workspace. We present a formulation of a non-linear optimization to perturb the DMP forcing weights regressed by locally-weighted regression to admit a Zeroing Barrier Function (ZBF), which certifies workspace constraint satisfaction. We demonstrate the proposed CDMP under different constraints on the end-effector movement such as obstacle avoidance and workspace constraints on a physical robot. A video showing the implementation of the proposed algorithm using different manipulators in different environments could be found here https://youtu.be/hJegJJkJfys.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/shaw_et_al_2022_constrained_dynamic_movement_primitives_for_safe_learning_of_motor_skills.pdf}
}

@article{shawcortezControlBarrierFunctions2021,
  title = {Control {{Barrier Functions}} for {{Mechanical Systems}}: {{Theory}} and {{Application}} to {{Robotic Grasping}}},
  shorttitle = {Control {{Barrier Functions}} for {{Mechanical Systems}}},
  author = {Shaw Cortez, Wenceslao and Oetomo, Denny and Manzie, Chris and Choong, Peter},
  date = {2021-03},
  journaltitle = {IEEE Transactions on Control Systems Technology},
  volume = {29},
  number = {2},
  pages = {530--545},
  issn = {1558-0865},
  doi = {10.1109/TCST.2019.2952317},
  abstract = {Control barrier functions have been demonstrated to be a useful method of ensuring constraint satisfaction for a wide class of controllers. However, the existing results are mostly restricted to continuous-time systems. Mechanical systems, including robots, are typically second-order systems in which the control occurs at the force/torque level. These systems have actuator, velocity, and position constraints (i.e., relative degree two) that are vital for safety and/or task execution. Additionally, mechanical systems are typically controlled digitally as sampled-data systems. The contribution of this article is twofold. The first contribution is the development of novel, robust control barrier functions that ensure constraint satisfaction for sampled-data systems in the presence of model uncertainty and allows for satisfaction of actuator constraints. The second contribution is the application of the proposed method to the challenging problem of robotic grasping in which a robotic hand must ensure that an object remains inside the grasp while manipulating it to the desired reference trajectory. A grasp constraint satisfying controller is proposed that can admit the existing nominal manipulation controllers from the literature while simultaneously ensuring no slip, no overextension (e.g., singular configurations), and no rolling off of the fingertips. Simulation and experimental results validate the proposed control for the robotic hand application.},
  eventtitle = {{{IEEE Transactions}} on {{Control Systems Technology}}},
  keywords = {Constraint satisfaction,control barrier functions,Control systems,control systems-control design,control systems-robot control,engineering-general-mechanical systems,Grasping,Mechanical systems,robotic grasping,Robots,robots-manipulators,Sampled data systems,Sensors,Uncertainty},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/shaw_cortez_et_al_2021_control_barrier_functions_for_mechanical_systems.pdf}
}

@inproceedings{shawRMPsSafeImpedance2022,
  title = {{{RMPs}} for {{Safe Impedance Control}} in {{Contact-Rich Manipulation}}},
  booktitle = {2022 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Shaw, Seiji and Abbatematteo, Ben and Konidaris, George},
  date = {2022},
  pages = {2707--2713},
  publisher = {{IEEE}},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/rmps_for_safe_impedance_control_in_contact-rich_manipulation.pdf}
}

@article{sheridanHumanRobotInteraction2016,
  title = {Human–{{Robot Interaction}}: {{Status}} and {{Challenges}}},
  shorttitle = {Human–{{Robot Interaction}}},
  author = {Sheridan, Thomas B.},
  date = {2016-06-01},
  journaltitle = {Human Factors},
  shortjournal = {Hum Factors},
  volume = {58},
  number = {4},
  pages = {525--532},
  publisher = {{SAGE Publications Inc}},
  issn = {0018-7208},
  doi = {10.1177/0018720816644364},
  url = {https://doi.org/10.1177/0018720816644364},
  urldate = {2022-05-24},
  abstract = {Objective:The current status of human?robot interaction (HRI) is reviewed, and key current research challenges for the human factors community are described.Background:Robots have evolved from continuous human-controlled master?slave servomechanisms for handling nuclear waste to a broad range of robots incorporating artificial intelligence for many applications and under human supervisory control.Methods:This mini-review describes HRI developments in four application areas and what are the challenges for human factors research.Results:In addition to a plethora of research papers, evidence of success is manifest in live demonstrations of robot capability under various forms of human control.Conclusions:HRI is a rapidly evolving field. Specialized robots under human teleoperation have proven successful in hazardous environments and medical application, as have specialized telerobots under human supervisory control for space and repetitive industrial tasks. Research in areas of self-driving cars, intimate collaboration with humans in manipulation tasks, human control of humanoid robots for hazardous environments, and social interaction with robots is at initial stages. The efficacy of humanoid general-purpose robots has yet to be proven.Applications:HRI is now applied in almost all robot tasks, including manufacturing, space, aviation, undersea, surgery, rehabilitation, agriculture, education, package fetch and delivery, policing, and military operations.},
  langid = {english},
  keywords = {human interaction,research needs,REVIEW,robot,supervisory control,teleoperator,telerobot},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/sheridan_2016_human–robot_interaction.pdf}
}

@article{shimogaRobotGraspSynthesis1996,
  title = {Robot {{Grasp Synthesis Algorithms}}: {{A Survey}}},
  shorttitle = {Robot {{Grasp Synthesis Algorithms}}},
  author = {Shimoga, K.B.},
  date = {1996-06},
  journaltitle = {The International Journal of Robotics Research},
  volume = {15},
  number = {3},
  pages = {230--266},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/027836499601500302},
  url = {http://journals.sagepub.com/doi/10.1177/027836499601500302},
  urldate = {2019-05-06},
  abstract = {This article presents a survey of the existing computational algorithms meant for achieving four important properties in autonomous multifingered robotic hands. The four properties are: dexterity, equilibrium, stability, and dynamic behavior The multifingered robotic hands must be controlled so as to possess these properties and hence be able to autonomously perform complex tasks in a way similar to human hands. Existing algorithms to achieve dexterity primarily involve solving an unconstrained linear programming problem where an objective function can be chosen to represent one or more of the currently known dexterity measures. Algorithms to achieve equilibrium also constitute solving a linear programming problem wherein the positivity, friction, and joint torque constraints of all fingers are accounted for while optimizing the internal grasping forces. Stability algorithms aim at achieving positive definite grasp impedance matrices by solving for the required fingertip impedances. This problem reduces to a nonlinear programming problem. Dynamic behavior algorithms determine fingertip impedances, which, when achieved, lead to a desired dynamic behavior. This problem, too, becomes a linear programming problem. If a robotic hand has to acquire any or all of these properties, the corresponding algorithms should become integral parts of the hand control system. These algorithms are collectively referred to in this article as robot grasp synthesis algorithms.},
  langid = {english},
  keywords = {#nosource,robotic grasping,tno internship},
  annotation = {00665}
}

@article{shorClassGlobalMinimum1987,
  title = {Class of Global Minimum Bounds of Polynomial Functions},
  author = {Shor, Naum Z.},
  date = {1987},
  journaltitle = {Cybernetics},
  volume = {23},
  number = {6},
  pages = {731--734},
  publisher = {{Springer}},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/shor_1987_class_of_global_minimum_bounds_of_polynomial_functions.pdf}
}

@article{shuklaCoupledDynamicalSystem2012,
  title = {Coupled Dynamical System Based Arm–Hand Grasping Model for Learning Fast Adaptation Strategies},
  author = {Shukla, Ashwini and Billard, Aude},
  date = {2012-03-01},
  journaltitle = {Robotics and Autonomous Systems},
  shortjournal = {Robotics and Autonomous Systems},
  series = {Autonomous {{Grasping}}},
  volume = {60},
  number = {3},
  pages = {424--440},
  issn = {0921-8890},
  doi = {10.1016/j.robot.2011.07.023},
  url = {https://www.sciencedirect.com/science/article/pii/S0921889011001576},
  urldate = {2022-09-23},
  abstract = {Performing manipulation tasks interactively in real environments requires a high degree of accuracy and stability. At the same time, when one cannot assume a fully deterministic and static environment, one must endow the robot with the ability to react rapidly to sudden changes in the environment. These considerations make the task of reach and grasp difficult to deal with. We follow a Programming by Demonstration (PbD) approach to the problem and take inspiration from the way humans adapt their reach and grasp motions when perturbed. This is in sharp contrast to previous work in PbD that uses unperturbed motions for training the system and then applies perturbation solely during the testing phase. In this work, we record the kinematics of arm and fingers of human subjects during unperturbed and perturbed reach and grasp motions. In the perturbed demonstrations, the target’s location is changed suddenly after the onset of the motion. Data show a strong coupling between the hand transport and finger motions. We hypothesize that this coupling enables the subject to seamlessly and rapidly adapt the finger motion in coordination with the hand posture. To endow our robot with this competence, we develop a coupled dynamical system based controller, whereby two dynamical systems driving the hand and finger motions are coupled. This offers a compact encoding for reach-to-grasp motions that ensures fast adaptation with zero latency for re-planning. We show in simulation and on the real iCub robot that this coupling ensures smooth and “human-like” motions. We demonstrate the performance of our model under spatial, temporal and grasp type perturbations which show that reaching the target with coordinated hand–arm motion is necessary for the success of the task.},
  langid = {english},
  keywords = {Fast perturbations,Grasping,Hand–arm coordination,Manipulation planning,Programming by demonstration,READ,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/shukla_billard_2012_coupled_dynamical_system_based_arm–hand_grasping_model_for_learning_fast.pdf}
}

@unpublished{shyamModelbasedActiveExploration2018,
  title = {Model-Based Active Exploration},
  author = {Shyam, Pranav and Jaśkowski, Wojciech and Gomez, Faustino},
  date = {2018},
  eprint = {1810.12162},
  eprinttype = {arxiv},
  keywords = {#nosource,machine learning control,safe-rl}
}

@book{sicilianoRoboticsModellingPlanning2010,
  title = {Robotics: Modelling, Planning and Control},
  shorttitle = {Robotics},
  author = {Siciliano, Bruno},
  date = {2010},
  abstract = {Robotics provides the know-how on the foundations of robotics: modelling, planning and control. It covers mobile robots, visual control and motion planning. A variety of problems are worked through, and the tools to find engineering solutions are explained.},
  isbn = {978-1-84996-634-4 978-1-84628-641-4},
  langid = {english},
  annotation = {OCLC: 1090652169},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/siciliano_2010_robotics.pdf}
}

@book{sicilianoSpringerHandbookRobotics2016,
  title = {Springer Handbook of Robotics},
  author = {Siciliano, Bruno and Khatib, Oussama},
  date = {2016},
  publisher = {{Springer}},
  keywords = {#nosource,robotic grasping,tno internship},
  annotation = {03198}
}

@article{siCompositeDynamicMovement2021,
  title = {Composite Dynamic Movement Primitives Based on Neural Networks for Human–Robot Skill Transfer},
  author = {Si, Weiyong and Wang, Ning and Yang, Chenguang},
  date = {2021-02-13},
  journaltitle = {Neural Computing and Applications},
  shortjournal = {Neural Comput \& Applic},
  issn = {1433-3058},
  doi = {10.1007/s00521-021-05747-8},
  url = {https://doi.org/10.1007/s00521-021-05747-8},
  urldate = {2022-09-12},
  abstract = {In this paper, composite dynamic movement primitives (DMPs) based on radial basis function neural networks (RBFNNs) are investigated for robots’ skill learning from human demonstrations. The composite DMPs could encode the position and orientation manipulation skills simultaneously for human-to-robot skills transfer. As the robot manipulator is expected to perform tasks in unstructured and uncertain environments, it requires the manipulator to own the adaptive ability to adjust its behaviours to new situations and environments. Since the DMPs can adapt to uncertainties and perturbation, and spatial and temporal scaling, it has been successfully employed for various tasks, such as trajectory planning and obstacle avoidance. However, the existing skill model mainly focuses on position or orientation modelling separately; it is a common constraint in terms of position and orientation simultaneously in practice. Besides, the generalisation of the skill learning model based on DMPs is still hard to deal with dynamic tasks, e.g., reaching a moving target and obstacle avoidance. In this paper, we proposed a composite DMPs-based framework representing position and orientation simultaneously for robot skill acquisition and the neural networks technique is used to train the skill model. The effectiveness of the proposed approach is validated by simulation and experiments.},
  langid = {english},
  keywords = {Composite dynamic movement primitive,Human–robot skill transfer,Learning from demonstration,Position and orientation skill learning framework,Radial basis function NNs (RBFNNs),READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/si_et_al_2021_composite_dynamic_movement_primitives_based_on_neural_networks_for_human–robot.pdf}
}

@article{sidiropoulosNovelFrameworkGeneralizing2022,
  title = {A Novel Framework for Generalizing Dynamic Movement Primitives under Kinematic Constraints},
  author = {Sidiropoulos, Antonis and Papageorgiou, Dimitrios and Doulgeri, Zoe},
  date = {2022-10-10},
  journaltitle = {Autonomous Robots},
  shortjournal = {Auton Robot},
  issn = {1573-7527},
  doi = {10.1007/s10514-022-10067-4},
  url = {https://doi.org/10.1007/s10514-022-10067-4},
  urldate = {2022-10-30},
  abstract = {In this work, we propose a novel framework for generalizing a desired trajectory pattern, encoded using Dynamic Movement Primitives (DMP), subject to kinematic constraints. DMP have been extensively used in robotics for encoding and reproducing kinematic behaviours, thanks to their generalization, stability and robustness properties. However, incorporating kinematic constraints has not yet been fully addressed. To this end, we design an optimization framework, based on the DMP formulation from our previous work, for generalizing trajectory patterns, encoded with DMP subject to kinematic constraints, considering also time-varying target and time duration, via-point and obstacle constraints. Simulations highlight these properties and comparisons are drawn with other approaches for enforcing constraints on DMP. The usefulness and applicability of the proposed framework is showcased in experimental scenarios, including a handover, where the target and time duration vary, and placing scenarios, where obstacles are dynamically introduced in the scene.},
  langid = {english},
  keywords = {Constrained motion generation,Constrained optimization,Dynamic movement primitives,Online trajectory adaptation,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/sidiropoulos_et_al_2022_a_novel_framework_for_generalizing_dynamic_movement_primitives_under_kinematic.pdf}
}

@inproceedings{sidiropoulosReversibleDynamicMovement2021,
  title = {A {{Reversible Dynamic Movement Primitive}} Formulation},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Sidiropoulos, Antonis and Doulgeri, Zoe},
  date = {2021-05},
  pages = {3147--3153},
  issn = {2577-087X},
  doi = {10.1109/ICRA48506.2021.9562059},
  abstract = {In this work, a novel Dynamic Movement Primitive (DMP) formulation is proposed which supports reversibility, i.e. backwards reproduction of a learned trajectory. Apart from sharing all favourable properties of the original DMP, decoupling the teaching of position and velocity profiles and bidirectional drivability along the encoded path are also supported. Original DMP have been extensively used for encoding and reproducing a desired motion pattern in several robotic applications. However, they lack reversibility, which is a useful and expedient property that can be leveraged in many scenarios. The proposed formulation is analyzed theoretically and its practical usefulness is showcased in an assembly by insertion experimental scenario.},
  eventtitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  keywords = {Asymptotic stability,Automation,Conferences,Dynamics,Education,Encoding,READ,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/sidiropoulos_doulgeri_2021_a_reversible_dynamic_movement_primitive_formulation.pdf}
}

@article{silverDeterministicPolicyGradient,
  title = {Deterministic {{Policy Gradient Algorithms}}},
  author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  pages = {9},
  abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counterparts in high-dimensional action spaces.},
  langid = {english},
  keywords = {#nosource,deterministic policy gradients,model-free,reinforcement learning},
  annotation = {00000}
}

@unpublished{silverMasteringChessShogi2017,
  title = {Mastering {{Chess}} and {{Shogi}} by {{Self-Play}} with a {{General Reinforcement Learning Algorithm}}},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  date = {2017-12-05},
  eprint = {1712.01815},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1712.01815},
  urldate = {2020-07-02},
  abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,designed model,model-based,reinforcement learning}
}

@article{silverMasteringGameGo2017,
  title = {Mastering the Game of Go without Human Knowledge},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian},
  date = {2017},
  journaltitle = {Nature},
  volume = {550},
  number = {7676},
  pages = {354},
  keywords = {#nosource,machine learning control}
}

@unpublished{sindhwaniLearningContractingVector2018,
  title = {Learning Contracting Vector Fields for Stable Imitation Learning},
  author = {Sindhwani, Vikas and Tu, Stephen and Khansari, Mohi},
  date = {2018},
  eprint = {1804.04878},
  eprinttype = {arxiv},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/sindhwani_et_al_2018_learning_contracting_vector_fields_for_stable_imitation_learning.pdf}
}

@online{Singularity,
  title = {Singularity},
  url = {https://sylabs.io/singularity/},
  urldate = {2019-11-26},
  abstract = {Singularity provides a single universal on-ramp from developers’ workstations to local resources, the cloud, and all the way to edge.},
  organization = {{Sylabs.io}},
  keywords = {#nosource,robotic grasping,tno internship}
}

@article{siReviewManipulationSkill2021,
  title = {A Review on Manipulation Skill Acquisition through Teleoperation-Based Learning from Demonstration},
  author = {Si, Weiyong and Wang, Ning and Yang, Chenguang},
  date = {2021},
  journaltitle = {Cognitive Computation and Systems},
  volume = {3},
  number = {1},
  pages = {1--16},
  issn = {2517-7567},
  doi = {10.1049/ccs2.12005},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1049/ccs2.12005},
  urldate = {2022-09-08},
  abstract = {Manipulation skill learning and generalisation have gained increasing attention due to the wide applications of robot manipulators and the spurt of robot learning techniques. Especially, the learning from demonstration method has been exploited widely and successfully in the robotic community, and it is regarded as a promising direction to realise the manipulation skill learning and generalisation. In addition to the learning techniques, the immersive teleoperation enables the human to operate a remote robot with an intuitive interface and achieve the telepresence. Thus, it is a promising way to transfer manipulation skills from humans to robots by combining the learning methods and teleoperation, and adapting the learned skills to different tasks in new situations. This review, therefore, aims to provide an overview of immersive teleoperation for skill learning and generalisation to deal with complex manipulation tasks. To this end, the key technologies, for example, manipulation skill learning, multimodal interfacing for teleoperation and telerobotic control, are introduced. Then, an overview is given in terms of the most important applications of immersive teleoperation platform for robot skill learning. Finally, this survey discusses the remaining open challenges and promising research topics.},
  langid = {english},
  keywords = {control engineering computing,learning (artificial intelligence),manipulators,mobile robots,READ,remote handling,REVIEW,telerobotics,virtual reality},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/si_et_al_2021_a_review_on_manipulation_skill_acquisition_through_teleoperation-based_learning.pdf}
}

@inproceedings{songImpedanceControlRobots2017,
  title = {Impedance {{Control}} of {{Robots}}: {{An Overview}}},
  shorttitle = {Impedance {{Control}} of {{Robots}}},
  booktitle = {2017 2nd {{International Conference}} on {{Cybernetics}}, {{Robotics}} and {{Control}} ({{CRC}})},
  author = {Song, Peng and Yu, Yueqing and Zhang, Xuping},
  date = {2017-07},
  pages = {51--55},
  doi = {10.1109/CRC.2017.20},
  abstract = {Impedance control has received significant efforts in recent decades in robotics as impedance control aims to achieve the desired mechanical interaction with uncertain environment. This paper is dedicated to the overview of basic concepts and principles, implementation strategies, challenging research problems and interests, etc. The objective is to help readers quickly get into problems of their interests related to impedance control and provide guidance and insights in finding appropriate strategies and solutions.},
  eventtitle = {2017 2nd {{International Conference}} on {{Cybernetics}}, {{Robotics}} and {{Control}} ({{CRC}})},
  keywords = {Control systems,Force,force control,Hardware,human-robot interaction,Impedance,impedance control,Manipulator dynamics,rehabilitation robot,REVIEW,robotic manipulation,STABILITY,Task analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/song_et_al_2017_impedance_control_of_robots.pdf}
}

@article{songTutorialSurveyComparison2019,
  title = {A Tutorial Survey and Comparison of Impedance Control on Robotic Manipulation},
  author = {Song, Peng and Yu, Yueqing and Zhang, Xuping},
  date = {2019},
  journaltitle = {Robotica},
  volume = {37},
  number = {5},
  pages = {801--836},
  publisher = {{Cambridge University Press}},
  keywords = {Force control,Human–robot interaction,Impedance control,Interaction control,REVIEW,Robotic manipulation},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/song_et_al_2019_a_tutorial_survey_and_comparison_of_impedance_control_on_robotic_manipulation.pdf;/home/ricks/Zotero/storage/RRBAKHV3/link.html}
}

@article{sontagInputtoStateStabilityProperty1995,
  title = {On the {{Input-to-State Stability Property}}},
  author = {Sontag, Eduardo D.},
  date = {1995-01-01},
  journaltitle = {European Journal of Control},
  shortjournal = {European Journal of Control},
  volume = {1},
  number = {1},
  pages = {24--36},
  issn = {0947-3580},
  doi = {10.1016/S0947-3580(95)70005-X},
  url = {https://www.sciencedirect.com/science/article/pii/S094735809570005X},
  urldate = {2022-06-14},
  abstract = {The ‘input to state stability’ (ISS) property provides a natural framework in which to formulate notions of stability with respect to input perturbations. In this expository paper, we review various equivalent definitions expressed in stability, Lyapunov-theoretic, and dissipation terms. We sketch some applications to the stabilisation of cascades of systems and of linear systems subject to control saturation.},
  langid = {english},
  keywords = {Cascades,Dissipation,Lyapunuv functions,Saturation,Stability},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/sontag_1995_on_the_input-to-state_stability_property.pdf}
}

@article{sontagUniversalConstructionArtstein1989,
  title = {A ‘Universal’Construction of {{Artstein}}'s Theorem on Nonlinear Stabilization},
  author = {Sontag, Eduardo D.},
  date = {1989},
  journaltitle = {Systems \& control letters},
  volume = {13},
  number = {2},
  pages = {117--123},
  publisher = {{Elsevier}},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/sontag_1989_a_‘universal’construction_of_artstein's_theorem_on_nonlinear_stabilization.pdf}
}

@online{SpinoffsTNO,
  title = {Spin-Offs | {{TNO}}},
  url = {https://techtransfer.tno.nl/en/spin-offs/},
  urldate = {2019-11-24},
  keywords = {#nosource,robotic grasping,tno internship}
}

@article{spyrakos-papastavridisPassivityPreservationVariable2020,
  title = {Passivity {{Preservation}} for {{Variable Impedance Control}} of {{Compliant Robots}}},
  author = {Spyrakos-Papastavridis, Emmanouil and Childs, Peter R. N. and Dai, Jian S.},
  date = {2020-10},
  journaltitle = {IEEE/ASME Transactions on Mechatronics},
  volume = {25},
  number = {5},
  pages = {2342--2353},
  issn = {1941-014X},
  doi = {10.1109/TMECH.2019.2961478},
  abstract = {The notable performance exhibited by impedance controllers during robotic interaction has led to the widespread use of this control methodology. Improved position and interaction control might be attainable through utilisation of variable impedance control (VIC) techniques. Interactional performance could be further improved by combining structural compliance with VIC. However, utilization of VIC tends to induce energy-injecting elements, which could impact on a robotic system's stability/passivity. Additionally, implementation of active VIC techniques on passively compliant robots has not been investigated (although several works consider VIC using variable stiffness actuators), which renders the existing rigid-joint robot, passivity-inducing control schemes inapplicable to compliant systems. To this end, the work presented here introduces a novel scheme, termed passivity-preservation control (PPC), which suppresses the energy injections that could be introduced into compliant robots, as a result of VIC. Compared to tank-based VIC approaches the PPC scheme is directly applicable to flexible-joint robots, even ones with nonlinear passive stiffness elements, while its performance is independent of the tank-energy levels. Moreover, the proposed scheme permits stable VIC using full-state feedback, thereby enabling impedance modulations relating to both motor and link-side variables. Consequently, full-state feedback gains can be generated via linear quadratic regulator optimisation, thus enabling application of gain-scheduling techniques on flexible-joint robots for enhanced position control. Passivity and stability analyses are performed for joint and Cartesian-space versions of the PPC scheme, which justify their applicability to both interaction and “free-motion” scenarios. The PPC scheme's efficacy, compared to constant gain impedance methods, in terms of convergence and interactional performance, is corroborated via simulation and experimental means involving the Sawyer robot, which is powered by series elastic actuators. Theoretical and experimental results mathematically and practically verify VIC stability, thus enabling flexible-joint robots to more accurately mimic biologically inspired behaviors.},
  eventtitle = {{{IEEE}}/{{ASME Transactions}} on {{Mechatronics}}},
  keywords = {Damping,Human-robot interaction,Impedance,impedance control,manipulators,Modulation,Robots,Stability analysis,Task analysis,Torque},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/spyrakos-papastavridis_et_al_2020_passivity_preservation_for_variable_impedance_control_of_compliant_robots.pdf}
}

@online{StableDynamicWalking,
  title = {Stable Dynamic Walking over Uneven Terrain - {{Ian R Manchester}}, {{Uwe Mettin}}, {{Fumiya Iida}}, {{Russ Tedrake}}, 2011},
  url = {https://journals-sagepub-com.tudelft.idm.oclc.org/doi/10.1177/0278364910395339},
  urldate = {2021-04-18},
  keywords = {#nosource}
}

@unpublished{stookeAcceleratedMethodsDeep2019,
  title = {Accelerated {{Methods}} for {{Deep Reinforcement Learning}}},
  author = {Stooke, Adam and Abbeel, Pieter},
  date = {2019-01-10},
  eprint = {1803.02811},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1803.02811},
  urldate = {2020-07-02},
  abstract = {Deep reinforcement learning (RL) has achieved many recent successes, yet experiment turn-around time remains a key bottleneck in research and in practice. We investigate how to optimize existing deep RL algorithms for modern computers, specifically for a combination of CPUs and GPUs. We confirm that both policy gradient and Q-value learning algorithms can be adapted to learn using many parallel simulator instances. We further find it possible to train using batch sizes considerably larger than are standard, without negatively affecting sample complexity or final performance. We leverage these facts to build a unified framework for parallelization that dramatically hastens experiments in both classes of algorithm. All neural network computations use GPUs, accelerating both data collection and training. Our results include using an entire DGX-1 to learn successful strategies in Atari games in mere minutes, using both synchronous and asynchronous algorithms.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,reinforcement learning,scaling RL}
}

@inproceedings{stulpReinforcementLearningImpedance2011,
  title = {Reinforcement Learning of Impedance Control in Stochastic Force Fields},
  booktitle = {2011 {{IEEE International Conference}} on {{Development}} and {{Learning}} ({{ICDL}})},
  author = {Stulp, Freek and Buchli, Jonas and Ellmer, Alice and Mistry, Michael and Theodorou, Evangelos and Schaal, Stefan},
  date = {2011-08},
  volume = {2},
  pages = {1--6},
  issn = {2161-9476},
  doi = {10.1109/DEVLRN.2011.6037312},
  abstract = {Variable impedance control is essential for ensuring robust and safe physical interaction with the environment. As demonstrated in numerous force field experiments, humans combine two strategies to adapt their impedance to external perturbations: 1) if perturbations are unpredictable, subjects increase their impedance through co-contraction; 2) if perturbations are predictable, subjects learn a feed-forward command to counter the known perturbation. In this paper, we apply the force field paradigm to a simulated 7-DOF robot, by exerting stochastic forces on the robot's end-effector. The robot `subject' uses our model-free reinforcement learning algorithm PI2 to simultaneously learn the end-effector trajectories and variable impedance schedules. We demonstrate how the robot learns the same two-fold strategy to perturbation rejection as humans do, resulting in qualitatively similar behavior. Our results provide a biologically plausible approach to learning appropriate impedances purely from experience, without requiring a model of either body or environment dynamics.},
  eventtitle = {2011 {{IEEE International Conference}} on {{Development}} and {{Learning}} ({{ICDL}})},
  keywords = {Computational modeling,Noise measurement,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/stulp_et_al_2011_reinforcement_learning_of_impedance_control_in_stochastic_force_fields2.pdf}
}

@inproceedings{sultangazinExploitingExpertsLearning2021,
  title = {Exploiting the Experts: {{Learning}} to Control Unknown {{SISO}} Feedback Linearizable Systems from Expert Demonstrations},
  shorttitle = {Exploiting the Experts},
  booktitle = {2021 60th {{IEEE Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Sultangazin, Alimzhan and Fraile, Lucas and Tabuada, Paulo},
  date = {2021-12},
  pages = {5789--5794},
  issn = {2576-2370},
  doi = {10.1109/CDC45484.2021.9683193},
  abstract = {It was shown, in recent work by the authors, that it is possible to learn an asymptotically stabilizing controller from a small number of demonstrations performed by an expert on a feedback linearizable system. These results rely on knowledge of the plant dynamics to assemble the learned controller from the demonstrations. In this paper we show how to leverage recent results on data-driven control to dispense with the need to use the plant model. By bringing these two methodologies — learning from demonstrations and data-driven control — together, this paper provides a technique that enables the control of unknown nonlinear feedback linearizable systems solely based on a small number of expert demonstrations.},
  eventtitle = {2021 60th {{IEEE Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  keywords = {Conferences,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/sultangazin_et_al_2021_exploiting_the_experts3.pdf}
}

@article{sunApplicationWavevariableControl2014,
  title = {Application of Wave-Variable Control to Bilateral Teleoperation Systems: {{A}} Survey},
  shorttitle = {Application of Wave-Variable Control to Bilateral Teleoperation Systems},
  author = {Sun, Da and Naghdy, Fazel and Du, Haiping},
  date = {2014-01-01},
  journaltitle = {Annual Reviews in Control},
  shortjournal = {Annual Reviews in Control},
  volume = {38},
  number = {1},
  pages = {12--31},
  issn = {1367-5788},
  doi = {10.1016/j.arcontrol.2014.03.002},
  url = {https://www.sciencedirect.com/science/article/pii/S1367578814000030},
  urldate = {2022-06-23},
  abstract = {Teleoperation systems allow an operator to perform complex tasks in a remote environment. Stability of a bilateral teleoperation system is quite sensitive to time delays. One of the methods to guarantee the stability of bilateral telerobotics in the presence of time delays is wave variable control. A review of various applications of wave variable methods in telerobotics has been conducted. An evaluation of different methods proposed to compensate for the intrinsic problems associated with wave variable methods, including position drift, wave reflection and time varying delay, has also been carried out. In addition, different techniques developed to enhance the performance of the wave-based systems are also identified and reviewed. The research gaps in this field are identified and future directions for further research are proposed.},
  langid = {english},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/sun_et_al_2014_application_of_wave-variable_control_to_bilateral_teleoperation_systems.pdf}
}

@article{sunCompositeLearningEnhanced2020,
  title = {Composite {{Learning Enhanced Robot Impedance Control}}},
  author = {Sun, Tairen and Peng, Liang and Cheng, Long and Hou, Zeng-Guang and Pan, Yongping},
  date = {2020-03},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {31},
  number = {3},
  pages = {1052--1059},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2019.2912212},
  abstract = {The desired impedance dynamics can be achieved for a robot if and only if an impedance error converges to zero or a small neighborhood of zero. Although the convergence of impedance errors is important, it is seldom obtained in the existing impedance controllers due to robots modeling uncertainties and external disturbances. This brief proposes two composite learning impedance controllers (CLICs) for robots with parameter uncertainties based on whether a factorization assumption is satisfied or not. In the proposed control designs, the convergence of impedance errors, reflected by the convergence of parameter estimation errors and some auxiliary errors, is achieved by using composite learning laws under a relaxed excitation condition. The theoretical results are proven based on the Lyapunov theory. The effectiveness and advantages of the proposed CLICs are validated by simulations on a parallel robot in three cases.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}} and {{Learning Systems}}},
  keywords = {Adaptive control,composite adaptation,Convergence,Impedance,impedance control,learning control,parameter convergence,Parameter estimation,robot,Robots,Stability criteria,Uncertainty},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/sun_et_al_2020_composite_learning_enhanced_robot_impedance_control.pdf}
}

@article{sunIntegratingReinforcementLearning2022,
  title = {Integrating {{Reinforcement Learning}} and {{Learning From Demonstrations}} to {{Learn Nonprehensile Manipulation}}},
  author = {Sun, Xilong and Li, Jiqing and Kovalenko, Anna Vladimirovna and Feng, Wei and Ou, Yongsheng},
  date = {2022},
  journaltitle = {IEEE Transactions on Automation Science and Engineering},
  pages = {1--10},
  issn = {1558-3783},
  doi = {10.1109/TASE.2022.3185071},
  abstract = {Motor skills are essential for robots to accomplish complicated and dexterous manipulation tasks, which are difficult to be mastered through traditional controller designs. Currently, robots learning from demonstrations enable them to learn control policies automatically from human motor demonstrations. However, the nonlinearity and instantaneousness of the demonstrated forces prohibit robots from fully mastering the motor skill features by simply exploiting force examples. Therefore, a self-improvement learning scheme is required to refine the control policy further until satisfactory motor skills are acquired. Hence, this paper combines learning from demonstrations and reinforcement learning to learn a controller for complex motor skills. The proposed method is validated on an IIWA KUKA robot, performing a specified nonprehensile manipulation task. Note to Practitioners—The motivation of this paper originates from the requirement to develop an efficient and fast learning algorithm that improves the robot skill learning efficiency. Specifically, our research focuses on the nonprehensile manipulation task, easily subject to environmental changes. Therefore, the robot must continuously interact with the environment to master the skill. To accelerate the skill learning process, learning from demonstrations initializes the control policies, and then the robot starts to practice the demonstrated skill. After each practice round, the robot receives a reward from the environment, and based on the reinforcement learning algorithm limited up to 100 trials, the robot masters the nonprehensile manipulation skill.},
  eventtitle = {{{IEEE Transactions}} on {{Automation Science}} and {{Engineering}}},
  keywords = {Education,Force,learning from demonstrations,Nonprehensile manipulation,reinforcement learning,Reinforcement learning,Robots,Sports,STABILITY,Task analysis,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/sun_et_al_2022_integrating_reinforcement_learning_and_learning_from_demonstrations_to_learn.pdf}
}

@article{sunLearningImpedanceControl2020,
  title = {Learning Impedance Control of Robots with Enhanced Transient and Steady-State Control Performances},
  author = {Sun, Tairen and Cheng, Long and Peng, Liang and Hou, Zengguang and Pan, Yongping},
  date = {2020-07-24},
  journaltitle = {Science China Information Sciences},
  shortjournal = {Sci. China Inf. Sci.},
  volume = {63},
  number = {9},
  pages = {192205},
  issn = {1869-1919},
  doi = {10.1007/s11432-019-2639-6},
  url = {https://doi.org/10.1007/s11432-019-2639-6},
  urldate = {2022-05-02},
  abstract = {This study proposes a learning impedance controller comprising a proportional feedback control term, a composite-learning-based uncertainty estimation term, and a robot-environment interaction control term. The impedance control problem is converted into a particular reference-trajectory tracking problem based on a generated reference trajectory. The proposed controller ensures the exponential convergence of the auxiliary tracking error and the uncertainty estimation error. The interaction control term improves the transient control performance through suppression/encouragement of the incorrect/correct robot movements. The composite-learning update law enhances the transient and steady-state control performances based on the exponential convergence of the uncertainty estimation error and auxiliary tracking error. Finally, the effectiveness and advantages of the proposed impedance controller are validated by theoretical analysis and simulations on a parallel robot.},
  langid = {english},
  keywords = {adaptive control,impedance control,neural network,parameter convergence,robot,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/sun_et_al_2020_learning_impedance_control_of_robots_with_enhanced_transient_and_steady-state.pdf}
}

@article{sunModelReferenceAdaptive2021,
  title = {A Model Reference Adaptive Variable Impedance Control Method for Robot},
  author = {Sun, Xinchao and Zhao, Lianyu and Liu, Zhenzhong},
  date = {2021},
  journaltitle = {MATEC Web of Conferences},
  shortjournal = {MATEC Web Conf.},
  volume = {336},
  pages = {03005},
  publisher = {{EDP Sciences}},
  issn = {2261-236X},
  doi = {10.1051/matecconf/202133603005},
  url = {https://www.matec-conferences.org/articles/matecconf/abs/2021/05/matecconf_cscns20_03005/matecconf_cscns20_03005.html},
  urldate = {2022-04-28},
  abstract = {As a simple and effective force tracking control method, impedance control is widely used in robot contact operations. The internal control parameters of traditional impedance control are constant and cannot be corrected in real time, which will lead to instability of control system or large force tracking error. Therefore, it is difficult to be applied to the occasions requiring higher force accuracy, such as robotic medical surgery, robotic space operation and so on. To solve this problem, this paper proposes a model reference adaptive variable impedance control method, which can realize force tracking control by adjusting internal impedance control parameters in real time and generating a reference trajectory at the same time. The simulation experiment proves that compared with the traditional impedance control method, this method has faster force tracking speed and smaller force tracking error. It is a better force tracking control method.},
  langid = {english},
  keywords = {READ,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/sun_et_al_2021_a_model_reference_adaptive_variable_impedance_control_method_for_robot.pdf}
}

@article{sunStabilityGuaranteedVariableImpedance2021,
  title = {Stability-{{Guaranteed Variable Impedance Control}} of {{Robots Based}} on {{Approximate Dynamic Inversion}}},
  author = {Sun, Tairen and Peng, Liang and Cheng, Long and Hou, Zeng-Guang and Pan, Yongping},
  date = {2021-07},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  volume = {51},
  number = {7},
  pages = {4193--4200},
  issn = {2168-2232},
  doi = {10.1109/TSMC.2019.2930582},
  abstract = {Variable impedance control has been considered as one of the most important compliant control approaches for its abilities in improving compliance, safety, and efficiency in robot-environment interaction. However, existing variable impedance controllers have deficits in stability guarantee. This article proposes a stability-guaranteed variable impedance control approach for robots with modeling uncertainties based on approximate dynamic inversion (ADI). Novel constraints on variable impedance profiles are given to guarantee the exponential stability of the desired variable impedance dynamics. An ADI-based impedance control law is designed to achieve the desired variable impedance dynamics through the convergence of a variable impedance error. Based on the extended Tikhonovs theorem, it is proven that the closed-loop control system has semiglobal practical exponential stability. The proposed impedance controller can be implemented in a PID form and is appealing for its simple structure, easy implementation, and control stability guarantee. The effectiveness of the proposed variable impedance controller is illustrated by an illustrative example taken on a five-bar parallel robot.},
  eventtitle = {{{IEEE Transactions}} on {{Systems}}, {{Man}}, and {{Cybernetics}}: {{Systems}}},
  keywords = {Approximate dynamic inversion (ADI),Control theory,Convergence,Dynamics,Force,Impedance,impedance control,IMPORTANT,Joint impedance,READ,robot control,Robots,STABILITY,Stability analysis,trajectory tracking,variable impedance},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/sun_et_al_2021_stability-guaranteed_variable_impedance_control_of_robots_based_on_approximate.pdf}
}

@unpublished{suomalainenSurveyRobotManipulation2021,
  title = {A {{Survey}} of {{Robot Manipulation}} in {{Contact}}},
  author = {Suomalainen, Markku and Karayiannidis, Yiannis and Kyrki, Ville},
  date = {2021-12-03},
  eprint = {2112.01942},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2112.01942},
  urldate = {2022-04-22},
  abstract = {In this survey we present the current status on robots performing manipulation tasks that require varying contact with the environment, such that the robot must either implicitly or explicitly control the contact force with the environment to complete the task. Robots can perform more and more manipulation tasks that are still done by humans, and there is a growing number of publications on the topics of 1) performing tasks that always require contact and 2) mitigating uncertainty by leveraging the environment in tasks that, under perfect information, could be performed without contact. The recent trends have seen robots perform tasks earlier left for humans, such as massage, and in the classical tasks, such as peg-in-hole, there is more efficient generalization to other similar tasks, better error tolerance, and faster planning or learning of the tasks. Thus, in this survey we cover the current stage of robots performing such tasks, starting from surveying all the different in-contact tasks robots can perform, observing how these tasks are controlled and represented, and finally presenting the learning and planning of the skills required to complete these tasks.},
  keywords = {Computer Science - Robotics,REVIEW},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/suomalainen_et_al_2021_a_survey_of_robot_manipulation_in_contact.pdf}
}

@online{SuttonBartoBook,
  title = {Sutton \& {{Barto Book}}: {{Reinforcement Learning}}: {{An Introduction}}},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  urldate = {2020-08-10},
  keywords = {#nosource},
  annotation = {00015}
}

@article{suttonPolicyGradientMethods,
  title = {Policy {{Gradient Methods}} for {{Reinforcement Learning}} with {{Function Approximation}}},
  author = {Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  pages = {7},
  abstract = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy.},
  langid = {english},
  keywords = {#nosource,classical RL,policy gradients,reinforcement learning}
}

@article{suttonReinforcementLearningIntroduction2020,
  title = {Reinforcement {{Learning}}: {{An Introduction}}},
  author = {Sutton, Richard S and Barto, Andrew G},
  date = {2020},
  pages = {352},
  langid = {english},
  keywords = {#nosource,reinforcement learning,review},
  annotation = {37671}
}

@article{szepesvariAlgorithmsReinforcementLearning2010,
  title = {Algorithms for {{Reinforcement Learning}}},
  author = {Szepesvári, Csaba},
  date = {2010-01},
  journaltitle = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  shortjournal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  volume = {4},
  number = {1},
  pages = {1--103},
  issn = {1939-4608, 1939-4616},
  doi = {10.2200/S00268ED1V01Y201005AIM009},
  url = {http://www.morganclaypool.com/doi/abs/10.2200/S00268ED1V01Y201005AIM009},
  urldate = {2020-07-02},
  langid = {english},
  keywords = {#nosource,books,classical RL,reinforcement learning},
  annotation = {00901}
}

@inproceedings{tadeleCombiningEnergyPower2014,
  title = {Combining Energy and Power Based Safety Metrics in Controller Design for Domestic Robots},
  booktitle = {2014 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Tadele, Tadele Shiferaw and family=Vries, given=Theo J. A., prefix=de, useprefix=true and Stramigioli, Stefano},
  date = {2014-05},
  pages = {1209--1214},
  issn = {1050-4729},
  doi = {10.1109/ICRA.2014.6907007},
  abstract = {This paper presents a general passivity based interaction controller design approach that utilizes a combined energy and power based safety norms to assert safety of domestic robots. Since these robots are expected to co-habit the same environment with a human user, analysing and ensuring their safety is an important requirement. Safety analysis of domestic robots determine whether a robot achieves a desired safety level according to some quantitative safety metrics. When it comes to controller design for human friendly robots, it often involves introducing compliance and ensuring asymptotic stability using impedance control technique and passivity theories. The controller proposed in this work also uses a passive design that extends the standard impedance control scheme with energy and power based safety metrics to ensure that safety requirements defined in these norms are achieved by domestic robots. The effectiveness of the proposed guideline is illustrated with simulation and experimental results.},
  eventtitle = {2014 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  keywords = {Domestic Robots,Force,Impedance,Impedance Control,Injuries,Interaction Control,Manipulators,Measurement,Passivity Based Control,PD,Safety,Safety Metric},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/tadele_et_al_2014_combining_energy_and_power_based_safety_metrics_in_controller_design_for.pdf}
}

@online{takayukiAlgorithmicPerspectiveImitation2018,
  title = {An {{Algorithmic Perspective}} on {{Imitation Learning}}},
  author = {Takayuki, Osa and Joni, Pajarinen and Gerhard, Neumann and J. Andrew, Bagnell and Pieter, Abbeel and Jan, Peters},
  date = {2018-11-16},
  url = {https://arxiv.org/abs/1811.06711},
  urldate = {2020-08-10},
  keywords = {#nosource,imitation learning,review},
  annotation = {00151}
}

@inproceedings{takeishiLearningDynamicsModels2021,
  title = {Learning Dynamics Models with Stable Invariant Sets},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Takeishi, Naoya and Kawahara, Yoshinobu},
  date = {2021},
  volume = {35},
  number = {11},
  pages = {9782--9790},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/takeishi_kawahara_2021_learning_dynamics_models_with_stable_invariant_sets.pdf}
}

@inproceedings{talignanilandiPassivityBasedStrategyCoaching2018,
  title = {A {{Passivity-Based Strategy}} for {{Coaching}} in {{Human-Robot Interaction}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Talignani Landi, Chiara and Ferraguti, Federica and Fantuzzi, Cesare and Secchi, Cristian},
  date = {2018-05},
  pages = {3279--3284},
  issn = {2577-087X},
  doi = {10.1109/ICRA.2018.8460836},
  abstract = {In order to make robot programming more easy and immediate, walk-through programming techniques can be exploited. However, a modification of a portion of the trajectory usually means to execute the path from the beginning. In this paper we propose a passivity-based framework to modify the trajectory online, manually driving the robot throughout the desired correction. The system follows the initial trajectory, encoded with Dynamical Movement Primitives, by setting high gains in the admittance control. When the human operator grabs the end-effector, the robot becomes compliant and the user can easily teach the desired correction, until he/she releases it at the end of the modification. Finally, the correction is optimally joined to the initial trajectory, restarting the path tracking. To avoid unsafe behaviors, the variation of the admittance parameters is performed exploiting energy tanks, in order to preserve the passivity of the interaction.},
  eventtitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  keywords = {Admittance,Hidden Markov models,Robot sensing systems,Service robots,Task analysis,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/talignani_landi_et_al_2018_a_passivity-based_strategy_for_coaching_in_human-robot_interaction.pdf}
}

@article{talignanilandiPassivityBasedStrategyManual2019,
  title = {A {{Passivity-Based Strategy}} for {{Manual Corrections}} in {{Human-Robot Coaching}}},
  author = {Talignani Landi, Chiara and Ferraguti, Federica and Fantuzzi, Cesare and Secchi, Cristian},
  date = {2019-03},
  journaltitle = {Electronics},
  volume = {8},
  number = {3},
  pages = {320},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2079-9292},
  doi = {10.3390/electronics8030320},
  url = {https://www.mdpi.com/2079-9292/8/3/320},
  urldate = {2022-08-09},
  abstract = {In recent years, new programming techniques have been developed in the human-robot collaboration (HRC) field. For example, walk-through programming allows to program the robot in an easy and intuitive way. In this context, a modification of a portion of the trajectory usually requires the teaching of the path from the beginning. In this paper we propose a passivity-based method to locally change a trajectory based on a manual human correction. At the beginning the robot follows the nominal trajectory, encoded through the Dynamical Movement Primitives, by setting high control gains. When the human grasps the end-effector, the robot is made compliant and he/she can drive it along the correction. The correction is optimally joined to the nominal trajectory, resuming the path tracking. In order to avoid unstable behaviors, the variation of the control gains is performed exploiting energy tanks, preserving the passivity of the interaction. Finally, the correction is spatially fixed so that a variation in the boundary conditions (e.g., the initial/final points) does not affect the modification.},
  issue = {3},
  langid = {english},
  keywords = {admittance control,coaching,human-robot interaction,passivity},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/talignani_landi_et_al_2019_a_passivity-based_strategy_for_manual_corrections_in_human-robot_coaching.pdf}
}

@article{tamosiunaiteLearningPourRobot2011,
  title = {Learning to Pour with a Robot Arm Combining Goal and Shape Learning for Dynamic Movement Primitives},
  author = {Tamosiunaite, Minija and Nemec, Bojan and Ude, Aleš and Wörgötter, Florentin},
  date = {2011-11-01},
  journaltitle = {Robotics and Autonomous Systems},
  shortjournal = {Robotics and Autonomous Systems},
  volume = {59},
  number = {11},
  pages = {910--922},
  issn = {0921-8890},
  doi = {10.1016/j.robot.2011.07.004},
  url = {https://www.sciencedirect.com/science/article/pii/S0921889011001254},
  urldate = {2022-09-22},
  abstract = {When describing robot motion with dynamic movement primitives (DMPs), goal (trajectory endpoint), shape and temporal scaling parameters are used. In reinforcement learning with DMPs, usually goals and temporal scaling parameters are predefined and only the weights for shaping a DMP are learned. Many tasks, however, exist where the best goal position is not a priori known, requiring to learn it. Thus, here we specifically address the question of how to simultaneously combine goal and shape parameter learning. This is a difficult problem because learning of both parameters could easily interfere in a destructive way. We apply value function approximation techniques for goal learning and direct policy search methods for shape learning. Specifically, we use “policy improvement with path integrals” and “natural actor critic” for the policy search. We solve a learning-to-pour-liquid task in simulations as well as using a Pa10 robot arm. Results for learning from scratch, learning initialized by human demonstration, as well as for modifying the tool for the learned DMPs are presented. We observe that the combination of goal and shape learning is stable and robust within large parameter regimes. Learning converges quickly even in the presence of disturbances, which makes this combined method suitable for robotic applications.},
  langid = {english},
  keywords = {-method,Dynamic movement primitives,Natural actor critic,Reinforcement learning,Value function approximation},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/tamosiunaite_et_al_2011_learning_to_pour_with_a_robot_arm_combining_goal_and_shape_learning_for_dynamic.pdf}
}

@unpublished{tangExplorationStudyCountBased2017,
  title = {\#{{Exploration}}: {{A Study}} of {{Count-Based Exploration}} for {{Deep Reinforcement Learning}}},
  shorttitle = {\#{{Exploration}}},
  author = {Tang, Haoran and Houthooft, Rein and Foote, Davis and Stooke, Adam and Chen, Xi and Duan, Yan and Schulman, John and De Turck, Filip and Abbeel, Pieter},
  date = {2017-12-05},
  eprint = {1611.04717},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1611.04717},
  urldate = {2020-07-02},
  abstract = {Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation. In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various high-dimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domain-dependent learned hash code may further improve these results. Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,exploration,intrinsic motivation,reinforcement learning}
}

@inproceedings{tangTeachIndustrialRobots2016,
  title = {Teach Industrial Robots Peg-Hole-Insertion by Human Demonstration},
  booktitle = {2016 {{IEEE International Conference}} on {{Advanced Intelligent Mechatronics}} ({{AIM}})},
  author = {Tang, Te and Lin, Hsien-Chung and Zhao, Yu and Fan, Yongxiang and Chen, Wenjie and Tomizuka, Masayoshi},
  date = {2016},
  pages = {488--494},
  publisher = {{IEEE}},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/tang_et_al_2016_teach_industrial_robots_peg-hole-insertion_by_human_demonstration2.pdf}
}

@online{tanSimtoRealLearningAgile2018,
  title = {Sim-to-{{Real}}: {{Learning Agile Locomotion For Quadruped Robots}}},
  shorttitle = {Sim-to-{{Real}}},
  author = {Tan, Jie and Zhang, Tingnan and Coumans, Erwin and Iscen, Atil and Bai, Yunfei and Hafner, Danijar and Bohez, Steven and Vanhoucke, Vincent},
  date = {2018-05-16},
  eprint = {1804.10332},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1804.10332},
  url = {http://arxiv.org/abs/1804.10332},
  urldate = {2023-07-14},
  abstract = {Designing agile locomotion for quadruped robots often requires extensive expertise and tedious manual tuning. In this paper, we present a system to automate this process by leveraging deep reinforcement learning techniques. Our system can learn quadruped locomotion from scratch using simple reward signals. In addition, users can provide an open loop reference to guide the learning process when more control over the learned gait is needed. The control policies are learned in a physics simulator and then deployed on real robots. In robotics, policies trained in simulation often do not transfer to the real world. We narrow this reality gap by improving the physics simulator and learning robust policies. We improve the simulation using system identification, developing an accurate actuator model and simulating latency. We learn robust controllers by randomizing the physical environments, adding perturbations and designing a compact observation space. We evaluate our system on two agile locomotion gaits: trotting and galloping. After learning in simulation, a quadruped robot can successfully perform both gaits in the real world.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/Tan et al_2018_Sim-to-Real.pdf;/home/ricks/Zotero/storage/QZDPH8HV/1804.html}
}

@book{tedrakeImprovedDynamicStability,
  title = {Improved {{Dynamic Stability Using Reinforcement Learning}}},
  author = {Tedrake, R. and Seung, H. S.},
  abstract = {Many researchers studying legged locomotion have applied tools from reinforcement learning/optimal control to minimize characteristics of a walking gait, most notably the energy consumption. In this paper, we use similar tools to maximize the region of stability of the controller- defined as the set of initial conditions from which the robot maintains its balance for at least 5 seconds. Experiments were run on a simulation of a planar one-legged hopping robot. After a large number of iterations, the ‘learned ’ controller is able to maintain balance from a much larger region of initial conditions than the original controller proposed by Raibert[1]. 1},
  keywords = {#nosource,walking}
}

@inproceedings{teeAdaptiveAdmittanceControl2010,
  title = {Adaptive Admittance Control of a Robot Manipulator under Task Space Constraint},
  booktitle = {2010 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Tee, Keng Peng and Yan, Rui and Li, Haizhou},
  date = {2010-05},
  pages = {5181--5186},
  issn = {1050-4729},
  doi = {10.1109/ROBOT.2010.5509874},
  abstract = {We present adaptive admittance control of a robotic manipulator, with uncertain dynamic parameters, operating in a constrained task space. To provide compliance to external forces, we generate a differentiable reference trajectory that remains in the constrained task space. Then, adaptive backstepping control, based on a time-varying asymmetric Barrier Lyapunov Function (BLF), is designed to achieve tracking of the reference trajectory while guaranteeing constraint satisfaction. The improved BLF-based control renders the entire constrained task space positively invariant. Despite transient perturbations by external forces and online parameter adaptation, practical tracking of the reference trajectory is achieved without transgression of the constrained task space. In the absence of interaction forces, asymptotic tracking of the desired trajectory is achieved.},
  eventtitle = {2010 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  keywords = {Adaptive control,Admittance,Impedance,Manipulator dynamics,Motion control,Orbital robotics,Programmable control,Rehabilitation robotics,Robot control,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/tee_et_al_2010_adaptive_admittance_control_of_a_robot_manipulator_under_task_space_constraint.pdf}
}

@article{teeConcurrentAdaptationForce2010,
  title = {Concurrent Adaptation of Force and Impedance in the Redundant Muscle System},
  author = {Tee, Keng Peng and Franklin, David W. and Kawato, Mitsuo and Milner, Theodore E. and Burdet, Etienne},
  date = {2010-01-01},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol Cybern},
  volume = {102},
  number = {1},
  pages = {31--44},
  issn = {1432-0770},
  doi = {10.1007/s00422-009-0348-z},
  url = {https://doi.org/10.1007/s00422-009-0348-z},
  urldate = {2023-04-10},
  abstract = {This article examines the validity of a model to explain how humans learn to perform movements in environments with novel dynamics, including unstable dynamics typical of tool use. In this model, a simple rule specifies how the activation of each muscle is adapted from one movement to the next. Simulations of multijoint arm movements with a neuromuscular plant that incorporates neural delays, reflexes, and signal-dependent noise, demonstrate that the controller is able to compensate for changing internal or environment dynamics and noise properties. The computational model adapts by learning both the appropriate forces and required limb impedance to compensate precisely for forces and instabilities in arbitrary directions with patterns similar to those observed in motor learning experiments. It learns to regulate reciprocal activation and co-activation in a redundant muscle system during repeated movements without requiring any explicit transformation from hand to muscle space. Independent error-driven change in the activation of each muscle results in a coordinated control of the redundant muscle system and in a behavior that reduces instability, systematic error, and energy.},
  langid = {english},
  keywords = {End-effector redundancy,Hybrid force-impedance control,Iterative and nonlinear adaptive control,Learning,Muscle-space,Optimization},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/tee_et_al_2010_concurrent_adaptation_of_force_and_impedance_in_the_redundant_muscle_system.pdf}
}

@inproceedings{tesfazgiInverseReinforcementLearning2021,
  title = {Inverse {{Reinforcement Learning}}: {{A Control Lyapunov Approach}}},
  shorttitle = {Inverse {{Reinforcement Learning}}},
  booktitle = {2021 60th {{IEEE Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Tesfazgi, Samuel and Lederer, Armin and Hirche, Sandra},
  date = {2021-12},
  pages = {3627--3632},
  issn = {2576-2370},
  doi = {10.1109/CDC45484.2021.9683494},
  abstract = {Inferring the intent of an intelligent agent from demonstrations and subsequently predicting its behavior, is a critical task in many collaborative settings. A common approach to solve this problem is the framework of inverse reinforcement learning (IRL), where the observed agent, e.g., a human demonstrator, is assumed to behave according to an intrinsic cost function that reflects its intent and informs its control actions. In this work, we reformulate the IRL inference problem to learning control Lyapunov functions (CLF) from demonstrations by exploiting the inverse optimality property, which states that every CLF is also a meaningful value function. Moreover, the derived CLF formulation directly guarantees stability of the system under the inferred control policies. We show the flexibility of our proposed method by learning from goal-directed movement demonstrations in a continuous environment.},
  eventtitle = {2021 60th {{IEEE Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  keywords = {Collaboration,Conferences,Cost function,Dynamical systems,Intelligent agents,READ,Reinforcement learning,Task analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/tesfazgi_et_al_2021_inverse_reinforcement_learning.pdf}
}

@article{tianModelFreeReinforcement2019,
  title = {Model {{Free Reinforcement Learning}} with {{Stability Guarantee}}},
  author = {Tian, Yuan},
  date = {2019},
  url = {https://repository.tudelft.nl/islandora/object/uuid%3Adde4e58f-e109-4e7f-8ecb-ed1734294e5c},
  urldate = {2023-07-03},
  abstract = {Model-free reinforcement learning has proved to be successful in many tasks such as robotic manipulator, video games, and even stock trading. However, as the dynamics of the environment is unmodelled, it is fundamentally difficult to ensure the learned policy to be absolutely reliable and its performance is guaranteed. In this thesis, we borrow the concept of stability and Lyapunov analysis in control theory to design a policy with stability guarantee and assure the guaranteed behaviors of the agent. A novel sample-based approach is proposed for analyzing the stability of a learning control system, and on the basis of the theoretical result, we establish a practical model-free learning framework with provable stability, safety and performance guarantees.\&lt;br/\&gt;\% Specifically, a novel locally constrained method is proposed to solve the safety constrained problems with lower conservatism. In our solution, a Lyapunov function is searched automatically to guarantee the closed-loop system stability, which also guides the simultaneous learning (covering both the policy and value-based learning methods). Our approach is evaluated on a series of discrete and continuous control benchmarks and largely outperforms the state-of-the-art results concerning unconstrained and constrained problems. It is also shown that the algorithm has the ability of recovery to equilibrium under perturbation using the policy with stability guarantee. (Anonymous code is available to reproduce the experimental esults\textbackslash footnote\{\textbackslash url\{https://github.com/RLControlTheoreticGuarantee/Guarantee\_Learning\_Control\}\}.) Since sometimes the constraint is hard to define, we introduce a novel method to learn a constraint by representing the bad cases or situations as a distribution, and the constraint is the Wasserstein distance between the distribution.},
  langid = {english},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/Tian_2019_Model Free Reinforcement Learning with Stability Guarantee.pdf}
}

@article{timmersLearningGraspObjects2018,
  title = {Learning to {{Grasp Objects}} with {{Reinforcement Learning}}},
  author = {Timmers, Rik},
  date = {2018},
  pages = {58},
  langid = {english},
  keywords = {#nosource,robotic grasping,tno internship}
}

@online{tiseoAchievingDexterousBidirectional2022,
  title = {Achieving {{Dexterous Bidirectional Interaction}} in {{Uncertain Conditions}} for {{Medical Robotics}}},
  author = {Tiseo, Carlo and Rouxel, Quentin and Asenov, Martin and Babarahmati, Keyhan Kouhkiloui and Ramamoorthy, Subramanian and Li, Zhibin and Mistry, Michael},
  date = {2022-06-20},
  eprint = {2206.09906},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2206.09906},
  urldate = {2022-08-11},
  abstract = {Medical robotics can help improve and extend the reach of healthcare services. A major challenge for medical robots is the complex physical interaction between the robot and the patients which is required to be safe. This work presents the preliminary evaluation of a recently introduced control architecture based on the Fractal Impedance Control (FIC) in medical applications. The deployed FIC architecture is robust to delay between the master and the replica robots. It can switch online between an admittance and impedance behaviour, and it is robust to interaction with unstructured environments. Our experiments analyse three scenarios: teleoperated surgery, rehabilitation, and remote ultrasound scan. The experiments did not require any adjustment of the robot tuning, which is essential in medical applications where the operators do not have an engineering background required to tune the controller. Our results show that is possible to teleoperate the robot to cut using a scalpel, do an ultrasound scan, and perform remote occupational therapy. However, our experiments also highlighted the need for a better robots embodiment to precisely control the system in 3D dynamic tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/tiseo_et_al_2022_achieving_dexterous_bidirectional_interaction_in_uncertain_conditions_for.pdf}
}

@inproceedings{tiseoBiomimeticAdaptiveForce2020,
  title = {Bio-Mimetic {{Adaptive Force}}/{{Position Control Using Fractal Impedance}}},
  booktitle = {2020 8th {{IEEE RAS}}/{{EMBS International Conference}} for {{Biomedical Robotics}} and {{Biomechatronics}} ({{BioRob}})},
  author = {Tiseo, Carlo and Merkt, Wolfgang and Babarahmati, Keyhan Kouhkiloui and Wolfslag, Wouter and Vijayakumar, Sethu and Mistry, Michael},
  date = {2020-11},
  pages = {1180--1187},
  issn = {2155-1782},
  doi = {10.1109/BioRob49111.2020.9224377},
  abstract = {The ability of animals to interact with complex dynamics is unmatched in robots. Especially important to the interaction performances is the online adaptation of body dynamics, which can be modelled as an impedance behaviour. However, variable impedance control still continues to be a challenge in the current control frameworks due to the difficulties of retaining stability when adapting the controller gains. The fractal impedance controller has recently been proposed to solve this issue. However, it still has limitations such as sudden jumps in force when it starts to converge to the desired position and the lack of a force feedback loop. In this manuscript, two improvements are made to the control framework to solve these limitations. The force discontinuity has been addressed introducing a modulation of the impedance via a virtual antagonist that modulates the output force. The force tracking has been modelled after the parallel force/position controller architecture. In contrast to traditional methods, the fractal impedance controller enables the implementation of a search algorithm on the force feedback to adapt its behaviour to the external environment instead of on relying on a priori knowledge of the external dynamics. Preliminary simulation results presented in this paper show the feasibility of the proposed approach, and it allows to evaluate the trade-off that needs to be made when relying on the proposed controller for interaction. In conclusion, the proposed method mimics the behaviour of an agonist/antagonist system adapting to unknown external dynamics, and it may find application in computational neuroscience, haptics, and interaction control.},
  eventtitle = {2020 8th {{IEEE RAS}}/{{EMBS International Conference}} for {{Biomedical Robotics}} and {{Biomechatronics}} ({{BioRob}})},
  keywords = {Adaptation models,Dynamics,Force,Force feedback,Fractals,Heuristic algorithms,IMPORTANT,READ,Simulation},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/tiseo_et_al_2020_bio-mimetic_adaptive_force-position_control_using_fractal_impedance.pdf}
}

@online{tiseoFineManipulationDynamic2022,
  title = {Fine {{Manipulation}} and {{Dynamic Interaction}} in {{Haptic Teleoperation}}},
  author = {Tiseo, Carlo and Rouxel, Quentin and Li, Zhibin and Mistry, Michael},
  date = {2022-05-05},
  eprint = {2109.04524},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2109.04524},
  urldate = {2022-08-11},
  abstract = {The teleoperation of robots enables remote intervention in distant and dangerous tasks without putting the operator in harm's way. However, remote operation faces fundamental challenges due to limits in communication delays. The proposed work improves the performances of teleoperation architecture based on Fractal Impedance Controller (FIC) by integrating into the haptic teleoperation pipeline a postural optimisation that also accounts for the replica robots' physical limitations. This update improves dynamic interactions by trading off tracking accuracy to maintain the system within its power limits. Thus, allowing fine manipulation without renouncing the robustness of the FIC controller. Additionally, the proposed method allows an online trade-off between tracking the autonomous trajectory and executing the teleoperated command, allowing their safe superimposition. The validated experimental results show that the proposed method is robust to increased communication delays. Moreover, we demonstrated that the remote teleoperated robot remains stable and safe to interact with, even when the communication with the master side is abruptly interrupted. with, even when the communication with the master side is abruptly interrupted.},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/tiseo_et_al_2022_fine_manipulation_and_dynamic_interaction_in_haptic_teleoperation.pdf}
}

@article{tiseoGeometricalPosturalOptimisation2022,
  title = {Geometrical Postural Optimisation of 7-{{DoF}} Limb-like Manipulators},
  author = {Tiseo, Carlo and Charitos, Sydney Rebecca and Mistry, Michael},
  date = {2022-03},
  journaltitle = {Engineering Research Express},
  shortjournal = {Eng. Res. Express},
  volume = {4},
  number = {1},
  pages = {015036},
  publisher = {{IOP Publishing}},
  issn = {2631-8695},
  doi = {10.1088/2631-8695/ac59dd},
  url = {https://doi.org/10.1088/2631-8695/ac59dd},
  urldate = {2022-08-11},
  abstract = {Robots are moving towards applications in less structured environments, but their model-based controllers are challenged by the tasks’ complexity and intrinsic environmental unpredictability. Studying biological motor control can provide insights into overcoming these limitations due to the high dexterity and stability observable in humans and animals. This work presents a geometrical solution to the postural optimisation of 7-DoF limbs-like mechanisms, which are robust to singularities and computationally efficient. The theoretical formulation identified two separate decoupled optimisation strategies. The shoulder and elbow strategy align the plane of motion with the expected plane of motion and guarantee the reachability of the end-posture. The wrist strategy ensures the end-effector orientation, which is essential to retain manipulability when nearing a singular configuration. The numerical results confirmed the theoretical observations and allowed us to identify the effect of different grasp strategies on system manipulability. The geometrical method was numerically tested in thousands of configurations proving to be both robust and accurate. The tested scenarios include left and right arm postures, singular configurations, and walking scenarios. The proposed geometrical approach can find application in developing efficient and robust interaction controllers that could be applied in computational neuroscience and robotics.},
  langid = {english},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/tiseo_et_al_2022_geometrical_postural_optimisation_of_7-dof_limb-like_manipulators.pdf}
}

@inproceedings{tiseoRobustImpedanceControl2022,
  title = {Robust {{Impedance Control}} for {{Dexterous Interaction Using Fractal Impedance Controller}} with {{IK-Optimisation}}},
  booktitle = {2022 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Tiseo, Carlo and Rouxel, Quentin and Li, Zhibin and Mistry, Michael},
  date = {2022-05},
  pages = {840--846},
  doi = {10.1109/ICRA46639.2022.9812013},
  abstract = {Robust dynamic interactions are required to move robots in daily environments alongside humans. Optimisation and learning methods have been used to mimic and reproduce human movements. However, they are often not robust and their generalisation is limited. This work proposed a hierarchical control architecture for robot manipulators and provided capabilities of reproducing human-like motions during unknown interaction dynamics. Our results show that the reproduced end-effector trajectories can preserve the main characteristics of the initial human motion recorded via a motion capture system, and are robust against external perturbations. The data indicate that some detailed movements are hard to reproduce due to the physical limits of the hardware that cannot reach the same velocity recorded in human movements. Nevertheless, these technical problems can be addressed by using better hardware and our proposed algorithms can still be applied to produce imitated motions.},
  eventtitle = {2022 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  keywords = {Hardware,Impedance,Machine learning algorithms,Perturbation methods,READ,Robustness,Stability analysis,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/tiseo_et_al_2022_robust_impedance_control_for_dexterous_interaction_using_fractal_impedance.pdf}
}

@online{tiseoSafeCompliantControl2020,
  title = {Safe and {{Compliant Control}} of {{Redundant Robots Using Superimposition}} of {{Passive Task-Space Controllers}}},
  author = {Tiseo, Carlo and Merkt, Wolfgang and Wolfslag, Wouter and Vijayakumar, Sethu and Mistry, Michael},
  date = {2020-09-21},
  eprint = {2002.12249},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2002.12249},
  url = {http://arxiv.org/abs/2002.12249},
  urldate = {2022-08-15},
  abstract = {Safe and compliant control of dynamic systems in interaction with the environment, e.g., in shared workspaces, continues to represent a major challenge. Mismatches in the dynamic model of the robots, numerical singularities, and the intrinsic environmental unpredictability are all contributing factors. Online optimization of impedance controllers has recently shown great promise in addressing this challenge, however, their performance is not sufficiently robust to be deployed in challenging environments. This work proposes a compliant control method for redundant manipulators based on a superimposition of multiple passive task-space controllers in a hierarchy. Our control framework of passive controllers is inherently stable, numerically well-conditioned (as no matrix inversions are required), and computationally inexpensive (as no optimization is used). We leverage and introduce a novel stiffness profile for a recently proposed passive controller with smooth transitions between the divergence and convergence phases making it particularly suitable when multiple passive controllers are combined through superimposition. Our experimental results demonstrate that the proposed method achieves sub-centimeter tracking performance during demanding dynamic tasks with fast-changing references, while remaining safe to interact with and robust to singularities. he proposed framework achieves such results without knowledge of the robot dynamics and thanks to its passivity is intrinsically stable. The data further show that the robot can fully take advantage of the redundancy to maintain the primary task accuracy while compensating for unknown environmental interactions, which is not possible from current frameworks that require accurate contact information.},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/tiseo_et_al_2020_safe_and_compliant_control_of_redundant_robots_using_superimposition_of_passive.pdf}
}

@online{TNO,
  title = {About {{TNO}}},
  url = {https://www.tno.nl/en/about-tno/},
  urldate = {2019-11-24},
  langid = {english},
  organization = {{TNO}},
  keywords = {#nosource,robotic grasping,tno internship}
}

@online{TNOItsSocial,
  title = {{{TNO}} and Its Social Role},
  url = {/en/about-tno/tno-and-its-social-role/},
  urldate = {2019-11-24},
  abstract = {TNO is working on a better future. By its social role TNO has the ambition to make a demonstrable value to government and industry on the major social and ...},
  langid = {english},
  organization = {{TNO}},
  keywords = {#nosource,robotic grasping,tno internship}
}

@inproceedings{todiProbabilisticPathPlanning2019,
  title = {Probabilistic {{Path Planning}} Using {{Obstacle Trajectory Prediction}}},
  booktitle = {Proceedings of the {{ACM India Joint International Conference}} on {{Data Science}} and {{Management}} of {{Data}}},
  author = {Todi, Vasudha and Sengupta, Gunjan and Bhattacharya, Sourangshu},
  date = {2019},
  pages = {36--43},
  publisher = {{ACM}},
  keywords = {#nosource,path planning,robotic grasping,tno internship},
  annotation = {00000}
}

@inproceedings{todorovMujocoPhysicsEngine2012,
  title = {Mujoco: {{A}} Physics Engine for Model-Based Control},
  shorttitle = {Mujoco},
  booktitle = {2012 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  date = {2012},
  pages = {5026--5033},
  publisher = {{IEEE}},
  keywords = {#nosource,robotic grasping,simulation,tno internship}
}

@inproceedings{tomoriVariableImpedanceControl2013,
  title = {Variable Impedance Control with an Artificial Muscle Manipulator Using Instantaneous Force and {{MR}} Brake},
  booktitle = {2013 {{IEEE}}/{{RSJ}} International Conference on Intelligent Robots and Systems},
  author = {Tomori, Hiroki and Nagai, Suguru and Majima, Tatsuo and Nakamura, Taro},
  date = {2013},
  pages = {5396--5403},
  publisher = {{IEEE}}
}

@misc{towers_gymnasium_2023,
  title = {Gymnasium},
  author = {Towers, Mark and Terry, Jordan K. and Kwiatkowski, Ariel and Balis, John U. and family=Cola, given=Gianluca, prefix=de, useprefix=false and Deleu, Tristan and Goulão, Manuel and Kallinteris, Andreas and KG, Arjun and Krimmel, Markus and Perez-Vicente, Rodrigo and Pierré, Andrea and Schulhoff, Sander and Tai, Jun Jet and Shen, Andrew Tan Jin and Younis, Omar G.},
  date = {2023-03},
  doi = {10.5281/zenodo.8127026},
  url = {https://zenodo.org/record/8127025},
  urldate = {2023-07-08},
  abstract = {An API standard for single-agent reinforcement learning environments, with popular reference environments and related utilities (formerly Gym)},
  organization = {{Zenodo}}
}

@article{tsetserukouISoRAHumanoidRobot2009,
  title = {{{iSoRA}}: {{Humanoid Robot Arm}} for {{Intelligent Haptic Interaction}} with the {{Environment}}},
  shorttitle = {{{iSoRA}}},
  author = {Tsetserukou, Dzmitry and Kawakami, Naoki and Tachi, Susumu},
  date = {2009-01-01},
  journaltitle = {Advanced Robotics},
  volume = {23},
  number = {10},
  pages = {1327--1358},
  publisher = {{Taylor \& Francis}},
  issn = {0169-1864},
  doi = {10.1163/156855309X462619},
  url = {https://doi.org/10.1163/156855309X462619},
  urldate = {2022-06-23},
  abstract = {The paper concentrates on the development and control of the humanoid robot arm iSoRA, intended for operation in a dynamic unstructured environment. Optical torque sensors integrated into each joint enable measurement of contacting forces along the entire manipulator surface. A variable admittance control strategy was elaborated to increase the robot functionality and to achieve the human-like dynamics of interaction. The experimental results show that the proposed approach not only provides safe interaction of the robot arm with a person, but also improves the effectiveness of contact task performance. The paper also presents a novel concept of avoidance of an obstacle of unknown shape. The tactile sensory ability of the developed manipulator allows robot links to follow the object contour and to perform motion planning in the dynamic environment. The information on the applied normal force vector, object shape and target point coordinates is supplied to the motion planning system. The algorithms for contact point detection, object geometry recognition, and estimation of contacting object stiffness are detailed. The numerical simulation elicits a capability of the proposed method to approximate various object shapes precisely. The experimental results showed that the local admittance control and motion planner allowed the end-effector to follow the object contour in a very smooth, consistent manner while reaching the target point.},
  keywords = {HUMAN-ROBOT INTERACTION,HUMANOID ROBOT ARM,OBSTACLE AVOIDANCE,OPTICAL TORQUE SENSOR,VARIABLE ADMITTANCE CONTROL},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/tsetserukou_et_al_2009_isora.pdf}
}

@article{tsitsiklisAnalysisTemporaldifferenceLearning1997,
  title = {An Analysis of Temporal-Difference Learning with Function Approximation},
  author = {Tsitsiklis, J.N. and Van Roy, B.},
  date = {1997-05},
  journaltitle = {IEEE Transactions on Automatic Control},
  shortjournal = {IEEE Trans. Automat. Contr.},
  volume = {42},
  number = {5},
  pages = {674--690},
  issn = {00189286},
  doi = {10.1109/9.580874},
  url = {http://ieeexplore.ieee.org/document/580874/},
  urldate = {2020-07-02},
  abstract = {We discuss the temporal-difference learning algorithm, as applied to approximating the cost-to-go function of an infinite-horizon discounted Markov chain. The algorithm we analyze updates parameters of a linear function approximator online during a single endless trajectory of an irreducible aperiodic Markov chain with a finite or infinite state space. We present a proof of convergence (with probability one), a characterization of the limit of convergence, and a bound on the resulting approximation error. Furthermore, our analysis is based on a new line of reasoning that provides new intuition about the dynamics of temporal-difference learning.},
  langid = {english},
  keywords = {#nosource,classical RL,reinforcement learning,temporal difference learning}
}

@article{tsukamotoContractionTheoryNonlinear2021,
  title = {Contraction Theory for Nonlinear Stability Analysis and Learning-Based Control: {{A}} Tutorial Overview},
  shorttitle = {Contraction Theory for Nonlinear Stability Analysis and Learning-Based Control},
  author = {Tsukamoto, Hiroyasu and Chung, Soon-Jo and Slotine, Jean-Jaques E.},
  date = {2021-01-01},
  journaltitle = {Annual Reviews in Control},
  shortjournal = {Annual Reviews in Control},
  volume = {52},
  pages = {135--169},
  issn = {1367-5788},
  doi = {10.1016/j.arcontrol.2021.10.001},
  url = {https://www.sciencedirect.com/science/article/pii/S1367578821000766},
  urldate = {2023-01-19},
  abstract = {Contraction theory is an analytical tool to study differential dynamics of a non-autonomous (i.e., time-varying) nonlinear system under a contraction metric defined with a uniformly positive definite matrix, the existence of which results in a necessary and sufficient characterization of incremental exponential stability of multiple solution trajectories with respect to each other. By using a squared differential length as a Lyapunov-like function, its nonlinear stability analysis boils down to finding a suitable contraction metric that satisfies a stability condition expressed as a linear matrix inequality, indicating that many parallels can be drawn between well-known linear systems theory and contraction theory for nonlinear systems. Furthermore, contraction theory takes advantage of a superior robustness property of exponential stability used in conjunction with the comparison lemma. This yields much-needed safety and stability guarantees for neural network-based control and estimation schemes, without resorting to a more involved method of using uniform asymptotic stability for input-to-state stability. Such distinctive features permit systematic construction of a contraction metric via convex optimization, thereby obtaining an explicit exponential bound on the distance between a time-varying target trajectory and solution trajectories perturbed externally due to disturbances and learning errors. The objective of this paper is therefore to present a tutorial overview of contraction theory and its advantages in nonlinear stability analysis of deterministic and stochastic systems, with an emphasis on deriving formal robustness and stability guarantees for various learning-based and data-driven automatic control methods. In particular, we provide a detailed review of techniques for finding contraction metrics and associated control and estimation laws using deep neural networks.},
  langid = {english},
  keywords = {Adaptive control,Contraction theory,Data-driven control,Learning-based control,Nonlinear stability,Optimal control and estimation,READ,REVIEW,Robust control and estimation},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/tsukamoto_et_al_2021_contraction_theory_for_nonlinear_stability_analysis_and_learning-based_control.pdf}
}

@unpublished{tuckerMirageActionDependentBaselines2018,
  title = {The {{Mirage}} of {{Action-Dependent Baselines}} in {{Reinforcement Learning}}},
  author = {Tucker, George and Bhupatiraju, Surya and Gu, Shixiang and Turner, Richard E. and Ghahramani, Zoubin and Levine, Sergey},
  date = {2018-11-19},
  eprint = {1802.10031},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1802.10031},
  urldate = {2020-07-02},
  abstract = {Policy gradient methods are a widely used class of model-free reinforcement learning algorithms where a state-dependent baseline is used to reduce gradient estimator variance. Several recent papers extend the baseline to depend on both the state and action and suggest that this significantly reduces variance and improves sample efficiency without introducing bias into the gradient estimates. To better understand this development, we decompose the variance of the policy gradient estimator and numerically show that learned state-action-dependent baselines do not in fact reduce variance over a state-dependent baseline in commonly tested benchmark domains. We confirm this unexpected result by reviewing the open-source code accompanying these prior papers, and show that subtle implementation decisions cause deviations from the methods presented in the papers and explain the source of the previously observed empirical gains. Furthermore, the variance decomposition highlights areas for improvement, which we demonstrate by illustrating a simple change to the typical value function parameterization that can significantly improve performance.},
  keywords = {#nosource,action-dependent baseline,Computer Science - Machine Learning,model-free,policy gradients,reinforcement learning,Statistics - Machine Learning}
}

@inproceedings{uekiForceRestrainedControl2021,
  title = {Force Restrained Control to Extend Flexibility of Trajectory Planning},
  booktitle = {2021 {{IEEE International Conference}} on {{Mechatronics}} ({{ICM}})},
  author = {Ueki, Toshihiro and Sakaino, Sho and Tsuji, Toshiaki},
  date = {2021-03},
  pages = {1--6},
  doi = {10.1109/ICM46511.2021.9385600},
  abstract = {Force restraint in contact-rich tasks with robots is vital in order to avoid breaking objects. Impedance/admittance controls are effective for achieving the restraint; however, they require adjustment of the kinematic impedance, which may induce instability. We proposed a force restrain method while PD controller is used for avoiding unstable robot behavior. This method alters position command in case the robot may give extreme force which leads to damaging objects. This method expands the trajectory planning and motion generation area practically because it can be executed under the condition that the controller autonomously restrains excessive force. To examine the effectiveness of the proposed method, we experimented with contact and detach motion in 1- and 2- DoF translational directions.},
  eventtitle = {2021 {{IEEE International Conference}} on {{Mechatronics}} ({{ICM}})},
  keywords = {Assembly task,Force,Force restrainment,Robot kinematics,Robot sensing systems,Task analysis,Trajectory,Trajectory planning,Tuning},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/ueki_et_al_2021_force_restrained_control_to_extend_flexibility_of_trajectory_planning2.pdf}
}

@article{umlauftFeedbackLinearizationBased2020,
  title = {Feedback {{Linearization Based}} on {{Gaussian Processes With Event-Triggered Online Learning}}},
  author = {Umlauft, Jonas and Hirche, Sandra},
  date = {2020-10},
  journaltitle = {IEEE Transactions on Automatic Control},
  volume = {65},
  number = {10},
  pages = {4154--4169},
  issn = {1558-2523},
  doi = {10.1109/TAC.2019.2958840},
  abstract = {Combining control engineering with nonparametric modeling techniques from machine learning allows for the control of systems without analytic description using data-driven models. Most of the existing approaches separate learning, i.e., the system identification based on a fixed dataset, and control, i.e., the execution of the model-based control law. This separation makes the performance highly sensitive to the initial selection of training data and possibly requires very large datasets. This article proposes a learning feedback linearizing control law using online closed-loop identification. The employed Gaussian process model updates its training data only if the model uncertainty becomes too large. This event-triggered online learning ensures high data efficiency and thereby reduces computational complexity, which is a major barrier for using Gaussian processes under real-time constraints. We propose safe forgetting strategies of data points to adhere to budget constraints and to further increase data efficiency. We show asymptotic stability for the tracking error under the proposed event-triggering law and illustrate the effective identification and control in simulation.},
  eventtitle = {{{IEEE Transactions}} on {{Automatic Control}}},
  keywords = {Adaptation models,Adaptive control,Autoregressive processes,closed-loop identification,Computational modeling,Control systems,data-driven control,event-based control,Gaussian processes,Gaussian processes (GPs),machine learning,Noise measurement,online learning,READ,switched systems,Training data,uncertain systems},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/umlauft_hirche_2020_feedback_linearization_based_on_gaussian_processes_with_event-triggered_online.pdf}
}

@inproceedings{umlauftLearningStableGaussian2017,
  title = {Learning Stable {{Gaussian}} Process State Space Models},
  booktitle = {2017 {{American Control Conference}} ({{ACC}})},
  author = {Umlauft, Jonas and Lederer, Armin and Hirche, Sandra},
  date = {2017-05},
  pages = {1499--1504},
  issn = {2378-5861},
  doi = {10.23919/ACC.2017.7963165},
  abstract = {Data-driven nonparametric models gain importance as control systems are increasingly applied in domains where classical system identification is difficult, e.g., because of the system's complexity, sparse training data or its probabilistic nature. Gaussian process state space models (GP-SSM) are a data-driven approach which requires only high-level prior knowledge like smoothness characteristics. Prior known properties like stability are also often available but rarely exploited during modeling. The enforcement of stability using control Lyapunov functions allows to incorporate this prior knowledge, but requires a data-driven Lyapunov function search. Therefore, we propose the use of Sum of Squares to enforce convergence of GP-SSMs and compare the performance to other approaches on a real-world handwriting motion dataset.},
  eventtitle = {2017 {{American Control Conference}} ({{ACC}})},
  keywords = {Computational modeling,Convergence,Gaussian processes,Lyapunov methods,READ,Stability analysis,Training data},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/umlauft_et_al_2017_learning_stable_gaussian_process_state_space_models.pdf}
}

@inproceedings{umlauftLearningStableStochastic2017,
  title = {Learning Stable Stochastic Nonlinear Dynamical Systems},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Umlauft, Jonas and Hirche, Sandra},
  date = {2017},
  pages = {3502--3510},
  publisher = {{PMLR}},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/umlauft_hirche_2017_learning_stable_stochastic_nonlinear_dynamical_systems.pdf}
}

@article{umlauftLearningStochasticallyStable2020,
  title = {Learning Stochastically Stable {{Gaussian}} Process State–Space Models},
  author = {Umlauft, Jonas and Hirche, Sandra},
  date = {2020-06-01},
  journaltitle = {IFAC Journal of Systems and Control},
  shortjournal = {IFAC Journal of Systems and Control},
  volume = {12},
  pages = {100079},
  issn = {2468-6018},
  doi = {10.1016/j.ifacsc.2020.100079},
  url = {https://www.sciencedirect.com/science/article/pii/S2468601820300055},
  urldate = {2022-10-28},
  abstract = {Control systems are increasingly applied in domains where an analytic description of the system dynamics does not exist or is difficult to obtain. Example applications include autonomous robots in unstructured environments, human behavior modeling for prediction and action recognition in human–machine-interaction, and chemical process industry. In many of these cases, classical system identification is challenging, because a parametric model structure is unknown. Data-driven nonparametric models such as Gaussian process state–space models (GPSSMs) offer a suitable alternative: GPSSMs are known for their data-efficiency and rely on Bayesian principles to include prior knowledge. However, properties like stability or boundedness are often known a priori, but rarely exploited during modeling. We therefore propose a novel approach for learning GPSSMs subject to stability constraints. Our approach enforces the convergence using control Lyapunov functions which are also obtained in a data-driven fashion. We analyze the resulting dynamics with respect to convergence radius and data collection. In simulation, we illustrate the precision of the identified model on a real-world dataset of goal-directed motions.},
  langid = {english},
  keywords = {Gaussian processes,Machine learning,Probabilistic models,READ,State–space models,Stochastic modeling,System identification},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/umlauft_hirche_2020_learning_stochastically_stable_gaussian_process_state–space_models.pdf}
}

@article{umlauftUncertaintybasedControlLyapunov2018,
  title = {An Uncertainty-Based Control {{Lyapunov}} Approach for Control-Affine Systems Modeled by {{Gaussian}} Process},
  author = {Umlauft, Jonas and Pöhler, Lukas and Hirche, Sandra},
  date = {2018},
  journaltitle = {IEEE Control Systems Letters},
  volume = {2},
  number = {3},
  pages = {483--488},
  publisher = {{IEEE}},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/umlauft_et_al_2018_an_uncertainty-based_control_lyapunov_approach_for_control-affine_systems.pdf}
}

@inproceedings{umlauftUncertaintybasedHumanMotion2019,
  title = {Uncertainty-Based {{Human Motion Tracking}} with {{Stable Gaussian Process State Space Models}}},
  booktitle = {Ifac {{Papersonline}}},
  author = {Umlauft, Lukas Poehler Jonas and Hirche, Sandra},
  date = {2019},
  volume = {51},
  number = {34},
  pages = {8--14},
  publisher = {{Elsevier Science Bv}},
  location = {{Amsterdam}},
  issn = {2405-8963},
  doi = {10.1016/j.ifacol.2019.01.002},
  url = {https://www.sciencedirect.com/science/article/pii/S2405896319300023},
  urldate = {2022-12-06},
  abstract = {Data-driven approaches are well suited to represent human motion because arbitrary complex trajectories can be captured. Gaussian process state space models allow to encode human motion while quantifying uncertainty due to missing data. Such human motion models are relevant for many application domains such as learning by demonstration and motion prediction in human-robot collaboration. For goal-directed tasks it is essential to impose stability constraints on the model representing the human motion. Motivated by learning by demonstration applications, this paper proposes an uncertainty-based control Lyapunov function approach for goal-directed path tracking. We exploit the model fidelity which is related to the location of the training and test data: Our approach actively strives into regions with more demonstration data and thus higher model certainty. This achieves accurate reproduction of the human motion independent of the initial condition and we show that generated trajectories are uniformly globally asymptotically stable. The approach is validated in a nonlinear learning by demonstration task where human-demonstrated motions are reproduced by the learned dynamical system, and higher precision than competitive state of the art methods is achieved. (C) 2019, IFAC (International Federation of Automatic Control) Hosting by Elsevier Ltd. All rights reserved.},
  langid = {english},
  keywords = {Control under   uncertainty,Human centered automation,Lyapunov methods,Modeling of human performance,Nonlinear system identification,Path tracking,READ},
  annotation = {WOS:000458143400003},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/umlauft_hirche_2019_uncertainty-based_human_motion_tracking_with_stable_gaussian_process_state.pdf}
}

@online{unsplashPhotoKalenEmsley,
  title = {Photo by {{Kalen Emsley}} on {{Unsplash}}},
  author = {Unsplash},
  url = {https://unsplash.com/photos/Bkci_8qcdvQ},
  urldate = {2022-09-14},
  abstract = {Wet mountain valley – Download this photo by Kalen Emsley on Unsplash},
  langid = {english}
}

@inproceedings{urainImitationFlowLearningDeep2020,
  title = {{{ImitationFlow}}: {{Learning Deep Stable Stochastic Dynamic Systems}} by {{Normalizing Flows}}},
  shorttitle = {{{ImitationFlow}}},
  booktitle = {2020 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Urain, Julen and Ginesi, Michele and Tateo, Davide and Peters, Jan},
  date = {2020-10},
  pages = {5231--5237},
  issn = {2153-0866},
  doi = {10.1109/IROS45743.2020.9341035},
  abstract = {We introduce ImitationFlow, a novel Deep generative model that allows learning complex globally stable, stochastic, nonlinear dynamics. Our approach extends the Normalizing Flows framework to learn stable Stochastic Differential Equations. We prove the Lyapunov stability for a class of Stochastic Differential Equations and we propose a learning algorithm to learn them from a set of demonstrated trajectories. Our model extends the set of stable dynamical systems that can be represented by state-of-the-art approaches, eliminates the Gaussian assumption on the demonstrations, and outperforms the previous algorithms in terms of representation accuracy. We show the effectiveness of our method with both standard datasets and a real robot experiment.},
  eventtitle = {2020 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  keywords = {Differential equations,Heuristic algorithms,Mathematical model,Nonlinear dynamical systems,READ,Standards,Stochastic processes,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/urain_et_al_2020_imitationflow.pdf}
}

@online{urainLearningStableVector2022,
  title = {Learning {{Stable Vector Fields}} on {{Lie Groups}}},
  author = {Urain, Julen and Tateo, Davide and Peters, Jan},
  date = {2022-10-01},
  eprint = {2110.11774},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2110.11774},
  url = {http://arxiv.org/abs/2110.11774},
  urldate = {2022-10-28},
  abstract = {Learning robot motions from demonstration requires models able to specify vector fields for the full robot pose when the task is defined in operational space. Recent advances in reactive motion generation have shown that learning adaptive, reactive, smooth, and stable vector fields is possible. However, these approaches define vector fields on a flat Euclidean manifold, while representing vector fields for orientations requires modeling the dynamics in non-Euclidean manifolds, such as Lie Groups. In this paper, we present a novel vector field model that can guarantee most of the properties of previous approaches i.e., stability, smoothness, and reactivity beyond the Euclidean space. In the experimental evaluation, we show the performance of our proposed vector field model to learn stable vector fields for full robot poses as SE(2) and SE(3) in both simulated and real robotics tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/urain_et_al_2022_learning_stable_vector_fields_on_lie_groups.pdf}
}

@online{urainStructuredPolicyRepresentation2020,
  title = {Structured {{Policy Representation}}: {{Imposing Stability}} in Arbitrarily Conditioned Dynamic Systems},
  shorttitle = {Structured {{Policy Representation}}},
  author = {Urain, Julen and Tateo, Davide and Ren, Tianyu and Peters, Jan},
  date = {2020-12-11},
  eprint = {2012.06224},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2012.06224},
  urldate = {2023-01-18},
  abstract = {We present a new family of deep neural network-based dynamic systems. The presented dynamics are globally stable and can be conditioned with an arbitrary context state. We show how these dynamics can be used as structured robot policies. Global stability is one of the most important and straightforward inductive biases as it allows us to impose reasonable behaviors outside the region of the demonstrations.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/urain_et_al_2020_structured_policy_representation.pdf}
}

@book{vanderschaftL2GainPassivityTechniques2017,
  title = {L2-{{Gain}} and {{Passivity Techniques}} in {{Nonlinear Control}}},
  author = {family=Schaft, given=Arjan, prefix=van der, useprefix=true},
  date = {2017},
  series = {Communications and {{Control Engineering}}},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-49992-5},
  url = {http://link.springer.com/10.1007/978-3-319-49992-5},
  urldate = {2022-06-13},
  isbn = {978-3-319-49991-8 978-3-319-49992-5},
  langid = {english},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/van_der_schaft_2017_l2-gain_and_passivity_techniques_in_nonlinear_control.pdf}
}

@incollection{vanderschaftPortHamiltonianSystemsIntroductory2007,
  title = {Port-{{Hamiltonian}} Systems: An Introductory Survey},
  shorttitle = {Port-{{Hamiltonian}} Systems},
  booktitle = {Proceedings of the {{International Congress}} of {{Mathematicians Madrid}}, {{August}} 22–30, 2006},
  author = {family=Schaft, given=Arjan, prefix=van der, useprefix=true},
  editor = {Sanz-Solé, Marta and Soria, Javier and Varona, Juan Luis and Verdera, Joan},
  date = {2007-05-15},
  pages = {1339--1365},
  publisher = {{European Mathematical Society Publishing House}},
  location = {{Zuerich, Switzerland}},
  doi = {10.4171/022-3/65},
  url = {https://ems.press/doi/10.4171/022-3/65},
  urldate = {2022-08-17},
  abstract = {The theory of port-Hamiltonian systems provides a framework for the geometric description of network models of physical systems. It turns out that port-based network models of physical systems immediately lend themselves to a Hamiltonian description. While the usual geometric approach to Hamiltonian systems is based on the canonical symplectic structure of the phase space or on a Poisson structure that is obtained by (symmetry) reduction of the phase space, in the case of a port-Hamiltonian system the geometric structure derives from the interconnection of its sub-systems. This motivates to consider Dirac structures instead of Poisson structures, since this notion enables one to define Hamiltonian systems with algebraic constraints. As a result, any power-conserving interconnection of port-Hamiltonian systems again defines a port-Hamiltonian system.},
  isbn = {978-3-03719-022-7},
  langid = {english},
  file = {/home/ricks/Zotero/storage/XBLYG736/ICMvanderSchaft.pdf}
}

@unpublished{vanhasseltDeepReinforcementLearning2015,
  title = {Deep {{Reinforcement Learning}} with {{Double Q-learning}}},
  author = {family=Hasselt, given=Hado, prefix=van, useprefix=true and Guez, Arthur and Silver, David},
  date = {2015-12-08},
  eprint = {1509.06461},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1509.06461},
  urldate = {2020-07-02},
  abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
  keywords = {#nosource,Computer Science - Machine Learning,DQL,model-free,reinforcement learning},
  annotation = {02177}
}

@article{vanspaandonkLearningVariableImpedance2016,
  title = {Learning {{Variable Impedance Control}}: {{A Model-Based Approach Using Gaussian Processes}}},
  shorttitle = {Learning {{Variable Impedance Control}}},
  author = {family=Spaandonk, given=V., prefix=van, useprefix=true},
  date = {2016},
  url = {http://repository.tudelft.nl/islandora/object/uuid%3A79ab3ded-be04-4343-87c9-e3581ca948f2},
  urldate = {2022-05-02},
  abstract = {Modern robotic systems are increasingly expected to interact with unstructured and unpredictable environments. This has reiterated the importance of sophisticated reasoning and adaptive motor skill learning. Although low-level methodologies for sensorimotor control have been relatively well studied, constrained motion for robotic manipulators in general environments still remains a challenge. In addition to viable kinematic trajectories, successful interaction requires a system to adjust the magnitude and direction of the force applied on an object or human being. One force control architecture that is stable under a wide range of conditions is impedance control. However the defining inertial, damping and stiffness parameters are highly task dependent and often difficult to deduce a priori. To this end, one promising strategy is through reinforcement learning. Several frameworks have already emerged that are capable of learning compliant behaviour in this fashion. However the complex and sometimes discontinuous nature of physical interaction in robotics provides additional challenges in designing algorithms capable of learning complex behaviours with minimal interaction time. This thesis details an extension to the PILCO algorithm for learning variable impedance control. The proposed method attempts to construct a Gaussian Process model of the robot-environment system through interaction. This approach permits long-term inference and planning in a fully Bayesian manner, reducing the required interaction time for convergence and allow for efficient analytical gradient-based policy updates. Two skill learning problems are investigated both in simulation and experiment on a 7-DOF `KUKA LBR iiwa 7 R800' robot. The first scenario entails learning state-conditioned variable stiffness parameters along predefined motion plans. In the second framework, the agent learns both stiffness parameters and kinematic trajectories simultaneously to complete a constrained motion task.},
  langid = {english},
  keywords = {READ,REVIEW},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/van_spaandonk_2016_learning_variable_impedance_control.pdf}
}

@inproceedings{varleyGeneratingMultifingeredRobotic2015,
  title = {Generating Multi-Fingered Robotic Grasps via Deep Learning},
  booktitle = {2015 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Varley, Jacob and Weisz, Jonathan and Weiss, Jared and Allen, Peter},
  date = {2015},
  pages = {4415--4420},
  publisher = {{IEEE}},
  keywords = {#nosource,object grasping,robotic grasping,tno internship}
}

@unpublished{vezhnevetsFeUdalNetworksHierarchical2017,
  title = {{{FeUdal Networks}} for {{Hierarchical Reinforcement Learning}}},
  author = {Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
  date = {2017-03-06},
  eprint = {1703.01161},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1703.01161},
  urldate = {2020-07-02},
  abstract = {We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning. Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton, and gains power and efficacy by decoupling end-to-end learning across multiple levels -- allowing it to utilise different resolutions of time. Our framework employs a Manager module and a Worker module. The Manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates primitive actions at every tick of the environment. The decoupled structure of FuN conveys several benefits -- in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the Manager. These properties allow FuN to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation. We demonstrate the performance of our proposed system on a range of tasks from the ATARI suite and also from a 3D DeepMind Lab environment.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,hierarchical RL,reinforcement learning},
  annotation = {00335}
}

@book{vidyasagarNonlinearSystemsAnalysis2002,
  title = {Nonlinear Systems Analysis},
  author = {Vidyasagar, Mathukumalli},
  date = {2002},
  publisher = {{SIAM}},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/vidyasagar_2002_nonlinear_systems_analysis.pdf}
}

@unpublished{viereckLearningVisuomotorController2017,
  title = {Learning a Visuomotor Controller for Real World Robotic Grasping Using Simulated Depth Images},
  author = {Viereck, Ulrich and family=Pas, given=Andreas, prefix=ten, useprefix=false and Saenko, Kate and Platt, Robert},
  date = {2017-06-14},
  eprint = {1706.04652},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1706.04652},
  urldate = {2019-04-15},
  abstract = {We want to build robots that are useful in unstructured real world applications, such as doing work in the household. Grasping in particular is an important skill in this domain, yet it remains a challenge. One of the key hurdles is handling unexpected changes or motion in the objects being grasped and kinematic noise or other errors in the robot. This paper proposes an approach to learning a closed-loop controller for robotic grasping that dynamically guides the gripper to the object. We use a wrist-mounted sensor to acquire depth images in front of the gripper and train a convolutional neural network to learn a distance function to true grasps for grasp configurations over an image. The training sensor data is generated in simulation, a major advantage over previous work that uses real robot experience, which is costly to obtain. Despite being trained in simulation, our approach works well on real noisy sensor images. We compare our controller in simulated and real robot experiments to a strong baseline for grasp pose detection, and find that our approach significantly outperforms the baseline in the presence of kinematic noise, perceptual errors and disturbances of the object during grasping.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Robotics,object grasping,robotic grasping,tno internship}
}

@article{villaniExponentiallyStableAdaptive1999,
  title = {An Exponentially Stable Adaptive Control for Force and Position Tracking of Robot Manipulators},
  author = {Villani, L. and De Wit, C.C. and Brogliato, B.},
  date = {1999-04},
  journaltitle = {IEEE Transactions on Automatic Control},
  volume = {44},
  number = {4},
  pages = {798--802},
  issn = {1558-2523},
  doi = {10.1109/9.754821},
  abstract = {The problem of controlling a robot manipulator while the end effector is in contact with an environment of finite but unknown stiffness is considered. An exponentially stable control law is derived starting from a passivity-based position control algorithm. The original position trajectory is scaled along the interaction direction so as to achieve force tracking as well as position tracking along the unconstrained directions. A passivity-based adaptive algorithm is designed to avoid the explicit computation of the scaling factor, which depends on the unknown stiffness of the environment, leading to time-varying PID control actions on the force error.},
  eventtitle = {{{IEEE Transactions}} on {{Automatic Control}}},
  keywords = {Adaptive algorithm,Adaptive control,Algorithm design and analysis,End effectors,Force control,Manipulators,Position control,Robot control,Three-term control,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/villani_et_al_1999_an_exponentially_stable_adaptive_control_for_force_and_position_tracking_of.pdf}
}

@incollection{villaniForceControl2016,
  title = {Force {{Control}}},
  booktitle = {Springer {{Handbook}} of {{Robotics}}},
  author = {Villani, Luigi and De Schutter, Joris},
  editor = {Siciliano, Bruno and Khatib, Oussama},
  date = {2016},
  series = {Springer {{Handbooks}}},
  pages = {195--220},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-32552-1_9},
  url = {https://doi.org/10.1007/978-3-319-32552-1_9},
  urldate = {2022-05-27},
  abstract = {A~fundamental requirement for the success of a~manipulation task is the capability to handle the physical contact between a~robot and the environment. Pure motion control turns out to be inadequate because the unavoidable modeling errors and uncertainties may cause a~rise of the contact force, ultimately leading to an unstable behavior during the interaction, especially in the presence of rigid environments. Force feedback and force control becomes mandatory to achieve a~robust and versatile behavior of a~robotic system in poorly structured environments as well as safe and dependable operation in the presence of humans. This chapter starts from the analysis of indirect force control strategies, conceived to keep the contact forces limited by ensuring a~suitable compliant behavior to the end effector, without requiring an accurate model of the environment. Then the problem of interaction tasks modeling is analyzed, considering both the case of a~rigid environment and the case of a~compliant environment. For the specification of an interaction task, natural constraints set by the task geometry and artificial constraints set by the control strategy are established, with respect to suitable task frames. This formulation is the essential premise to the synthesis of hybrid force/motion control schemes.},
  isbn = {978-3-319-32552-1},
  langid = {english},
  keywords = {Active Interaction Control,Indirect Force Control,Motion Controller,Rigid Environment,Task Frame},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/villani_de_schutter_2016_force_control.pdf}
}

@article{vlachosControlSchemeNovel2020,
  title = {A {{Control Scheme With}} a {{Novel DMP-Robot Coupling Achieving Compliance}} and {{Tracking Accuracy Under Unknown Task Dynamics}} and {{Model Uncertainties}}},
  author = {Vlachos, Konstantinos and Doulgeri, Zoe},
  date = {2020-04},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {5},
  number = {2},
  pages = {2310--2316},
  issn = {2377-3766},
  doi = {10.1109/LRA.2020.2970985},
  abstract = {A control scheme consisting of a novel coupling of a DMP based virtual reference with a low stiffness controlled robot is proposed. The overall system is proved to achieve superior tracking of a DMP encoded trajectory and accurate target reaching with respect to the conventional scheme under the presence of constant and periodic disturbances owing to unknown task dynamics and robot model uncertainties. It further preserves the desired compliance under contact forces that may arise in human interventions and collisions. Results in simulations and experiments validate the theoretical findings.},
  eventtitle = {{{IEEE Robotics}} and {{Automation Letters}}},
  keywords = {Compliance and impedance control,Couplings,Dynamics,motion control,Robots,Target tracking,Task analysis,Trajectory,Uncertainty},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/vlachos_doulgeri_2020_a_control_scheme_with_a_novel_dmp-robot_coupling_achieving_compliance_and2.pdf}
}

@unpublished{wagenerOnlineLearningApproach2019,
  title = {An {{Online Learning Approach}} to {{Model Predictive Control}}},
  author = {Wagener, Nolan and Cheng, Ching-An and Sacks, Jacob and Boots, Byron},
  date = {2019-02-24},
  eprint = {1902.08967},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1902.08967},
  urldate = {2019-04-18},
  abstract = {Model predictive control (MPC) is a powerful technique for solving dynamic control tasks. In this paper, we show that there exists a close connection between MPC and online learning, an abstract theoretical framework for analyzing online decision making in the optimization literature. This new perspective provides a foundation for leveraging powerful online learning algorithms to design MPC algorithms. Specifically, we propose a new algorithm based on dynamic mirror descent (DMD), an online learning algorithm that is designed for non-stationary setups. Our algorithm, Dynamic Mirror Decent Model Predictive Control (DMD-MPC), represents a general family of MPC algorithms that includes many existing techniques as special instances. DMD-MPC also provides a fresh perspective on previous heuristics used in MPC and suggests a principled way to design new MPC algorithms. In the experimental section of this paper, we demonstrate the flexibility of DMD-MPC, presenting a set of new MPC algorithms on a simple simulated cartpole and a simulated and real-world aggressive driving task.},
  langid = {english},
  keywords = {#nosource,Computer Science - Machine Learning,Computer Science - Robotics,machine learning control}
}

@article{wahballaConstantForceTracking2022,
  title = {Constant Force Tracking Using Online Stiffness and Reverse Damping Force of Variable Impedance Controller for Robotic Polishing},
  author = {Wahballa, Hosham and Duan, Jinjun and Dai, Zhendong},
  date = {2022-08-01},
  journaltitle = {The International Journal of Advanced Manufacturing Technology},
  shortjournal = {Int J Adv Manuf Technol},
  volume = {121},
  number = {9},
  pages = {5855--5872},
  issn = {1433-3015},
  doi = {10.1007/s00170-022-09599-x},
  url = {https://doi.org/10.1007/s00170-022-09599-x},
  urldate = {2022-08-11},
  abstract = {This paper proposes a novel constant force tracking control scheme based on an impedance controller with online stiffness and reverse damping force (OSRDF) to track desired force. An interaction contact force between the robot end-effector and its environment was represented and analyzed using the full mechanical second-order system and individual spring model. A position-based impedance controller is used to receive a contact force signal to track the constant desired force. The proposed approach tracks the desired contact force and reference trajectory based on reference position and velocity. This OSRDF controller is implemented by adjusting the online stiffness parameter and merging the inverse damping force with the force tracking error to compensate for the unknown environment and reduce the force error to zero. A Lyapunov function is applied to investigate the stability of the OSRDF impedance controller during implementation. Simulation studies and experimental tests on a seven degree of freedom (7DOF) robot manipulator are performed to evaluate the efficiency of the proposed method compared to the traditional constant impedance controller. The results showed the validity and effectiveness of the OSRDF method and revealed a relationship between the end-effector velocity and force tracking error. The proposed approach improves force tracking accuracy with simpler computational processes in simulation and practice.},
  langid = {english},
  keywords = {Constant force,Online stiffness,OSRDF impedance control,READ,Reverse damping force,X-mate},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/wahballa_et_al_2022_constant_force_tracking_using_online_stiffness_and_reverse_damping_force_of.pdf}
}

@inproceedings{wahrburgMPCbasedAdmittanceControl2016,
  title = {{{MPC-based}} Admittance Control for Robotic Manipulators},
  booktitle = {2016 {{IEEE}} 55th {{Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Wahrburg, Arne and Listmann, Kim},
  date = {2016-12},
  pages = {7548--7554},
  doi = {10.1109/CDC.2016.7799435},
  abstract = {Robotic applications involving environmental interaction require control structures that go beyond traditional position control. One approach to handle interaction forces is admittance control. Therein, position or speed reference values are modified based on interaction forces. To this end, the underlying position control scheme, widely used in industrial manipulators, is augmented with an additional admittance control loop. It is well known that such approaches raise stability issues in the interaction with stiff environments (termed contact stability). As a consequence, in traditional admittance control, the speed of operation needs to be reduced in order to prevent excessive contact force magnitudes and to ensure stability. In this paper, we propose an MPC-based admittance control scheme to circumvent both and discuss its properties and difficulties for practical use.},
  eventtitle = {2016 {{IEEE}} 55th {{Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  keywords = {Admittance,Aerospace electronics,Contact instabilities,Kinematics,Manipulator dynamics,Service robots},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/wahrburg_listmann_2016_mpc-based_admittance_control_for_robotic_manipulators.pdf}
}

@book{wallenHistoryIndustrialRobot2008,
  title = {The {{History}} of the {{Industrial Robot}}},
  author = {Wallén, Johanna},
  date = {2008},
  publisher = {{Linköping University Electronic Press}},
  url = {http://urn.kb.se/resolve?urn=urn:nbn:se:liu:diva-56167},
  urldate = {2019-11-28},
  abstract = {DiVA portal is a finding tool for research publications and student theses written at the following 49 universities and research institutions.},
  langid = {english},
  keywords = {#nosource,machine learning control}
}

@unpublished{wangBenchmarkingModelBasedReinforcement2019,
  title = {Benchmarking {{Model-Based Reinforcement Learning}}},
  author = {Wang, Tingwu and Bao, Xuchan and Clavera, Ignasi and Hoang, Jerrick and Wen, Yeming and Langlois, Eric and Zhang, Shunshi and Zhang, Guodong and Abbeel, Pieter and Ba, Jimmy},
  date = {2019-07-03},
  eprint = {1907.02057},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1907.02057},
  urldate = {2020-07-02},
  abstract = {Model-based reinforcement learning (MBRL) is widely seen as having the potential to be significantly more sample efficient than model-free RL. However, research in model-based RL has not been very standardized. It is fairly common for authors to experiment with self-designed environments, and there are several separate lines of research, which are sometimes closed-sourced or not reproducible. Accordingly, it is an open question how these various existing MBRL algorithms perform relative to each other. To facilitate research in MBRL, in this paper we gather a wide collection of MBRL algorithms and propose over 18 benchmarking environments specially designed for MBRL. We benchmark these algorithms with unified problem settings, including noisy environments. Beyond cataloguing performance, we explore and unify the underlying algorithmic differences across MBRL algorithms. We characterize three key research challenges for future MBRL research: the dynamics bottleneck, the planning horizon dilemma, and the early-termination dilemma. Finally, to maximally facilitate future research on MBRL, we open-source our benchmark in http://www.cs.toronto.edu/\textasciitilde tingwuwang/mbrl.html.},
  keywords = {#nosource,benchmarks,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,reinforcement learning,Statistics - Machine Learning},
  annotation = {00000}
}

@online{wangDatadrivenPredictiveTracking2022,
  title = {Data-Driven {{Predictive Tracking Control}} Based on {{Koopman Operators}}},
  author = {Wang, Ye and Yang, Yujia and Pu, Ye and Manzie, Chris},
  date = {2022-08-25},
  eprint = {2208.12000},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2208.12000},
  urldate = {2023-01-31},
  abstract = {We seek to combine the nonlinear modeling capabilities of a wide class of neural networks with the safety guarantees of model predictive control (MPC) in a rigorous and online computationally tractable framework. The class of networks considered can be captured using Koopman operators, and are integrated into a Koopman-based tracking MPC (KTMPC) for nonlinear systems to track piecewise constant references. The effect of model mismatch between original nonlinear dynamics and its trained Koopman linear model is handled by using a constraint tightening approach in the proposed tracking MPC strategy. By choosing two Lyapunov candidate functions, we prove that solution is recursively feasible and input-to-state stable to a neighborhood of both online and offline optimal reachable steady outputs in the presence of bounded modeling errors. Finally, we show the results of a numerical example and an application of autonomous ground vehicle to track given references.},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/wang_et_al_2022_data-driven_predictive_tracking_control_based_on_koopman_operators.pdf}
}

@article{wangDeepDeterministicPolicy2022,
  title = {Deep {{Deterministic Policy Gradient}} with {{Reward Function Based}} on {{Fuzzy Logic}} for {{Robotic Peg-in-Hole Assembly Tasks}}},
  author = {Wang, Ziyue and Li, Fengming and Men, Yu and Fu, Tianyu and Yang, Xuting and Song, Rui},
  date = {2022-01},
  journaltitle = {Applied Sciences},
  volume = {12},
  number = {6},
  pages = {3181},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2076-3417},
  doi = {10.3390/app12063181},
  url = {https://www.mdpi.com/2076-3417/12/6/3181},
  urldate = {2022-04-29},
  abstract = {Robot automatic assembly of weak stiffness parts is difficult due to potential deformation during assembly. The robot manipulation cannot adapt to the dynamic contact changes during the assembly process. A robot assembly skill learning system is designed by combining the compliance control and deep reinforcement, which could acquire a better robot assembly strategy. In this paper, a robot assembly strategy learning method based on variable impedance control is proposed to solve the robot assembly contact tasks. During the assembly process, the quality evaluation is designed based on fuzzy logic, and the impedance parameters in the assembly process are studied with a deep deterministic policy gradient. Finally, the effectiveness of the method is verified using the KUKA iiwa robot in the weak stiffness peg-in-hole assembly. Experimental results show that the robot obtains the robot assembly strategy with variable compliant in the process of weak stiffness peg-in-hole assembly. Compared with the previous methods, the assembly success rate of the proposed method reaches 100\%.},
  issue = {6},
  langid = {english},
  keywords = {compliant control,deep reinforcement learning,fuzzy reward,READ,robot assembly},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/wang_et_al_2022_deep_deterministic_policy_gradient_with_reward_function_based_on_fuzzy_logic.pdf}
}

@unpublished{wangDuelingNetworkArchitectures2016,
  title = {Dueling {{Network Architectures}} for {{Deep Reinforcement Learning}}},
  author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and family=Hasselt, given=Hado, prefix=van, useprefix=true and Lanctot, Marc and family=Freitas, given=Nando, prefix=de, useprefix=true},
  date = {2016-04-05},
  eprint = {1511.06581},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1511.06581},
  urldate = {2020-07-02},
  abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
  keywords = {#nosource,Computer Science - Machine Learning,DQL,model-free,reinforcement learning},
  annotation = {01181}
}

@article{wangLearningDeepRobotic2022,
  title = {Learning {{Deep Robotic Skills}} on {{Riemannian}} Manifolds},
  author = {Wang, Weitao and Saveriano, Matteo and Abu-Dakka, Fares J.},
  date = {2022},
  journaltitle = {IEEE Access},
  pages = {1--1},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3217800},
  abstract = {In this paper, we propose RiemannianFlow, a deep generative model that allows robots to learn complex and stable skills evolving on the Riemannian manifold. Examples of Riemannian data in robotics include stiffness (symmetric and positive definite matrix (SPD)) and orientation (unit quaternion (UQ)) trajectories. For Riemannian data, unlike Euclidean ones, different dimensions are interconnected by geometric constraints which have to be properly considered during the learning process. Using distance preserving mappings, our approach transfers the data between their original manifold and the tangent space, realizing the removing and re-fulfilling of the constraints. This allows to extend existing frameworks to learn stable skills from Riemannian data while guaranteeing the stability of the learning results. The ability of RiemannianFlow to learn various data patterns and the stability of the learned models are experimentally shown on a dataset of manifold motions. Further, we analyze from different perspectives the robustness of the model with different hyperparameter combinations. While stability is not affected by different hyperparameters, a proper choice of the hyperparameters leads to a significant improvement (up to 27.6\%) in the model accuracy. Last, we show the effectiveness of RiemannianFlow in a real peg-in-hole (PiH) task where we need to generate stable and consistent position and orientation trajectories for the robot starting from different initial poses.},
  eventtitle = {{{IEEE Access}}},
  keywords = {Compliance and Impedance Control,Deep learning,Deep Learning Methods,Learning from Demonstration,Manifolds,Motion control,Motion Control of Manipulators,READ,Riemannian Manifold,Robot sensing systems,Robots,Stability criteria,Symmetric matrices,Task analysis,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/wang_et_al_2022_learning_deep_robotic_skills_on_riemannian_manifolds.pdf}
}

@inproceedings{wangLearningDemonstrationUsing2021,
  title = {Learning from Demonstration Using Improved Dynamic Movement Primitives},
  booktitle = {2021 {{IEEE}} 16th {{Conference}} on {{Industrial Electronics}} and {{Applications}} ({{ICIEA}})},
  author = {Wang, Tiantian and Yan, Liang and Wang, Gang and Gao, Xiaoshan and Du, Nannan and Chen, I-Ming},
  date = {2021-08},
  pages = {2130--2135},
  issn = {2158-2297},
  doi = {10.1109/ICIEA51954.2021.9516425},
  abstract = {It is important to endow the robot with the ability of learning the complex motion sequences and thus adopt such motions when facing to changeable environment. This paper proposes an improved Dynamic Movement Primitives (DMP) method. In order to solve the problem of invalidation of forcing term in conventional DMP, the improved DMP approach, i.e., the DMP together with Deep Neural Network (DNN), is proposed. Specially, DNN is introduced to fit the target nonlinear function with the demonstrated trajectory information, instead of using a specific formula to describe the forcing term in DMP. Thus, improved DMP method can avoid the drawback of conventional DMP. Simulation work is conducted and the results show that the invalidation performance of forcing term is improved compared with conventional DMP. In addition, the generalization property of improved DMP is also beneficial to work environmental adaptability.},
  eventtitle = {2021 {{IEEE}} 16th {{Conference}} on {{Industrial Electronics}} and {{Applications}} ({{ICIEA}})},
  keywords = {Adaptation models,Conferences,Deep learning,DMP,DNN,Dynamics,Fitting of non-linear function,Industrial electronics,READ,Simulation,Target tracking,Trajectory learning},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/wang_et_al_2021_learning_from_demonstration_using_improved_dynamic_movement_primitives.pdf}
}

@unpublished{wangLearningReinforcementLearn2017,
  title = {Learning to Reinforcement Learn},
  author = {Wang, Jane X. and Kurth-Nelson, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z. and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
  date = {2017-01-23},
  eprint = {1611.05763},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1611.05763},
  urldate = {2020-07-02},
  abstract = {In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,meta-RL,reinforcement learning,Statistics - Machine Learning}
}

@article{wangMultimodalGraspData2019,
  title = {Multimodal Grasp Data Set: {{A}} Novel Visual–Tactile Data Set for Robotic Manipulation},
  shorttitle = {Multimodal Grasp Data Set},
  author = {Wang, Tao and Yang, Chao and Kirchner, Frank and Du, Peng and Sun, Fuchun and Fang, Bin},
  date = {2019-01-01},
  journaltitle = {International Journal of Advanced Robotic Systems},
  shortjournal = {International Journal of Advanced Robotic Systems},
  volume = {16},
  number = {1},
  pages = {1729881418821571},
  issn = {1729-8814},
  doi = {10.1177/1729881418821571},
  url = {https://doi.org/10.1177/1729881418821571},
  urldate = {2019-05-09},
  abstract = {This article introduces a visual–tactile multimodal grasp data set, aiming to further the research on robotic manipulation. The data set was built by the novel designed dexterous robot hand, the Intel’s Eagle Shoal robot hand (Intel Labs China, Beijing, China). The data set contains 2550 sets data, including tactile, joint, time label, image, and RGB and depth video. With the integration of visual and tactile data, researchers could be able to better understand the grasping process and analyze the deeper grasping issues. In this article, the building process of the data set was introduced, as well as the data set composition. In order to evaluate the quality of data set, the tactile data were analyzed by short-time Fourier transform. The tactile data–based slip detection was realized by long short-term memory and contrasted with visual data. The experiments compared the long short-term memory with the traditional classifiers, and generalization ability on different grasp directions and different objects is implemented. The results have proved that the data set’s value in promoting research on robotic manipulation area showed the effective slip detection and generalization ability of long short-term memory. Further work on visual and tactile data will be devoted to in the future.},
  langid = {english},
  keywords = {#nosource,object manipulation,robotic grasping,tno internship},
  annotation = {00000}
}

@article{wangPartialContractionAnalysis2005,
  title = {On Partial Contraction Analysis for Coupled Nonlinear Oscillators},
  author = {Wang, Wei and Slotine, Jean-Jacques E.},
  date = {2005},
  journaltitle = {Biological cybernetics},
  volume = {92},
  number = {1},
  pages = {38--53},
  publisher = {{Springer}},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/wang_slotine_2005_on_partial_contraction_analysis_for_coupled_nonlinear_oscillators.pdf}
}

@inproceedings{wangPredrnnRecurrentNeural2017,
  title = {Predrnn: {{Recurrent}} Neural Networks for Predictive Learning Using Spatiotemporal Lstms},
  shorttitle = {Predrnn},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wang, Yunbo and Long, Mingsheng and Wang, Jianmin and Gao, Zhifeng and Philip, S. Yu},
  date = {2017},
  pages = {879--888},
  keywords = {#nosource,path planning,robotic grasping,tno internship},
  annotation = {00016}
}

@unpublished{wangPredRNNResolutionDeepinTime2018,
  title = {{{PredRNN}}++: {{Towards A Resolution}} of the {{Deep-in-Time Dilemma}} in {{Spatiotemporal Predictive Learning}}},
  shorttitle = {{{PredRNN}}++},
  author = {Wang, Yunbo and Gao, Zhifeng and Long, Mingsheng and Wang, Jianmin and Yu, Philip S.},
  date = {2018-04-17},
  eprint = {1804.06300},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1804.06300},
  urldate = {2019-05-08},
  abstract = {We present PredRNN++, an improved recurrent network for video predictive learning. In pursuit of a greater spatiotemporal modeling capability, our approach increases the transition depth between adjacent states by leveraging a novel recurrent unit, which is named Causal LSTM for re-organizing the spatial and temporal memories in a cascaded mechanism. However, there is still a dilemma in video predictive learning: increasingly deep-in-time models have been designed for capturing complex variations, while introducing more difficulties in the gradient back-propagation. To alleviate this undesirable effect, we propose a Gradient Highway architecture, which provides alternative shorter routes for gradient flows from outputs back to long-range inputs. This architecture works seamlessly with causal LSTMs, enabling PredRNN++ to capture short-term and long-term dependencies adaptively. We assess our model on both synthetic and real video datasets, showing its ability to ease the vanishing gradient problem and yield state-of-the-art prediction results even in a difficult objects occlusion scenario.},
  keywords = {#nosource,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,path planning,robotic grasping,Statistics - Machine Learning,tno internship},
  annotation = {00007}
}

@article{wangRobotGraspDetection2016,
  title = {Robot Grasp Detection Using Multimodal Deep Convolutional Neural Networks},
  author = {Wang, Zhichao and Li, Zhiqi and Wang, Bin and Liu, Hong},
  date = {2016},
  journaltitle = {Advances in Mechanical Engineering},
  volume = {8},
  number = {9},
  pages = {1687814016668077},
  keywords = {#nosource,grasp pose detection,neural network,object grasping,robotic grasping,tno internship},
  annotation = {00024}
}

@article{wangRobotLearningFramework2020,
  title = {A Robot Learning Framework Based on Adaptive Admittance Control and Generalizable Motion Modeling with Neural Network Controller},
  author = {Wang, Ning and Chen, Chuize and Yang, Chenguang},
  date = {2020-05-21},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {390},
  pages = {260--267},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2019.04.100},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231219314432},
  urldate = {2022-11-03},
  abstract = {Robot learning from demonstration (LfD) enables robots to be fast programmed. This paper presents a novel LfD framework involving a teaching phase, a learning phase and a reproduction phase, and proposes methods in each of these phases to guarantee the overall system performance. An adaptive admittance controller is developed to take into account the unknown human dynamics so that the human tutor can smoothly move the robot around in the teaching phase. The task model in this controller is formulated by the Gaussian mixture regression to extract the human-related motion characteristics. In the learning and reproduction phases, the dynamic movement primitive is employed to model a robotic motion that is generalizable. A neural network-based controller is designed for the robot to track the trajectories generated from the motion model, and a radial basis function neural network is used to compensate for the effect caused by the dynamic environments. Experiments have been performed using a Baxter robot and the results have confirmed the validity of the proposed robot learning framework.},
  langid = {english},
  keywords = {Adaptive admittance control,Motion generalization,Neural network,READ,Robot learning},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/wang_et_al_2020_a_robot_learning_framework_based_on_adaptive_admittance_control_and.pdf}
}

@online{wangSafeOnlineGain2021,
  title = {Safe {{Online Gain Optimization}} for {{Variable Impedance Control}}},
  author = {Wang, Changhao and Kuang, Zhian and Zhang, Xiang and Tomizuka, Masayoshi},
  date = {2021-11-01},
  eprint = {2111.01258},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2111.01258},
  urldate = {2022-08-15},
  abstract = {Smooth behaviors are preferable for many contact-rich manipulation tasks. Impedance control arises as an effective way to regulate robot movements by mimicking a mass-spring-damping system. Consequently, the robot behavior can be determined by the impedance gains. However, tuning the impedance gains for different tasks is tricky, especially for unstructured environments. Moreover, online adapting the optimal gains to meet the time-varying performance index is even more challenging. In this paper, we present Safe Online Gain Optimization for Variable Impedance Control (Safe OnGO-VIC). By reformulating the dynamics of impedance control as a control-affine system, in which the impedance gains are the inputs, we provide a novel perspective to understand variable impedance control. Additionally, we innovatively formulate an optimization problem with online collected force information to obtain the optimal impedance gains in real-time. Safety constraints are also embedded in the proposed framework to avoid unwanted collisions. We experimentally validated the proposed algorithm on three manipulation tasks. Comparison results with a constant gain baseline and an adaptive control method prove that the proposed algorithm is effective and generalizable to different scenarios.},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control,READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/wang_et_al_2021_safe_online_gain_optimization_for_variable_impedance_control.pdf}
}

@unpublished{wangSampleEfficientActorCritic2017,
  title = {Sample {{Efficient Actor-Critic}} with {{Experience Replay}}},
  author = {Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and family=Freitas, given=Nando, prefix=de, useprefix=true},
  date = {2017-07-10},
  eprint = {1611.01224},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1611.01224},
  urldate = {2020-07-02},
  abstract = {This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.},
  keywords = {#nosource,Computer Science - Machine Learning,model-free,policy gradients,reinforcement learning},
  annotation = {00000}
}

@article{wangStabilityLearningHuman2019,
  title = {On Stability for Learning Human Control Strategy by Demonstrations Using {{SVM}}},
  author = {Wang, Zhiyang and Ou, Yongsheng},
  date = {2019-01-01},
  journaltitle = {Assembly Automation},
  volume = {40},
  number = {1},
  pages = {118--131},
  publisher = {{Emerald Publishing Limited}},
  issn = {0144-5154},
  doi = {10.1108/AA-11-2018-0236},
  url = {https://doi.org/10.1108/AA-11-2018-0236},
  urldate = {2022-09-11},
  abstract = {Purpose This paper aims to deal with the trade-off of the stability and the accuracy in learning human control strategy from demonstrations. With the stability conditions and the estimated stability region, this paper aims to conveniently get rid of the unstable controller or controller with relatively small stability region. With this evaluation, the learning human strategy controller becomes much more robust to perturbations. Design/methodology/approach In this paper, the criterion to verify the stability and a method to estimate the domain of attraction are provided for the learning controllers trained with support vector machines (SVMs). Conditions are formulated based on the discrete-time system Lyapunov theory to ensure that a closed-form of the learning control system is strongly stable under perturbations (SSUP). Then a Chebychev point based approach is proposed to estimate its domain of attraction. Findings Some of such learning controllers have been implemented in the vertical balance control of a dynamically stable, statically unstable wheel mobile robot.},
  keywords = {Domain of attraction,Learning human control strategy by demonstrations,Manufacturing and robotics,READ,Stability analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/wang_ou_2019_on_stability_for_learning_human_control_strategy_by_demonstrations_using_svm.pdf}
}

@inproceedings{watsonRealWorldRealTimeRobotic2017,
  title = {Real-{{World}}, {{Real-Time Robotic Grasping}} with {{Convolutional Neural Networks}}},
  booktitle = {Towards {{Autonomous Robotic Systems}}},
  author = {Watson, Joe and Hughes, Josie and Iida, Fumiya},
  editor = {Gao, Yang and Fallah, Saber and Jin, Yaochu and Lekakou, Constantina},
  date = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {617--626},
  publisher = {{Springer International Publishing}},
  abstract = {Adapting to uncertain environments is a key obstacle in the development of robust robotic object manipulation systems, as there is a trade-off between the computationally expensive methods of handling the surrounding complexity, and the real-time requirement for practical operation. We investigate the use of Deep Learning to develop a real-time scheme on a physical robot. Using a Baxter Research Robot and Kinect sensor, a convolutional neural network (CNN) was trained in a supervised manner to regress grasping coordinates from RGB-D data. Compared to existing methods, regression via deep learning offered an efficient process that learnt generalised grasping features and processed the scene in real-time. The system achieved a successful grasp rate of 62\% and a successful detection rate of 78\% on a diverse set of physical objects across varying position and orientation, executing grasp detection in 1.8 s on a CPU machine and a complete physical grasp and move in 60 s on the robot.},
  isbn = {978-3-319-64107-2},
  langid = {english},
  keywords = {#nosource,Convolution Neural Networks,Deep learning,Grasping,Manipulation,robotic grasping,tno internship},
  annotation = {00007}
}

@unpublished{wayneUnsupervisedPredictiveMemory2018,
  title = {Unsupervised {{Predictive Memory}} in a {{Goal-Directed Agent}}},
  author = {Wayne, Greg and Hung, Chia-Chun and Amos, David and Mirza, Mehdi and Ahuja, Arun and Grabska-Barwinska, Agnieszka and Rae, Jack and Mirowski, Piotr and Leibo, Joel Z. and Santoro, Adam and Gemici, Mevlana and Reynolds, Malcolm and Harley, Tim and Abramson, Josh and Mohamed, Shakir and Rezende, Danilo and Saxton, David and Cain, Adam and Hillier, Chloe and Silver, David and Kavukcuoglu, Koray and Botvinick, Matt and Hassabis, Demis and Lillicrap, Timothy},
  date = {2018-03-28},
  eprint = {1803.10760},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1803.10760},
  urldate = {2020-07-02},
  abstract = {Animals execute goal-directed behaviours despite the limited range and scope of their sensors. To cope, they explore environments and store memories maintaining estimates of important information that is not presently available. Recently, progress has been made with artificial intelligence (AI) agents that learn to perform tasks from sensory input, even at a human level, by merging reinforcement learning (RL) algorithms with deep neural networks, and the excitement surrounding these results has led to the pursuit of related ideas as explanations of non-human animal learning. However, we demonstrate that contemporary RL algorithms struggle to solve simple tasks when enough information is concealed from the sensors of the agent, a property called "partial observability". An obvious requirement for handling partially observed tasks is access to extensive memory, but we show memory is not enough; it is critical that the right information be stored in the right format. We develop a model, the Memory, RL, and Inference Network (MERLIN), in which memory formation is guided by a process of predictive modeling. MERLIN facilitates the solution of tasks in 3D virtual reality environments for which partial observability is severe and memories must be maintained over long durations. Our model demonstrates a single learning agent architecture that can solve canonical behavioural tasks in psychology and neurobiology without strong simplifying assumptions about the dimensionality of sensory input or the duration of experiences.},
  keywords = {#nosource,Computer Science - Machine Learning,memory,reinforcement learning,Statistics - Machine Learning}
}

@unpublished{weberImaginationAugmentedAgentsDeep2018,
  title = {Imagination-{{Augmented Agents}} for {{Deep Reinforcement Learning}}},
  author = {Weber, Théophane and Racanière, Sébastien and Reichert, David P. and Buesing, Lars and Guez, Arthur and Rezende, Danilo Jimenez and Badia, Adria Puigdomènech and Vinyals, Oriol and Heess, Nicolas and Li, Yujia and Pascanu, Razvan and Battaglia, Peter and Hassabis, Demis and Silver, David and Wierstra, Daan},
  date = {2018-02-14},
  eprint = {1707.06203},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1707.06203},
  urldate = {2020-07-02},
  abstract = {We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,learned model,learned-model,model-based,reinforcement learning,Statistics - Machine Learning}
}

@article{weeHowWriteLiterature2016,
  title = {How to {{Write}} a {{Literature Review Paper}}?},
  author = {Wee, Bert Van and Banister, David},
  date = {2016-03-03},
  journaltitle = {Transport Reviews},
  volume = {36},
  number = {2},
  pages = {278--288},
  publisher = {{Routledge}},
  issn = {0144-1647},
  doi = {10.1080/01441647.2015.1065456},
  url = {https://doi.org/10.1080/01441647.2015.1065456},
  urldate = {2022-03-08},
  abstract = {This paper discusses the question about how to write a literature review paper (LRP). It stresses the primary importance of adding value, rather than only providing an overview, and it then discusses some of the reasons for (or not) actually writing an LRP, including issues relating to the nature and scope of the paper. It also presents different types of LRPs, advises on reporting the methodology used for the selection of papers for review, and the structure of an LRP. An important conclusion is that the heterogeneity in LRPs is very large. This paper also presents some of the aspects that the authors feel are important structural and contextual considerations that help produce high-quality review papers.},
  keywords = {added value,guidelines,method,paper structure},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/wee_banister_2016_how_to_write_a_literature_review_paper.pdf}
}

@inproceedings{weiImpedanceControlUncertain2019,
  title = {Impedance {{Control}} in {{Uncertain Environment}} Using {{Reinforcement Learning}}},
  booktitle = {2019 {{IEEE}} 9th {{Annual International Conference}} on {{CYBER Technology}} in {{Automation}}, {{Control}}, and {{Intelligent Systems}} ({{CYBER}})},
  author = {Wei, Wu and Peng, Hang and Pang, Jiankun and Yu, Qiuda},
  date = {2019-07},
  pages = {1203--1208},
  issn = {2379-7711},
  doi = {10.1109/CYBER46603.2019.9066697},
  abstract = {Impedance control is a very classic control method in robot and human interaction. Humans use their ability to adaptively adjust arm impedance parameters to perform well in a variety of tasks. This ability allows us to successfully complete interactive tasks even in uncertain disturbance environments. By analyzing the results of many force field experiments, We got a conclusion that humans usually use two methods to change their impedance in external perturbations: 1) In the situation of unpredictable perturbations, humans adapt their impedance through muscle contraction; 2) In the situation of predictable perturbations, humans adds a constant term to offset the known perturbations. In this paper, We show how 3-DOFs simulated robot use the reinforcement learning algorithm to perform similar behavior in both situations. We apply our model-free reinforcement learning algorithm PI2 (policy improvement with path integrals) to the robot to learn the end-effector variable impedance schedules and trajectories. Our findings provide a approach to automatically learn the impedance empirically without requiring to build a physical or environmental dynamics model.},
  eventtitle = {2019 {{IEEE}} 9th {{Annual International Conference}} on {{CYBER Technology}} in {{Automation}}, {{Control}}, and {{Intelligent Systems}} ({{CYBER}})},
  keywords = {Adaptation models,Force,Impedance,Perturbation methods,READ,Robots,Task analysis,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/wei_et_al_2019_impedance_control_in_uncertain_environment_using_reinforcement_learning.pdf}
}

@article{westerveltHybridZeroDynamics2003,
  title = {Hybrid Zero Dynamics of Planar Biped Walkers},
  author = {Westervelt, E. R. and Grizzle, J. W. and Koditschek, D. E.},
  date = {2003-01},
  journaltitle = {IEEE Transactions on Automatic Control},
  volume = {48},
  number = {1},
  pages = {42--56},
  issn = {1558-2523},
  doi = {10.1109/TAC.2002.806653},
  abstract = {Planar, underactuated, biped walkers form an important domain of applications for hybrid dynamical systems. This paper presents the design of exponentially stable walking controllers for general planar bipedal systems that have one degree-of-freedom greater than the number of available actuators. The within-step control action creates an attracting invariant set - a two-dimensional zero dynamics submanifold of the full hybrid model \$whose restriction dynamics admits a scalar linear time-invariant return map. Exponentially stable periodic orbits of the zero dynamics correspond to exponentially stabilizable orbits of the full model. A convenient parameterization of the hybrid zero dynamics is imposed through the choice of a class of output functions. Parameter optimization is used to tune the hybrid zero dynamics in order to achieve closed-loop, exponentially stable walking with low energy consumption, while meeting natural kinematic and dynamic constraints. The general theory developed in the paper is illustrated on a five link walker, consisting of a torso and two legs with knees.},
  eventtitle = {{{IEEE Transactions}} on {{Automatic Control}}},
  keywords = {#nosource,Actuators,Constraint optimization,Control systems,Energy consumption,Kinematics,Knee,Leg,Legged locomotion,Orbits,Torso}
}

@inproceedings{wieberStabilityWalkingSystems2002,
  title = {On the Stability of Walking Systems},
  author = {Wieber, Pierre-Brice},
  date = {2002},
  url = {https://hal.inria.fr/inria-00390866},
  urldate = {2021-04-18},
  abstract = {We reconsider here the stability criteria usually proposed for the analysis of walking systems, exhibiting their limits and their ambiguity. We propose then some new criteria based on a thorough analysis of the dynamics of walking systems and precise definitions concerning their stability. Numerical methods are presented then to deal with these new criteria.},
  eventtitle = {Proceedings of the {{International Workshop}} on {{Humanoid}} and {{Human Friendly Robotics}}},
  langid = {english},
  keywords = {#nosource}
}

@article{willemsDissipativeDynamicalSystems1972,
  title = {Dissipative Dynamical Systems Part {{II}}: {{Linear}} Systems with Quadratic Supply Rates},
  shorttitle = {Dissipative Dynamical Systems Part {{II}}},
  author = {Willems, Jan C.},
  date = {1972},
  journaltitle = {Archive for rational mechanics and analysis},
  volume = {45},
  number = {5},
  pages = {352--393},
  publisher = {{Springer}},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/willems_1972_dissipative_dynamical_systems_part_ii.pdf}
}

@article{willemsDissipativeDynamicalSystems1972a,
  title = {Dissipative Dynamical Systems Part {{I}}: {{General}} Theory},
  shorttitle = {Dissipative Dynamical Systems Part {{I}}},
  author = {Willems, Jan C.},
  date = {1972},
  journaltitle = {Archive for rational mechanics and analysis},
  volume = {45},
  number = {5},
  pages = {321--351},
  publisher = {{Springer}},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/willems_1972_dissipative_dynamical_systems_part_i.pdf}
}

@article{williamsDataDrivenApproximation2015,
  title = {A Data–Driven Approximation of the Koopman Operator: {{Extending}} Dynamic Mode Decomposition},
  shorttitle = {A Data–Driven Approximation of the Koopman Operator},
  author = {Williams, Matthew O. and Kevrekidis, Ioannis G. and Rowley, Clarence W.},
  date = {2015},
  journaltitle = {Journal of Nonlinear Science},
  volume = {25},
  pages = {1307--1346},
  publisher = {{Springer}},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/williams_et_al_2015_a_data–driven_approximation_of_the_koopman_operator.pdf}
}

@article{wilzConstrainedHapticguidedShared2021,
  title = {Constrained Haptic-Guided Shared Control for Collaborative Human–Robot Percutaneous Nephrolithotomy Training},
  author = {Wilz, Olivia and Sainsbury, Ben and Rossa, Carlos},
  date = {2021-05-01},
  journaltitle = {Mechatronics},
  shortjournal = {Mechatronics},
  volume = {75},
  pages = {102528},
  issn = {0957-4158},
  doi = {10.1016/j.mechatronics.2021.102528},
  url = {https://www.sciencedirect.com/science/article/pii/S0957415821000301},
  urldate = {2023-04-29},
  abstract = {Percutaneous nephrolithotomy is a procedure used to treat patients with large or irregularly shaped kidney stones. Surgical instruments are inserted through a small incision to access the kidney and remove the calculi. Surgeons who have less experience with the procedure manifest significantly higher rates of complications due to the procedure’s steep learning curve. This issue is further exacerbated by a lack of training opportunities in clinical settings. This paper introduces a teleoperative framework that can provide training to surgeons as well as assistance during procedures, based on two main components. Firstly, a type of constrained inverse kinematics that decouples the tooltip position from its orientation using a remote centre of motion, and incorporates the joint limits analytically. This reduces the workload of the procedure by having the surgeon control only the tooltip position rather than the position and the orientation while preventing the inverse kinematics from returning joint angles outside of the robot’s abilities. This kinematic framework also allows a three-degrees-of-freedom haptic device to control a six-degrees-of-freedom manipulator. Secondly, haptic feedback is provided to help guide and teach the surgeon during the procedure. Haptic feedback allows the surgeon to remain in full control during the procedure while still receiving haptic cues and assistance. Experimental results indicate that the haptic cues improved user’s accuracy, and they had shorter and smoother paths. This leads to a shorter procedure time overall. The results also indicate that the haptic assistance helped teach users the ideal trajectory of the procedure and that users who were taught with haptic feedback performed better than those who never experienced any haptic feedback.},
  langid = {english},
  keywords = {Constrained inverse kinematics,Haptic assistance,Human–robot interface,Medical robotics,Percutaneous nephrolithotomy,READ,Remote centre of motion,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/wilz_et_al_2021_constrained_haptic-guided_shared_control_for_collaborative_human–robot.pdf}
}

@inproceedings{winterRoleCouplingTerms2016,
  title = {The Role of Coupling Terms in Variable Impedance Policies Learning},
  booktitle = {International {{Workshop}} on {{Human-Friendly Robotics}}},
  author = {Winter, Florian and Saveriano, Matteo and Lee, Dongheui},
  date = {2016},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/winter_et_al_2016_the_role_of_coupling_terms_in_variable_impedance_policies_learning.pdf}
}

@book{wuAdaptiveImpedanceControl2020,
  title = {Adaptive {{Impedance Control Based}} on {{Reinforcement Learning}} in a {{Human-Robot Collaboration Task}} with {{Human Reference Estimation}}},
  author = {Wu, Min and He, Yanhao and Liu, Steven},
  date = {2020-02-13},
  abstract = {In this work an adaptive impedance control scheme in a human-root collaboration task is designed. Both motion reference and impedance parameters of the robot control are adapted in real-time so that no priori task information is required. Reinforcement learning is used to find an optimal impedance parameter set to minimize a task-orient cost function, without fully knowledge of the system dynamic. The learned parameters are further adjusted by taking human's disagreement into consideration. The human motion reference is estimated from the lineralized contact dynamic model using system identification technology. The proposed method enables robot to generate active contribution in the collaboration and be flexible to variation of the task or enviroment. Experimental results are presented to illustrate the performance.},
  keywords = {Human robot interaction,Impedance control,Instability,pHRI,Q-learning,Reinforcement learning,Shared impedance control,STABILITY,Tracking},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/wu_et_al_2020_adaptive_impedance_control_based_on_reinforcement_learning_in_a_human-robot.pdf}
}

@article{wuDeepReinforcementLearning2023,
  title = {Deep Reinforcement Learning Control Approach to Mitigating Actuator Attacks},
  author = {Wu, Chengwei and Pan, Wei and Staa, Rick and Liu, Jianxing and Sun, Guanghui and Wu, Ligang},
  date = {2023-06-01},
  journaltitle = {Automatica},
  shortjournal = {Automatica},
  volume = {152},
  pages = {110999},
  issn = {0005-1098},
  doi = {10.1016/j.automatica.2023.110999},
  url = {https://www.sciencedirect.com/science/article/pii/S0005109823001528},
  urldate = {2023-05-05},
  abstract = {This paper investigates the deep reinforcement learning based secure control problem for cyber–physical systems (CPS) under false data injection attacks. We describe the CPS under attacks as a Markov decision process (MDP), based on which the secure controller design for CPS under attacks is formulated as an action policy learning using data. Rendering the soft actor–critic learning algorithm, a Lyapunov-based soft actor–critic learning algorithm is proposed to offline train a secure policy for CPS under attacks. Different from the existing results, not only the convergence of the learning algorithm but the stability of the system using the learned policy is proved, which is quite important for security and stability-critical applications. Finally, both a satellite attitude control system and a robot arm system are used to show the effectiveness of the proposed scheme, and comparisons between the proposed learning algorithm and the classical PD controller are also provided to demonstrate the advantages of the control algorithm designed in this paper.},
  langid = {english},
  keywords = {Cyber–physical systems,Deep reinforcement learning,False data injection attacks,Lyapunov stability},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/wu_et_al_2023_deep_reinforcement_learning_control_approach_to_mitigating_actuator_attacks.pdf;/home/ricks/Zotero/storage/LT9QLXP3/S0005109823001528.html}
}

@article{wuFrameworkAutonomousImpedance2021,
  title = {A {{Framework}} for {{Autonomous Impedance Regulation}} of {{Robots Based}} on {{Imitation Learning}} and {{Optimal Control}}},
  author = {Wu, Yuqiang and Zhao, Fei and Tao, Tao and Ajoudani, Arash},
  date = {2021-01},
  journaltitle = {Ieee Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {6},
  number = {1},
  pages = {127--134},
  publisher = {{Ieee-Inst Electrical Electronics Engineers Inc}},
  location = {{Piscataway}},
  issn = {2377-3766},
  doi = {10.1109/LRA.2020.3033260},
  url = {https://ieeexplore.ieee.org/document/9237100},
  urldate = {2022-05-02},
  abstract = {In this work, we propose a framework to address the autonomous impedance regulation problem of robots in a class of constrained manipulation tasks. In this framework, a human arm endpoint stiffness model is used to extract the task stiffness geometry along the constrained trajectory, which is then encoded offline and reproduced online by a Gaussian Mixture Model (GMM) and the Gaussian Mixture Regression (GMR), respectively. Furthermore, the full Cartesian impedance of the robot is formulated through an optimal control problem, i.e., the Linear-Quadratic Regulator (LQR), in which the task stiffness geometry (extracted from human demonstrations) is considered as the time-varying weighting matrix Q. The optimal impedance is eventually realised by the robot through a task geometry consistent Cartesian impedance controller. A tank-based passivity observer is implemented to give evidence on the stability of the system during online impedance variations. To evaluate the performance of the framework, a comparative experiment with three different impedance settings (i.e., the proposed framework, the framework without LQR and the framework without GMM/GMR) for Franka Emika Panda to perform a door opening task was conducted. The results reveal that our framework outperforms the other two, in terms of tracking error and the interaction forces.},
  langid = {english},
  keywords = {adaptation,Ellipsoids,energy tanks,force,Gaussian mixture model,gaussian mixture regression,Imitation learning,Impedance,manipulation,Optimal control,optimization and optimal control,physical human-robot interaction,READ,Robots,STABILITY,Task analysis,teleoperation,Trajectory},
  annotation = {WOS:000589691100001},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/wu_et_al_2021_a_framework_for_autonomous_impedance_regulation_of_robots_based_on_imitation.pdf}
}

@article{wuLearningDemonstrationInteractive2022,
  title = {Learning {{From Demonstration}} and {{Interactive Control}} of {{Variable-Impedance}} to {{Cut Soft Tissues}}},
  author = {Wu, Rui and Billard, Aude},
  date = {2022},
  journaltitle = {IEEE/ASME Transactions on Mechatronics},
  pages = {1--12},
  issn = {1941-014X},
  doi = {10.1109/TMECH.2021.3123356},
  abstract = {In this article, we propose an approach to extract variable-impedance during cutting tasks from human demonstrations, so as to ease soft-tissue cutting by robots. We model the dynamic adjustment of the human arm during interactions with the tissue and transfer these adaptive capabilities to the robot, by learning both the motion and change of impedance. To improve performance during task execution, our variable-impedance skill-transfer framework combines the learned model with an interactive-operation and feedback controller. To offer the flexibility of modifying the trajectory at run time, we use a control law based on dynamical systems. We couple this control law with virtual dynamics that describes the cutting dynamics. This ensures that the robot can control the interaction force and can plan the trajectory. The approach is validated in a real robot cutting-experiment of cutting different hardness tissues. Results show that our learning-based feedback controller, with the assistance from interactive-operation models, can effectively improve the task's success rate, cutting length, cutting depth, and other indicators, as compared to using a fixed and variable-impedance gain controller.},
  eventtitle = {{{IEEE}}/{{ASME Transactions}} on {{Mechatronics}}},
  keywords = {Adaptation models,Dynamical systems (DS),Force,Impedance,learning from demonstration (LfD),Manipulator dynamics,READ,Robots,soft tissues,Task analysis,Trajectory,variable-impedance skill transfer},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/wu_billard_2022_learning_from_demonstration_and_interactive_control_of_variable-impedance_to.pdf}
}

@unpublished{wulfmeierMutualAlignmentTransfer2017,
  title = {Mutual {{Alignment Transfer Learning}}},
  author = {Wulfmeier, Markus and Posner, Ingmar and Abbeel, Pieter},
  date = {2017-09-26},
  eprint = {1707.07907},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1707.07907},
  urldate = {2020-07-02},
  abstract = {Training robots for operation in the real world is a complex, time consuming and potentially expensive task. Despite significant success of reinforcement learning in games and simulations, research in real robot applications has not been able to match similar progress. While sample complexity can be reduced by training policies in simulation, such policies can perform sub-optimally on the real platform given imperfect calibration of model dynamics. We present an approach -- supplemental to fine tuning on the real robot -- to further benefit from parallel access to a simulator during training and reduce sample requirements on the real robot. The developed approach harnesses auxiliary rewards to guide the exploration for the real world agent based on the proficiency of the agent in simulation and vice versa. In this context, we demonstrate empirically that the reciprocal alignment for both agents provides further benefit as the agent in simulation can adjust to optimize its behaviour for states commonly visited by the real-world agent.},
  keywords = {#nosource,Computer Science - Artificial Intelligence,multitask RL,reinforcement learning,transfer learning},
  annotation = {00028}
}

@article{wuMultiObjectGraspingDetection2019,
  title = {Multi-{{Object Grasping Detection With Hierarchical Feature Fusion}}},
  author = {Wu, Guangbin and Chen, Weishan and Cheng, Hui and Zuo, Wangmeng and Zhang, David and You, Jane},
  date = {2019},
  journaltitle = {IEEE Access},
  volume = {7},
  pages = {43884--43894},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2908281},
  url = {https://ieeexplore.ieee.org/document/8678647/},
  urldate = {2019-05-10},
  abstract = {Grasping in cluttered and tight scenes is a necessary skill for intelligent robotics to achieve more general application. Such universal robotics can use their perception abilities to visually identify grasps from a stack of objects. However, most existing grasping detection methods based on deep learning just focus on estimating grasping pose with single-layer features. In this paper, we present a novel grasp detection algorithm termed as multi-object grasping detection network, which can utilize hierarchical features to learn object detector and grasping pose estimator simultaneously. The network is mainly composed of two branches: 1) Object detection branch which is based on the single shot multibox detection approach to discriminate object categories and locate object positions by bounding boxes; 2) Grasping pose estimation branch where hierarchical features are fused together to predict grasping position and orientation. To improve grasping detection performance, attention mechanism is employed in hierarchical feature fusion. For evaluating the proposed model, we build a multi-object grasping dataset where every image contains numerous different graspable objects. The extensive experiments demonstrate that the multi-object grasping detection method achieves the state-of-the-art performance on both object detection and grasping pose estimation.},
  langid = {english},
  keywords = {#nosource,grasp pose detection,robotic grasping,tno internship},
  annotation = {00000}
}

@article{wuReducingEstimationBias2020,
  title = {Reducing {{Estimation Bias}} via {{Triplet-Average Deep Deterministic Policy Gradient}}},
  author = {Wu, Dongming and Dong, Xingping and Shen, Jianbing and Hoi, Steven C. H.},
  date = {2020-11},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {31},
  number = {11},
  pages = {4933--4945},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2019.2959129},
  url = {https://ieeexplore.ieee.org/abstract/document/8957368},
  urldate = {2024-03-01},
  abstract = {The overestimation caused by function approximation is a well-known property in Q-learning algorithms, especially in single-critic models, which leads to poor performance in practical tasks. However, the opposite property, underestimation, which often occurs in Q-learning methods with double critics, has been largely left untouched. In this article, we investigate the underestimation phenomenon in the recent twin delay deep deterministic actor-critic algorithm and theoretically demonstrate its existence. We also observe that this underestimation bias does indeed hurt performance in various experiments. Considering the opposite properties of single-critic and double-critic methods, we propose a novel triplet-average deep deterministic policy gradient algorithm that takes the weighted action value of three target critics to reduce the estimation bias. Given the connection between estimation bias and approximation error, we suggest averaging previous target values to reduce per-update error and further improve performance. Extensive empirical results over various continuous control tasks in OpenAI gym show that our approach outperforms the state-of-the-art methods.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}} and {{Learning Systems}}},
  keywords = {Approximation algorithms,Averaging technology,deep reinforcement learning (DRL),Estimation,estimation bias,Function approximation,Games,Minimization,Neural networks,Task analysis,triplet networks},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/Wu et al_2020_Reducing Estimation Bias via Triplet-Average Deep Deterministic Policy Gradient.pdf}
}

@article{wuSafeLearningBasedFeedback2022,
  title = {Safe {{Learning-Based Feedback Linearization Tracking Control}} for {{Nonlinear System With Event-Triggered Model Update}}},
  author = {Wu, Zhixuan and Yang, Rui and Zheng, Lei and Cheng, Hui},
  date = {2022-04},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {2},
  pages = {3286--3293},
  issn = {2377-3766},
  doi = {10.1109/LRA.2022.3146930},
  abstract = {Learning-based methods are powerful in handling complex scenarios. However, it is still challenging to use learning-based methods under uncertain environments while stability, safety, and real-time performance of the system are desired to guarantee. In this letter, we propose a learning-based tracking control scheme based on a feedback linearization controller in which uncertain disturbances are approximated online using Gaussian Processes (GPs). Using the predicted distribution of disturbances given by GPs, a Control Lyapunov Function (CLF) and Control Barrier Function (CBF) based Quadratic Program is applied, with which probabilistic stability and safety are guaranteed. In addition, the trajectory is optimized first by Model Predictive Control (MPC) based on the linearized dynamics systems to further reduce the tracking error. We also design an event trigger for GPs updates to improve efficiency while stability and safety of the system are still guaranteed. The effectiveness of the proposed tracking control strategy is illustrated in numerical simulations.},
  eventtitle = {{{IEEE Robotics}} and {{Automation Letters}}},
  keywords = {Adaptive control,Computational modeling,control architectures and programming,Feedback linearization,machine learning for robot control,Numerical stability,READ,Robots,Safety,Trajectory,Uncertainty},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/wu_et_al_2022_safe_learning-based_feedback_linearization_tracking_control_for_nonlinear.pdf}
}

@unpublished{wuScalableTrustregionMethod2017,
  title = {Scalable Trust-Region Method for Deep Reinforcement Learning Using {{Kronecker-factored}} Approximation},
  author = {Wu, Yuhuai and Mansimov, Elman and Liao, Shun and Grosse, Roger and Ba, Jimmy},
  date = {2017-08-18},
  eprint = {1708.05144},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1708.05144},
  urldate = {2020-07-02},
  abstract = {In this work, we propose to apply trust region optimization to deep reinforcement learning using a recently proposed Kronecker-factored approximation to the curvature. We extend the framework of natural policy gradient and propose to optimize both the actor and the critic using Kronecker-factored approximate curvature (K-FAC) with trust region; hence we call our method Actor Critic using Kronecker-Factored Trust Region (ACKTR). To the best of our knowledge, this is the first scalable trust region natural gradient method for actor-critic methods. It is also a method that learns non-trivial tasks in continuous control as well as discrete control policies directly from raw pixel inputs. We tested our approach across discrete domains in Atari games as well as continuous domains in the MuJoCo environment. With the proposed methods, we are able to achieve higher rewards and a 2- to 3-fold improvement in sample efficiency on average, compared to previous state-of-the-art on-policy actor-critic methods. Code is available at https://github.com/openai/baselines},
  keywords = {#nosource,Computer Science - Machine Learning,model-free,policy gradients,reinforcement learning},
  annotation = {00000}
}

@article{xiaoLearningStableNonparametric2020,
  title = {Learning Stable Nonparametric Dynamical Systems with {{Gaussian}} Process Regression},
  author = {Xiao, Wenxin and Lederer, Armin and Hirche, Sandra},
  date = {2020},
  journaltitle = {IFAC-PapersOnLine},
  volume = {53},
  number = {2},
  pages = {1194--1199},
  publisher = {{Elsevier}},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/xiao_et_al_2020_learning_stable_nonparametric_dynamical_systems_with_gaussian_process_regression.pdf}
}

@article{xieRobotLearningDemonstration2020,
  title = {Robot Learning from Demonstration for Path Planning: {{A}} Review},
  shorttitle = {Robot Learning from Demonstration for Path Planning},
  author = {Xie, ZongWu and Zhang, Qi and Jiang, ZaiNan and Liu, Hong},
  date = {2020-08},
  journaltitle = {Science China Technological Sciences},
  shortjournal = {Sci. China Technol. Sci.},
  volume = {63},
  number = {8},
  pages = {1325--1334},
  issn = {1674-7321, 1869-1900},
  doi = {10.1007/s11431-020-1648-4},
  url = {https://link.springer.com/10.1007/s11431-020-1648-4},
  urldate = {2022-10-24},
  langid = {english},
  keywords = {imitation learning,inverse reinforcement learning,learning from demonstration,obstacle avoidance,path planning,READ,REVIEW},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/xie_et_al_2020_robot_learning_from_demonstration_for_path_planning.pdf;/home/ricks/Zotero/storage/8NHCUFEQ/s11431-020-1648-4.pdf}
}

@inproceedings{xiongAdaptiveMotorControl2018,
  title = {Adaptive {{Motor Control}} for {{Human-like Spatial-temporal Adaptation}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Biomimetics}} ({{ROBIO}})},
  author = {Xiong, Xiaofeng and Manoonpong, Poramate},
  date = {2018-12},
  pages = {2107--2112},
  doi = {10.1109/ROBIO.2018.8665222},
  abstract = {Human arms can produce stable and variable compliant joint motions to achieve tasks in various spatial tasks and temporal scales. To emulate such motions we propose an adaptive motor controller (AMC) allowing for spatial-temporal adaptation of human-like motor control. The AMC is a biomimetic controller consisting of online force and impedance (i.e., stiffness and damping) adaptation to different tasks and unknown arm dynamics. As a result, the AMC can produce more accurate and stable human-like reaching and tracking behaviors, compared to conventional controllers. Moreover, the reproduced spatial-temporal adaptation is comparable to that found in the experiments of human motor control. The proposed AMC may pave a novel and simple way forward to understanding and solving inverse dynamics and variable impedance control in robotics and biomechanics.},
  eventtitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Biomimetics}} ({{ROBIO}})},
  keywords = {Damping,Force,Impedance,Motor drives,READ,Robots,Task analysis,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/xiong_manoonpong_2018_adaptive_motor_control_for_human-like_spatial-temporal_adaptation2.pdf}
}

@article{xiongOnlineSensorimotorLearning2021,
  title = {Online Sensorimotor Learning and Adaptation for Inverse Dynamics Control},
  author = {Xiong, Xiaofeng and Manoonpong, Poramate},
  date = {2021-11-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {143},
  pages = {525--536},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2021.06.029},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608021002616},
  urldate = {2022-05-02},
  abstract = {We propose a micro-data ({$<$} 10 trials) sensorimotor learning and adaptation (SEED) model for human-like arm inverse dynamics control. The SEED model consists of a feedforward Gaussian motor primitive (GATE) neural network and an adaptive feedback impedance (AIM) mechanism. Sensorimotor weights over trials are learned in the GATE network, while the AIM mechanism is used to online tune impedance gains in a trial. The model was validated by periodic and non-periodic tracking tasks on a two-joint robot arm. As a result, the proposed model enables the arm to stably learn the tasks within 10 trials, compared to thousands of trials required by state-of-art deep learning. This model facilitates the exploration of unknown arm dynamics, in which the elbow joint requires much less active control compared to the shoulder. This control goes below 3\% of the overall effort. This finding complies with a proximal–distal control gradient in human arm control. Taken together, the proposed SEED model paves a way for implementing data-efficient sensorimotor learning and adaptation of human-like arm movement.},
  langid = {english},
  keywords = {Gaussian model,Neural network,READ,Robot control,STABILITY,Variable compliant control},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/xiong_manoonpong_2021_online_sensorimotor_learning_and_adaptation_for_inverse_dynamics_control.pdf}
}

@article{xueRobotHumanLikeLearning2022,
  title = {A {{Robot Human-Like Learning Framework Applied}} to {{Unknown Environment Interaction}}},
  author = {Xue, Xianfa and Zuo, Lei and Wang, Ning},
  date = {2022-03-01},
  journaltitle = {Complexity},
  volume = {2022},
  pages = {e5648826},
  publisher = {{Hindawi}},
  issn = {1076-2787},
  doi = {10.1155/2022/5648826},
  url = {https://www.hindawi.com/journals/complexity/2022/5648826/},
  urldate = {2022-10-30},
  abstract = {Learning from demonstration (LfD) is one of the promising approaches for fast robot programming. Most learning systems learn both movements and stiffness profiles from human demonstrations. However, they rarely consider the unknown environment interaction. In this paper, a robot human-like learning framework is proposed, where it can learn human skills through demonstration and complete the interaction task with an unknown environment. Firstly, the desired trajectory was generated by dynamic movement primitive (DMP) based on human demonstration. Then, an adaptive optimal admittance control scheme was employed to interact with environments with the reference adaptation method. Finally, the experimental study was conducted, and the effectiveness of the framework proposed in this paper was verified via a group of curved surface wiping experiments on a balloon with unknown model parameters.},
  langid = {english},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/xue_et_al_2022_a_robot_human-like_learning_framework_applied_to_unknown_environment_interaction.pdf}
}

@article{xuLearningBasedKinematicControl2022,
  title = {Learning-{{Based Kinematic Control Using Position}} and {{Velocity Errors}} for {{Robot Trajectory Tracking}}},
  author = {Xu, Sheng and Ou, Yongsheng and Wang, Zhiyang and Duan, Jianghua and Li, Hao},
  date = {2022-02},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  volume = {52},
  number = {2},
  pages = {1100--1110},
  issn = {2168-2232},
  doi = {10.1109/TSMC.2020.3013904},
  abstract = {In this article, we address the trajectory tracking problem using the learning from demonstration (LFD) method. By using the LFD method, the parameter adjusting problem in the tracking controller is avoided. Consequently, a strategy can be provided to users with limited parameter adjusting experience. The kinematic tracking problem is formulated as a second-order system and the objective is to simultaneously reduce the errors in position and velocity. The extreme learning machines (ELM) algorithm is applied in the controller design. The velocity and position are utilized as the inputs and the output is the robot corrected kinematic movement. The controller parameters are learned from the desired human or programming demonstrations taking into consideration the stability constraints. In this work, we analyze the system local and global asymptotic stability in detail. The effectiveness of the proposed strategy is demonstrated by simulation comparisons and a practical experiment using a KUKA robot manipulator.},
  eventtitle = {{{IEEE Transactions}} on {{Systems}}, {{Man}}, and {{Cybernetics}}: {{Systems}}},
  keywords = {Acceleration,Extreme learning machines (ELM),Heuristic algorithms,Kinematics,learning from demonstration (LFD),position and velocity errors,READ,robot trajectory tracking,Robots,stability analysis,Trajectory,Trajectory tracking},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/xu_et_al_2022_learning-based_kinematic_control_using_position_and_velocity_errors_for_robot.pdf}
}

@article{xuRobotTrajectoryTracking2019,
  title = {Robot Trajectory Tracking Control Using Learning from Demonstration Method},
  author = {Xu, Sheng and Ou, Yongsheng and Duan, Jianghua and Wu, Xinyu and Feng, Wei and Liu, Ming},
  date = {2019-04-21},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {338},
  pages = {249--261},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2019.01.052},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231219300785},
  urldate = {2022-09-11},
  abstract = {This paper addresses robot trajectory tracking problem by using the learning from demonstration (LFD) method. Firstly, the trajectory tracking problem is formulated and the related previous works are introduced. Secondly, a trajectory tracking control policy using a three-layer neural network method, i.e., extreme learning machines (ELM), is proposed to minimize the real-time position and velocity errors. In the proposed method, the control algorithms are learnt from demonstrations directly such that the parameter adjusting problem in the traditional model-based methods is avoided. Besides, the trained controller has generalization ability to unseen situations which can be used to track different desired trajectories without any extra re-training. Thirdly, the stability analysis of the proposed control algorithm is provided and the corresponding parameter constraints are derived. Finally, the effectiveness and the generalization ability of the proposed control algorithms are demonstrated and discussed with simulation and experimental examples.},
  langid = {english},
  keywords = {Extreme learning machines (ELM),Learning from demonstration (LFD),READ,Robot trajectory tracking,Stability analysis,State errors},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/xu_et_al_2019_robot_trajectory_tracking_control_using_learning_from_demonstration_method.pdf}
}

@article{yangBiologicallyInspiredMotion2019,
  title = {Biologically {{Inspired Motion Modeling}} and {{Neural Control}} for {{Robot Learning From Demonstrations}}},
  author = {Yang, Chenguang and Chen, Chuize and Wang, Ning and Ju, Zhaojie and Fu, Jian and Wang, Min},
  date = {2019-06},
  journaltitle = {IEEE Transactions on Cognitive and Developmental Systems},
  volume = {11},
  number = {2},
  pages = {281--291},
  issn = {2379-8939},
  doi = {10.1109/TCDS.2018.2866477},
  abstract = {In this paper, we propose a biologically inspired framework for robot learning based on demonstrations. The dynamic movement primitive (DMP), which is motivated by neurobiology and human behavior, is employed to model a robotic motion that is generalizable. However, the DMP method can only be used to handle a single demonstration. To enable the robot to learn from multiple demonstrations, the DMP is combined with the Gaussian mixture model (GMM) to integrate the features of multiple demonstrations, where the conventional GMM is further replaced by the fuzzy GMM (FGMM) to improve the fitting performance. Also, a novel regression algorithm for FGMM is derived to retrieve the nonlinear term of the DMP. Additionally, a neural network-based controller is developed for the robot to track the generated motions. In this network, the cerebellar model articulation controller is employed to compensate for the unknown robot dynamics. The experiments have been performed on a Baxter robot to demonstrate the effectiveness of the proposed methods.},
  eventtitle = {{{IEEE Transactions}} on {{Cognitive}} and {{Developmental Systems}}},
  keywords = {Artificial neural networks,Cerebellar model articulation controller (CMAC),dynamic movement primitive (DMP),Dynamics,fuzzy Gaussian mixture model (FGMM),Gaussian mixture model,Gaussian mixture regression (GMR),neural control,READ,robot learning from demonstrations (LfDs),Service robots,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/yang_et_al_2019_biologically_inspired_motion_modeling_and_neural_control_for_robot_learning.pdf}
}

@article{yangDMPsbasedFrameworkRobot2018,
  title = {A {{DMPs-based}} Framework for Robot Learning and Generalization of Humanlike Variable Impedance Skills},
  author = {Yang, Chenguang and Zeng, Chao and Fang, Cheng and He, Wei and Li, Zhijun},
  date = {2018},
  journaltitle = {IEEE/ASME Transactions on Mechatronics},
  volume = {23},
  number = {3},
  pages = {1193--1203},
  publisher = {{IEEE}},
  keywords = {Dynamic movement primitives (DMPs),dynamical movement primitives,human-robot interaction,interface,manipulation,READ,stiffness   generalization,systems,tasks,variable impedance skill transfer},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/yang_et_al_2018_a_dmps-based_framework_for_robot_learning_and_generalization_of_humanlike.pdf}
}

@article{yangHumanlikeAdaptationForce2011,
  title = {Human-like Adaptation of Force and Impedance in Stable and Unstable Interactions},
  author = {Yang, Chenguang and Ganesh, Gowrishankar and Haddadin, Sami and Parusel, Sven and Albu-Schaeffer, Alin and Burdet, Etienne},
  date = {2011},
  journaltitle = {IEEE transactions on robotics},
  volume = {27},
  number = {5},
  pages = {918--930},
  publisher = {{IEEE}},
  keywords = {Adaptation model,Feedforward force,Feedforward neural networks,Force,human motor control,Humans,impedance,Impedance,READ,robotic control,Robots,STABILITY,Torque},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/yang_et_al_2011_human-like_adaptation_of_force_and_impedance_in_stable_and_unstable_interactions.pdf}
}

@inproceedings{yangIterativeSchemeSafe2021,
  title = {An {{Iterative Scheme}} of {{Safe Reinforcement Learning}} for {{Nonlinear Systems}} via {{Barrier Certificate Generation}}},
  booktitle = {Computer {{Aided Verification}}},
  author = {Yang, Zhengfeng and Zhang, Yidan and Lin, Wang and Zeng, Xia and Tang, Xiaochao and Zeng, Zhenbing and Liu, Zhiming},
  editor = {Silva, Alexandra and Leino, K. Rustan M.},
  date = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {467--490},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-81685-8_22},
  abstract = {In this paper, we propose a safe reinforcement learning approach to synthesize deep neural network (DNN) controllers for nonlinear systems subject to safety constraints. The proposed approach employs an iterative scheme where a learner and a verifier interact to synthesize safe DNN controllers. The learner trains a DNN controller via deep reinforcement learning, and the verifier certifies the learned controller through computing a maximal safe initial region and its corresponding barrier certificate, based on polynomial abstraction and bilinear matrix inequalities solving. Compared with the existing verification-in-the-loop synthesis methods, our iterative framework is a sequential synthesis scheme of controllers and barrier certificates, which can learn safe controllers with adaptive barrier certificates rather than user-defined ones. We implement the tool SRLBC and evaluate its performance over a set of benchmark examples. The experimental results demonstrate that our approach efficiently synthesizes safe DNN controllers even for a nonlinear system with dimension up~to 12.},
  isbn = {978-3-030-81685-8},
  langid = {english},
  keywords = {Barrier certificates,Continuous dynamical systems,Formal verification,READ,Safe reinforcement learning},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/yang_et_al_2021_an_iterative_scheme_of_safe_reinforcement_learning_for_nonlinear_systems_via.pdf}
}

@article{yangLearningBasedPredictivePath2021,
  title = {Learning-{{Based Predictive Path Following Control}} for {{Nonlinear Systems Under Uncertain Disturbances}}},
  author = {Yang, Rui and Zheng, Lei and Pan, Jiesen and Cheng, Hui},
  date = {2021-04},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {6},
  number = {2},
  pages = {2854--2861},
  issn = {2377-3766},
  doi = {10.1109/LRA.2021.3062805},
  abstract = {Accurate path following is challenging for autonomous robots operating in uncertain environments. Adaptive and predictive control strategies are crucial for a nonlinear robotic system to achieve high-performance path following control. In this letter, we propose a novel learning-based predictive control scheme that couples a high-level model predictive path following controller (MPFC) with a low-level learning-based feedback linearization controller (LB-FBLC) for nonlinear systems under uncertain disturbances. The low-level LB-FBLC utilizes Gaussian Processes to learn the uncertain environmental disturbances online and tracks the reference state accurately with a probabilistic stability guarantee. Meanwhile, the high-level MPFC exploits the linearized system model augmented with a virtual linear path dynamics model to optimize the evolution of path reference targets, and provides the reference states and controls for the low-level LB-FBLC. Simulation results illustrate the effectiveness of the proposed control strategy on a quadrotor path following task under unknown wind disturbances.},
  eventtitle = {{{IEEE Robotics}} and {{Automation Letters}}},
  keywords = {Adaptation models,Control architectures and programming,machine learning for robot control,motion control,Nonlinear dynamical systems,Predictive models,READ,Robots,Target tracking,Trajectory,Uncertainty},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/yang_et_al_2021_learning-based_predictive_path_following_control_for_nonlinear_systems_under.pdf}
}

@article{yangRobotLearningSystem2019,
  title = {Robot {{Learning System Based}} on {{Adaptive Neural Control}} and {{Dynamic Movement Primitives}}},
  author = {Yang, Chenguang and Chen, Chuize and He, Wei and Cui, Rongxin and Li, Zhijun},
  date = {2019-03},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {30},
  number = {3},
  pages = {777--787},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2018.2852711},
  abstract = {This paper proposes an enhanced robot skill learning system considering both motion generation and trajectory tracking. During robot learning demonstrations, dynamic movement primitives (DMPs) are used to model robotic motion. Each DMP consists of a set of dynamic systems that enhances the stability of the generated motion toward the goal. A Gaussian mixture model and Gaussian mixture regression are integrated to improve the learning performance of the DMP, such that more features of the skill can be extracted from multiple demonstrations. The motion generated from the learned model can be scaled in space and time. Besides, a neural-network-based controller is designed for the robot to track the trajectories generated from the motion model. In this controller, a radial basis function neural network is used to compensate for the effect caused by the dynamic environments. The experiments have been performed using a Baxter robot and the results have confirmed the validity of the proposed methods.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}} and {{Learning Systems}}},
  keywords = {Acceleration,Dynamic movement primitives (DMPs),Dynamics,Feature extraction,Gaussian mixture model (GMM),neural network (NN),READ,robot learning,Robot learning,STABILITY,Stability analysis,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/yang_et_al_2019_robot_learning_system_based_on_adaptive_neural_control_and_dynamic_movement.pdf}
}

@unpublished{yanLearningProbabilisticMultiModal2019,
  title = {Learning {{Probabilistic Multi-Modal Actor Models}} for {{Vision-Based Robotic Grasping}}},
  author = {Yan, Mengyuan and Li, Adrian and Kalakrishnan, Mrinal and Pastor, Peter},
  date = {2019-04-15},
  eprint = {1904.07319},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1904.07319},
  urldate = {2019-05-07},
  abstract = {Many previous works approach vision-based robotic grasping by training a value network that evaluates grasp proposals. These approaches require an optimization process at run-time to infer the best action from the value network. As a result, the inference time grows exponentially as the dimension of action space increases. We propose an alternative method, by directly training a neural density model to approximate the conditional distribution of successful grasp poses from the input images. We construct a neural network that combines Gaussian mixture and normalizing flows, which is able to represent multi-modal, complex probability distributions. We demonstrate on both simulation and real robot that the proposed actor model achieves similar performance compared to the value network using the Cross-Entropy Method (CEM) for inference, on top-down grasping with a 4 dimensional action space. Our actor model reduces the inference time by 3 times compared to the state-of-the-art CEM method. We believe that actor models will play an important role when scaling up these approaches to higher dimensional action spaces.},
  keywords = {#nosource,Computer Science - Machine Learning,Computer Science - Robotics,object grasping,reinforcement learning,robotic grasping,tno internship}
}

@inproceedings{yiEquivalenceContractionKoopman2021,
  title = {On the {{Equivalence}} of {{Contraction}} and {{Koopman Approaches}} for {{Nonlinear Stability}} and {{Control}}},
  booktitle = {2021 60th {{IEEE Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Yi, Bowen and Manchester, Ian R.},
  date = {2021-12},
  pages = {4609--4614},
  issn = {2576-2370},
  doi = {10.1109/CDC45484.2021.9683450},
  abstract = {In this paper we prove new connections between two frameworks for analysis and control of nonlinear systems: the Koopman operator framework and contraction analysis. Each method, in different ways, provides exact and global analyses of nonlinear systems by way of linear systems theory. The main results of this paper show equivalence between contraction and Koopman approaches for a wide class of stability analysis and control design problems. In particular: stability or stablizability in the Koopman framework implies the existence of a contraction metric (resp. control contraction metric) for the nonlinear system. Further, in certain cases, the converse holds: contraction implies the existence of a set of observables with which stability can verified via the Koopman framework. Moreover, the converse claims are based on a novel relation between the Koopman method and construction of a Kazantzis-Kravaris-Luenberger observer.},
  eventtitle = {2021 60th {{IEEE Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  keywords = {Conferences,Control design,Linear systems,Measurement,Nonlinear systems,Observers,READ,Stability analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/yi_manchester_2021_on_the_equivalence_of_contraction_and_koopman_approaches_for_nonlinear.pdf}
}

@inproceedings{yinConsensusbasedNormalizingFlowControl2022,
  title = {Consensus-Based {{Normalizing-Flow Control}}: {{A Case Study}} in {{Learning Dual-Arm Coordination}}},
  shorttitle = {Consensus-Based {{Normalizing-Flow Control}}},
  booktitle = {2022 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Yin, Hang and Verginis, Christos K. and Kragic, Danica},
  date = {2022-10},
  pages = {10417--10424},
  issn = {2153-0866},
  doi = {10.1109/IROS47612.2022.9981827},
  abstract = {We develop two consensus-based learning algorithms for multi-robot systems applied on complex tasks involving collision constraints and force interactions, such as the cooperative peg-in-hole placement. The proposed algorithms integrate multi-robot distributed consensus and normalizing-flow-based reinforcement learning. The algorithms guarantee the stability and the consensus of the multi-robot system's generalized variables in a transformed space. This transformed space is obtained via a diffeomorphic transformation parameterized by normalizing-flow models that the algorithms use to train the underlying task, learning hence skillful, dexterous trajectories required for the task accomplishment. We validate the proposed algorithms by parameterizing reinforcement learning policies, demonstrating efficient cooperative learning, and strong generalization of dual-arm assembly skills in a dynamics-engine simulator.},
  eventtitle = {2022 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  keywords = {End effectors,Heuristic algorithms,Neural networks,Protocols,READ,Reinforcement learning,Robot kinematics,Stability analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/yin_et_al_2022_consensus-based_normalizing-flow_control.pdf}
}

@inproceedings{yinEmbeddingKoopmanOptimal2022,
  title = {Embedding {{Koopman Optimal Control}} in {{Robot Policy Learning}}},
  booktitle = {2022 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Yin, Hang and Welle, Michael C. and Kragic, Danica},
  date = {2022-10},
  pages = {13392--13399},
  issn = {2153-0866},
  doi = {10.1109/IROS47612.2022.9981540},
  abstract = {Embedding an optimization process has been explored for imposing efficient and flexible policy structures. Existing work often build upon nonlinear optimization with explicitly iteration steps, making policy inference prohibitively expensive for online learning and real-time control. Our approach embeds a linear-quadratic-regulator (LQR) formulation with a Koopman representation, thus exhibiting the tractability from a closed-form solution and richness from a non-convex neural network. We use a few auxiliary objectives and reparameterization to enforce optimality conditions of the policy that can be easily integrated to standard gradient-based learning. Our approach is shown to be effective for learning policies rendering an optimality structure and efficient reinforcement learning, including simulated pendulum control, 2D and 3D walking, and manipulation for both rigid and deformable objects. We also demonstrate real world application in a robot pivoting task.},
  eventtitle = {2022 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  keywords = {Manipulators,READ,Reinforcement learning,Reliability,Rendering (computer graphics),Robot vision systems,Safety,Three-dimensional displays},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/yin_et_al_2022_embedding_koopman_optimal_control_in_robot_policy_learning.pdf}
}

@article{yinImitationLearningStability2022,
  title = {Imitation {{Learning With Stability}} and {{Safety Guarantees}}},
  author = {Yin, He and Seiler, Peter and Jin, Ming and Arcak, Murat},
  date = {2022},
  journaltitle = {IEEE Control Systems Letters},
  volume = {6},
  pages = {409--414},
  issn = {2475-1456},
  doi = {10.1109/LCSYS.2021.3077861},
  abstract = {A method is presented to learn neural network (NN) controllers with stability and safety guarantees through imitation learning (IL). Convex stability and safety conditions are derived for linear time-invariant systems with NN controllers by merging Lyapunov theory with local quadratic constraints to bound the activation functions in the NN. These conditions are incorporated in the IL process, which minimizes the IL loss, and maximizes the volume of the region of attraction associated with the NN controller simultaneously. An alternating direction method of multipliers based algorithm is proposed to solve the IL problem. The method is illustrated on a vehicle lateral control example.},
  eventtitle = {{{IEEE Control Systems Letters}}},
  keywords = {Artificial neural networks,Linear systems,Lyapunov methods,neural networks,READ,Safety,Stability criteria,Training,US Government,Vehicle dynamics},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/yin_et_al_2022_imitation_learning_with_stability_and_safety_guarantees.pdf}
}

@article{yinStabilityAnalysisUsing2021,
  title = {Stability Analysis Using Quadratic Constraints for Systems with Neural Network Controllers},
  author = {Yin, He and Seiler, Peter and Arcak, Murat},
  date = {2021},
  journaltitle = {IEEE Transactions on Automatic Control},
  volume = {67},
  number = {4},
  pages = {1980--1987},
  publisher = {{IEEE}},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/yin_et_al_2021_stability_analysis_using_quadratic_constraints_for_systems_with_neural_network.pdf}
}

@article{yuHumanRobotVariableImpedance2022,
  title = {Human-{{Robot Variable Impedance Skills Transfer Learning Based}} on {{Dynamic Movement Primitives}}},
  author = {Yu, Xinbo and Liu, Peisen and He, Wei and Liu, Yu and Chen, Qi and Ding, Liang},
  date = {2022-07},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {3},
  pages = {6463--6470},
  issn = {2377-3766},
  doi = {10.1109/LRA.2022.3154469},
  abstract = {Endowing robots with human-like abilities to perform motor skills smoothly and naturally is one of the important goals of robotics. Learning from demonstration (LfD) has been successfully applied for learning tasks on robots, for which the human tutor can demonstrate a successful execution. Learning human stiffness schedule strategy is one of the promising approaches for lightweight collaborative robots to execute physical in-contact tasks while remaining compliant when possible, as it enables robots to show human-like adaptive impedance behavior in an effective and efficient manner. In this letter, we develop a robot skill learning framework considering both movement and impedance features. Electromyography (EMG)-based method is used to estimate human upper limb stiffness such that the information of impedance can be obtained. Dynamic movement primitives (DMPs) model is employed to model movement and impedance features simultaneously. Throughout this procedure, the robot skills learning from a human tutor can be achieved. Besides, the learned skills can be generalized. An impedance controller imitating the human impedance mechanism is utilized in the task reproduction phase to achieve both task successful execution and safe physical interaction. The effectiveness of the proposed methods is verified in a water pumping task on a Kinova robot.},
  eventtitle = {{{IEEE Robotics}} and {{Automation Letters}}},
  keywords = {Bioinspired robot learning,dynamic movement primitives (DMPs),Electromyography,human factors and human-in-the-loop,Impedance,learning from demonstration (LfD),Muscles,READ,Robots,Task analysis,Trajectory,variable impedance skill transfer},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/yu_et_al_2022_human-robot_variable_impedance_skills_transfer_learning_based_on_dynamic.pdf}
}

@article{zacharakiSafetyBoundsHuman2020,
  title = {Safety Bounds in Human Robot Interaction: {{A}} Survey},
  shorttitle = {Safety Bounds in Human Robot Interaction},
  author = {Zacharaki, Angeliki and Kostavelis, Ioannis and Gasteratos, Antonios and Dokas, Ioannis},
  date = {2020-07-01},
  journaltitle = {Safety Science},
  shortjournal = {Safety Science},
  volume = {127},
  pages = {104667},
  issn = {0925-7535},
  doi = {10.1016/j.ssci.2020.104667},
  url = {https://www.sciencedirect.com/science/article/pii/S0925753520300643},
  urldate = {2022-07-22},
  abstract = {In the era of industrialization and automation, safety is a critical factor that should be considered during the design and realization of each new system that targets operation in close collaboration with humans. Of such systems are considered personal and professional service robots which collaborate and interact with humans at diverse applications environments. In this collaboration, human safety is an important factor in the wider field of human-robot interaction (HRI) since it facilitates their harmonic coexistence. The paper at hand aims to systemize the recent literature by describing the required levels of safety during human-robot interaction, focusing on the core functions of the collaborative robots when performing specific processes. It is also oriented towards the existing methods for psychological safety during human-robot collaboration and its impact at the robot behaviour, while also discusses in depth the psychological parameters of robots incorporation in industrial and social environments. Based on the existing works on safety features that minimize the risk of HRI, a classification of the existing works into five major categories namely, Robot Perceptions for Safe HRI, Cognition-enabled robot control in HRI, Action Planning for safe navigation close to humans, Hardware safety features, and Societal and Psychological factors is also applied. Finally, the current study further discusses the existing risk assessment techniques as methods to offer additional safety in robotic systems presenting thus a holistic analysis of the safety in contemporary robots, and proposes a roadmap for safety compliance features during the development of a robotic system.},
  langid = {english},
  keywords = {Human-robot interaction,Psychological safety issues,REVIEW,Robot safety,Safe collaboration,Safety standards,Safety techniques},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/zacharaki_et_al_2020_safety_bounds_in_human_robot_interaction2.pdf}
}

@article{zaidiModelbasedStrategyGrasping2017,
  title = {Model-Based Strategy for Grasping {{3D}} Deformable Objects Using a Multi-Fingered Robotic Hand},
  author = {Zaidi, Lazher and Corrales, Juan Antonio and Bouzgarrou, Belhassen Chedli and Mezouar, Youcef and Sabourin, Laurent},
  date = {2017-09-01},
  journaltitle = {Robotics and Autonomous Systems},
  shortjournal = {Robotics and Autonomous Systems},
  volume = {95},
  pages = {196--206},
  issn = {0921-8890},
  doi = {10.1016/j.robot.2017.06.011},
  url = {http://www.sciencedirect.com/science/article/pii/S0921889016308089},
  urldate = {2019-05-09},
  abstract = {This paper presents a model-based strategy for 3D deformable object grasping using a multi-fingered robotic hand. The developed contact model is based on two force components (normal force and tangential friction force, including slipping and sticking effects) and uses a non-linear mass–spring system to describe the object deformations due the mechanical load applied by the fingers of the robotic hand. The object–finger interaction is simulated in order to compute the required contact forces and deformations to robustly grasp objects with large deformations. Our approach is able to achieve this by using a non-linear model that outperforms current techniques that are limited to using linear models. After the contact forces computed by the simulation of the contact model guarantee the equilibrium of the grasp, they will be used as set-points for force-controlling the closing of the real fingers, and thus, the proposed grasping strategy is implemented. Two different objects (cube and sphere) made from two soft materials (foam and rubber) are tested in order to verify that the proposed model can represent their non-linear deformations and that the proposed grasp strategy can implement a robust grasp of them with a multi-fingered robotic hand equipped with tactile sensors. Thereby, both the grasping strategy and the proposed contact model are validated experimentally.},
  keywords = {#nosource,Deformable objects,Dexterous manipulation,machine learning,Multi-fingered robotic hands,object grasping,robotic grasping,Tactile sensors,tno internship},
  annotation = {00007}
}

@unpublished{zengLearningSynergiesPushing2018,
  title = {Learning {{Synergies}} between {{Pushing}} and {{Grasping}} with {{Self-supervised Deep Reinforcement Learning}}},
  author = {Zeng, Andy and Song, Shuran and Welker, Stefan and Lee, Johnny and Rodriguez, Alberto and Funkhouser, Thomas},
  date = {2018-03-27},
  eprint = {1803.09956},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1803.09956},
  urldate = {2019-04-17},
  abstract = {Skilled robotic manipulation benefits from complex synergies between non-prehensile (e.g. pushing) and prehensile (e.g. grasping) actions: pushing can help rearrange cluttered objects to make space for arms and fingers; likewise, grasping can help displace objects to make pushing movements more precise and collision-free. In this work, we demonstrate that it is possible to discover and learn these synergies from scratch through model-free deep reinforcement learning. Our method involves training two fully convolutional networks that map from visual observations to actions: one infers the utility of pushes for a dense pixel-wise sampling of end effector orientations and locations, while the other does the same for grasping. Both networks are trained jointly in a Q-learning framework and are entirely self-supervised by trial and error, where rewards are provided from successful grasps. In this way, our policy learns pushing motions that enable future grasps, while learning grasps that can leverage past pushes. During picking experiments in both simulation and real-world scenarios, we find that our system quickly learns complex behaviors amid challenging cases of clutter, and achieves better grasping success rates and picking efficiencies than baseline alternatives after only a few hours of training. We further demonstrate that our method is capable of generalizing to novel objects. Qualitative results (videos), code, pre-trained models, and simulation environments are available at http://vpg.cs.princeton.edu},
  langid = {english},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,object manipulation,robotic grasping,Statistics - Machine Learning,tno internship}
}

@unpublished{zengRoboticPickandPlaceNovel2017,
  title = {Robotic {{Pick-and-Place}} of {{Novel Objects}} in {{Clutter}} with {{Multi-Affordance Grasping}} and {{Cross-Domain Image Matching}}},
  author = {Zeng, Andy and Song, Shuran and Yu, Kuan-Ting and Donlon, Elliott and Hogan, Francois R. and Bauza, Maria and Ma, Daolin and Taylor, Orion and Liu, Melody and Romo, Eudald and Fazeli, Nima and Alet, Ferran and Dafle, Nikhil Chavan and Holladay, Rachel and Morona, Isabella and Nair, Prem Qu and Green, Druck and Taylor, Ian and Liu, Weber and Funkhouser, Thomas and Rodriguez, Alberto},
  date = {2017-10-03},
  eprint = {1710.01330},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1710.01330},
  urldate = {2019-05-09},
  abstract = {This paper presents a robotic pick-and-place system that is capable of grasping and recognizing both known and novel objects in cluttered environments. The key new feature of the system is that it handles a wide range of object categories without needing any task-specific training data for novel objects. To achieve this, it first uses a category-agnostic affordance prediction algorithm to select and execute among four different grasping primitive behaviors. It then recognizes picked objects with a cross-domain image classification framework that matches observed images to product images. Since product images are readily available for a wide range of objects (e.g., from the web), the system works out-of-the-box for novel objects without requiring any additional training data. Exhaustive experimental results demonstrate that our multi-affordance grasping achieves high success rates for a wide variety of objects in clutter, and our recognition algorithm achieves high accuracy for both known and novel grasped objects. The approach was part of the MIT-Princeton Team system that took 1st place in the stowing task at the 2017 Amazon Robotics Challenge. All code, datasets, and pre-trained models are available online at http://arc.cs.princeton.edu},
  keywords = {#nosource,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics,object grasping,reinforcement learning,robotic grasping,tno internship},
  annotation = {00057}
}

@article{zengRobotLearningHuman2018,
  title = {Robot Learning Human Stiffness Regulation for Hybrid Manufacture},
  author = {Zeng, Chao and Yang, Chenguang and Chen, Zhaopeng and Dai, Shi-Lu},
  date = {2018-01-01},
  journaltitle = {Assembly Automation},
  volume = {38},
  number = {5},
  pages = {539--547},
  publisher = {{Emerald Publishing Limited}},
  issn = {0144-5154},
  doi = {10.1108/AA-02-2018-019},
  url = {https://doi.org/10.1108/AA-02-2018-019},
  urldate = {2022-09-12},
  abstract = {Purpose Teaching by demonstration (TbD) is a promising way for robot learning skills in human and robot collaborative hybrid manufacturing lines. Traditionally, TbD systems have only concentrated on how to enable robots to learn movement skills from humans. This paper aims to develop an extended TbD system which can also enable learning stiffness regulation strategies from humans.Design/methodology/approach Here, the authors propose an extended dynamical motor primitives (DMP) framework to achieve this goal. In addition to the advantages of the traditional ones, the authors’ framework can enable robots to simultaneously learn stiffness and the movement from human demonstrations. Additionally, Gaussian mixture model (GMM) is used to capture the features of movement and of stiffness from multiple demonstrations of the same skill. Human limb surface electromyography (sEMG) signals are estimated to obtain the reference stiffness profiles.Findings The authors have experimentally demonstrated the effectiveness of the proposed framework. It shows that the authors approach could allow the robot to execute tasks in a variable impedance control mode with the learned movement trajectories and stiffness profiles.Originality/value In robot skill acquisition, DMP is widely used to encode robotic behaviors. So far, however, these DMP modes do not provide the ability to properly represent and generalize stiffness profiles. The authors argue that both movement trajectories and stiffness profiles should be considered equally in robot skill learning. The authors’ approach has great potential of applications in the future hybrid manufacturing lines.},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/zeng_et_al_2018_robot_learning_human_stiffness_regulation_for_hybrid_manufacture.pdf}
}

@article{zengSimultaneouslyEncodingMovement2021,
  title = {Simultaneously {{Encoding Movement}} and {{sEMG-Based Stiffness}} for {{Robotic Skill Learning}}},
  author = {Zeng, Chao and Yang, Chenguang and Cheng, Hong and Li, Yanan and Dai, Shi-Lu},
  date = {2021-02},
  journaltitle = {IEEE Transactions on Industrial Informatics},
  volume = {17},
  number = {2},
  pages = {1244--1252},
  issn = {1941-0050},
  doi = {10.1109/TII.2020.2984482},
  abstract = {Transferring human stiffness regulation strategies to robots enables them to effectively and efficiently acquire adaptive impedance control policies to deal with uncertainties during the accomplishment of physical contact tasks in an unstructured environment. In this article, we develop such a physical human-robot interaction system which allows robots to learn variable impedance skills from human demonstrations. Specifically, the biological signals, i.e., surface electromyography are utilized for the extraction of human arm stiffness features during the task demonstration. The estimated human arm stiffness is then mapped into a robot impedance controller. The dynamics of both movement and stiffness are simultaneously modeled by using a model combining the hidden semi-Markov model and the Gaussian mixture regression. More importantly, the correlation between the movement information and the stiffness information is encoded in a systematic manner. This approach enables capturing uncertainties over time and space and allows the robot to satisfy both position and stiffness requirements in a task with modulation of the impedance controller. The experimental study validated the proposed approach.},
  eventtitle = {{{IEEE Transactions}} on {{Industrial Informatics}}},
  keywords = {Adaptive impedance control,Force,Hidden Markov models,human–robot interaction systems,Impedance,Manipulators,multimodality,Muscles,Task analysis},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/zeng_et_al_2021_simultaneously_encoding_movement_and_semg-based_stiffness_for_robotic_skill.pdf}
}

@inproceedings{zengSystemsTheoreticAspects2018,
  title = {On Systems Theoretic Aspects of {{Koopman}} Operator Theoretic Frameworks},
  booktitle = {2018 {{IEEE Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Zeng, Shen},
  date = {2018},
  pages = {6422--6427},
  publisher = {{IEEE}},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/zeng_2018_on_systems_theoretic_aspects_of_koopman_operator_theoretic_frameworks.pdf}
}

@unpublished{zengTossingBotLearningThrow2019,
  title = {{{TossingBot}}: {{Learning}} to {{Throw Arbitrary Objects}} with {{Residual Physics}}},
  shorttitle = {{{TossingBot}}},
  author = {Zeng, Andy and Song, Shuran and Lee, Johnny and Rodriguez, Alberto and Funkhouser, Thomas},
  date = {2019-03-27},
  eprint = {1903.11239},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1903.11239},
  urldate = {2019-04-17},
  abstract = {We investigate whether a robot arm can learn to pick and throw arbitrary objects into selected boxes quickly and accurately. Throwing has the potential to increase the physical reachability and picking speed of a robot arm. However, precisely throwing arbitrary objects in unstructured settings presents many challenges: from acquiring reliable pre-throw conditions (e.g. initial pose of object in manipulator) to handling varying object-centric properties (e.g. mass distribution, friction, shape) and dynamics (e.g. aerodynamics). In this work, we propose an end-to-end formulation that jointly learns to infer control parameters for grasping and throwing motion primitives from visual observations (images of arbitrary objects in a bin) through trial and error. Within this formulation, we investigate the synergies between grasping and throwing (i.e., learning grasps that enable more accurate throws) and between simulation and deep learning (i.e., using deep networks to predict residuals on top of control parameters predicted by a physics simulator). The resulting system, TossingBot, is able to grasp and throw arbitrary objects into boxes located outside its maximum reach range at 500+ mean picks per hour (600+ grasps per hour with 85\% throwing accuracy); and generalizes to new objects and target locations. Videos are available at https://tossingbot.cs.princeton.edu},
  langid = {english},
  keywords = {#nosource,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,machine learning control,object manipulation,robotic grasping,Statistics - Machine Learning,tno internship}
}

@article{zhaiAdaptiveNeuralSynchronized2022,
  title = {Adaptive Neural Synchronized Impedance Control for Cooperative Manipulators Processing under Uncertain Environments},
  author = {Zhai, Anbang and Zhang, Haiyun and Wang, Jin and Lu, Guodong and Li, Junjie and Chen, Silu},
  date = {2022-06},
  journaltitle = {Robotics and Computer-Integrated Manufacturing},
  shortjournal = {Robot. Comput.-Integr. Manuf.},
  volume = {75},
  pages = {102291},
  publisher = {{Pergamon-Elsevier Science Ltd}},
  location = {{Oxford}},
  issn = {0736-5845},
  doi = {10.1016/j.rcim.2021.102291},
  url = {https://www.sciencedirect.com/science/article/pii/S073658452100171X},
  urldate = {2022-05-02},
  abstract = {In robotic cooperation manufacturing occasions, like grinding, assembling, welding, etc., the position-force synchronization tracking control for robotic cooperative manipulators is critical to improve the comprehensive manufacturing quality with high-precision and high-adaptability. In terms of these problems, this paper proposes an adaptive neural synchronized impedance controller (ANSIC) for cooperative manipulators processing. The proposed method includes two non-parallel control loops of the cooperative system to achieve and guarantee the desired movement trajectory and manufacturing force of the cooperation task. In the inner position tracking loop, an adaptive RBF-neural network based synchronization sliding controller is designed to simultaneously estimate the uncertain dynamic parameters of the robotic manipulators and improve the cooperative position tracking precision. In the outer force tracking loop, another RBF-neural network is applied to reform the impedance control model automatically and compensate the position and stiffness errors of the uncertain workpiece environment. Mathematical proof and experiments under various conditions are conducted. The results demonstrate the effective convergences of both the cooperative processing trajectory and force despite the uncertain environments.},
  langid = {english},
  keywords = {Adaptive synchronization control,control scheme,Cooperative manipulators,Neural   networks,Position-force tracking,Reformed impedance model,robotic manipulator,STABILITY,tracking control,Uncertain   environments},
  annotation = {WOS:000779404900002},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/zhai_et_al_2022_adaptive_neural_synchronized_impedance_control_for_cooperative_manipulators.pdf}
}

@article{zhaiMotionPlanningMethod2022,
  title = {A {{Motion Planning Method}} for {{Robots Based}} on {{DMPs}} and {{Modified Obstacle-Avoiding Algorithm}}},
  author = {Zhai, Di-Hua and Xia, Zhiqiang and Wu, Haocun and Xia, Yuanqing},
  date = {2022},
  journaltitle = {IEEE Transactions on Automation Science and Engineering},
  pages = {1--11},
  issn = {1558-3783},
  doi = {10.1109/TASE.2022.3202351},
  abstract = {This paper addresses the motion planning of the manipulator in task space. To improve the overall trajectory performance, a special motion planning method based on DMPs (Dynamic Movement Primitives) and the modified obstacle-avoiding algorithm is proposed. The proposed method solves the problems of trajectory jitter and inability to avoid obstacles in some scenarios, which are faced by the scheme of steering angle. Besides, it improves the retention of teaching intentions, reduces the loss of free space, and helps the system adapt to the dynamic environment. At the theoretical level, the convergence of the target state has been proved using Lyapunov stability theory-based analysis. The availability of the proposed method is validated and analyzed by performing a series of numerical simulations and Baxter robot experiments. The results indicate that the proposed method can provide reliable solutions for motion planning. Note to Practitioners—From the perspective of demonstration learning, this paper aims to elevate the motion planning of the manipulator in task space, especially in improving the obstacle avoidance performance. The existing DMPs-based motion planning algorithm uses the scheme of steering angle to achieve obstacle avoidance, and the obstacle is regarded as a mesh of points on the boundary. The obstacle-avoiding performance is limited. How to combine the obstacle-avoiding algorithm with the DMPs-based motion planning algorithm more effectively, so as to simultaneously achieve retaining teaching intentions as much as possible, still faces challenges. This paper designs a modified obstacle-avoiding algorithm, which solves the problems of trajectory jitter and inability to avoid obstacles in some circumstances, and improves the retention of teaching intentions. The modified obstacle-avoiding algorithm is also applied to the dynamic environment. The proposed method satisfies Lyapunov stability theory-based analysis, and the experiments on Baxter robot verify the feasibility.},
  eventtitle = {{{IEEE Transactions}} on {{Automation Science}} and {{Engineering}}},
  keywords = {dynamic movement primitives,Education,End effectors,Heuristic algorithms,imitation learning,Jitter,obstacle avoidance,Planning,READ,Robot,Task analysis,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/zhai_et_al_2022_a_motion_planning_method_for_robots_based_on_dmps_and_modified.pdf}
}

@article{zhakatayevClosedLoopControlVariable2015,
  title = {Closed-{{Loop Control}} of {{Variable Stiffness Actuated Robots}} via {{Nonlinear Model Predictive Control}}},
  author = {Zhakatayev, Altay and Rubagotti, Matteo and Varol, Huseyin Atakan},
  date = {2015},
  journaltitle = {IEEE Access},
  volume = {3},
  pages = {235--248},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2015.2418157},
  abstract = {Variable stiffness actuation has recently attracted great interest in robotics, especially in areas involving a high degree of human-robot interaction. After investigating various design approaches for variable stiffness actuated (VSA) robots, currently the focus is shifting to the control of these systems. Control of VSA robots is challenging due to the intrinsic nonlinearity of their dynamicsdynamics and the need to satisfy constraints on input and state variables. Contrary to the partially open-loop state-of-the-art approaches, in this paper, we present a close-loop control framework for VSA robots leveraging recent increases in computational resources and advances in optimization algorithms. In particular, we generate reference trajectories by means of open-loop optimal control, and track these trajectories via nonlinear model predictive control in a closed-loop manner. In order to show the advantages of our proposed scheme with respect to the previous (partially open-loop) ones, extensive simulation and real-world experiments were conducted using a two link planar manipulator for a ball throwing task. The results of these experiments indicate that the closed-loop scheme outperforms the partially open loop one due to its ability to compensate for model uncertainties and external disturbances, while satisfying the imposed constraints.},
  eventtitle = {{{IEEE Access}}},
  keywords = {Closed loop systems,Design methodology,embedded optimization,Embedded Optimization,Heuristic algorithms,Human-robot interaction,Kinematics,model predictive control,Model Predictive Control,Nonlinear systems,optimization algorithms,Optimization Algorithms,Predictive control,Robot manipulation,Robot Manipulation,Robots,variable stiffness actuation,Variable Stiffness Actuation},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/zhakatayev_et_al_2015_closed-loop_control_of_variable_stiffness_actuated_robots_via_nonlinear_model2.pdf;/home/ricks/Zotero/storage/6EG7KEN9/Zhakatayev et al. - 2015 - Closed-Loop Control of Variable Stiffness Actuated.pdf}
}

@article{zhangAdaptiveNeuralControl2018,
  title = {Adaptive {{Neural Control}} for {{Robotic Manipulators With Output Constraints}} and {{Uncertainties}}},
  author = {Zhang, Shuang and Dong, Yiting and Ouyang, Yuncheng and Yin, Zhao and Peng, Kaixiang},
  date = {2018-11},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {29},
  number = {11},
  pages = {5554--5564},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2018.2803827},
  abstract = {This paper investigates adaptive neural control methods for robotic manipulators, subject to uncertain plant dynamics and constraints on the joint position. The barrier Lyapunov function is employed to guarantee that the joint constraints are not violated, in which the Moore-Penrose pseudo-inverse term is used in the control design. To handle the unmodeled dynamics, the neural network (NN) is adopted to approximate the uncertain dynamics. The NN control based on full-state feedback for robots is proposed when all states of the closed loop are known. Subsequently, only the robot joint is measurable in practice; output feedback control is designed with a high-gain observer to estimate unmeasurable states. Through the Lyapunov stability analysis, system stability is achieved with the proposed control, and the system output achieves convergence without violation of the joint constraints. Simulation is conducted to approve the feasibility and superiority of the proposed NN control.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}} and {{Learning Systems}}},
  keywords = {Adaptive control,Artificial neural networks,barrier Lyapunov function (BLF),constraints,Control design,Manipulator dynamics,neural networks (NNs),robot,Uncertainty},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/zhang_et_al_2018_adaptive_neural_control_for_robotic_manipulators_with_output_constraints_and.pdf}
}

@article{zhangAdversarialDiscriminativeSimtoreal2018,
  title = {Adversarial {{Discriminative Sim-to-real Transfer}} of {{Visuo-motor Policies}}},
  author = {Zhang, Fangyi and Leitner, Jürgen and Ge, Zongyuan and Milford, Michael and Corke, Peter},
  date = {2018},
  url = {https://core.ac.uk/display/93946873},
  urldate = {2019-04-15},
  abstract = {Various approaches have been proposed to learn visuo-motor policies for real-world robotic applications. One solution is first learning in simulation then transferring to the real world. In the transfer, most existing approaches need real-world images with labels. However, the labelling process is often expensive or even impractical in many robotic applications. In this paper, we propose an adversarial discriminative sim-to-real transfer approach to reduce the cost of labelling real data. The effectiveness of the approach is demonstrated with modular networks in a table-top object reaching task where a 7 DoF arm is controlled in velocity mode to reach a blue cuboid in clutter through visual observations. The adversarial transfer approach reduced the labelled real data requirement by 50\%. Policies can be transferred to real environments with only 93 labelled and 186 unlabelled real images. The transferred visuo-motor policies are robust to novel (not seen in training) objects in clutter and even a moving target, achieving a 97.8\% success rate and 1.8 cm control accuracy.Comment: Under review for the International Journal of Robotics Researc},
  langid = {british},
  keywords = {#nosource,robotic grasping,sim-to-real,tno internship}
}

@article{zhangLearningAccurateStable2022,
  title = {Learning {{Accurate}} and {{Stable Point-to-Point Motions}}: {{A Dynamic System Approach}}},
  shorttitle = {Learning {{Accurate}} and {{Stable Point-to-Point Motions}}},
  author = {Zhang, Yu and Cheng, Long and Li, Houcheng and Cao, Ran},
  date = {2022-04},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {2},
  pages = {1510--1517},
  issn = {2377-3766},
  doi = {10.1109/LRA.2022.3140677},
  abstract = {This letter proposes a dynamic system approach to learn point-to-point motions while keeping the stability of the dynamic system. The proposed approach is grounded on a Learning from Demonstration (LfD) method based on a neural network, which gets a better reproduction performance while guaranteeing the generalization ability. The proposed approach has been experimentally validated on the LASA dataset and by the “pick-and-place” task of Franka Emika robot, and experimental results demonstrate that: (1) compared with the state-of-the-art results, the trajectory generated by the proposed approach achieves higher accuracy (approximately 24.79\%) in terms of the similarity with respect to the demonstration; (2) the proposed approach can handle high dimensional data and learn from one or more demonstrations; (3) the proposed approach can guarantee the performance regardless of the variation of starting points even in the case of high dimensional complex motions.},
  eventtitle = {{{IEEE Robotics}} and {{Automation Letters}}},
  keywords = {Data models,dynamic system,Dynamical systems,generalization performance,high dimensional data,IMPORTANT,Lyapunov methods,neural network,Numerical stability,Point-to-point tasks,READ,Robot kinematics,Task analysis,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/zhang_et_al_2022_learning_accurate_and_stable_point-to-point_motions.pdf}
}

@inproceedings{zhangLearningGeneralizingVariable2022,
  title = {Learning and {{Generalizing Variable Impedance Manipulation Skills}} from {{Human Demonstrations}}},
  booktitle = {2022 {{IEEE}}/{{ASME International Conference}} on {{Advanced Intelligent Mechatronics}} ({{AIM}})},
  author = {Zhang, Yan and Zhao, Fei and Liao, Zhiwei},
  date = {2022-07},
  pages = {810--815},
  issn = {2159-6255},
  doi = {10.1109/AIM52237.2022.9863389},
  abstract = {By learning a variable impedance control policy, robot assistants can intelligently adapt their manipulation compliance to ensure both safe interaction and proper task completion when operating in human-robot coexisting environments. In this paper, we propose a DMP-based framework that learns and generalizes variable impedance manipulation skills from human demonstrations. This framework improves robots′ adaptability to environment changes(i.e. the weight and shape changes of grasping object at the robot end-effector) and inherits the efficiency of demonstration-variance-based stiffness estimation methods. Besides, with our stiffness estimation method, we generate not only translational stiffness profiles but also rotational stiffness profiles that are ignored or incomplete in most learning variable impedance control papers. Real-world experiments on a 7 DoF redundant robot manipulator have been conducted to validate the effectiveness of our framework.},
  eventtitle = {2022 {{IEEE}}/{{ASME International Conference}} on {{Advanced Intelligent Mechatronics}} ({{AIM}})},
  keywords = {End effectors,Estimation,Grasping,Impedance,Mechatronics,READ,Shape,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/zhang_et_al_2022_learning_and_generalizing_variable_impedance_manipulation_skills_from_human.pdf}
}

@article{zhangLearningImpedanceRegulation2021,
  title = {Learning Impedance Regulation Skills for Robot Belt Grinding from Human Demonstrations},
  author = {Zhang, Guojun and Ni, Fenglei and Liu, Hong and Jiang, Zainan and Yang, Guocai and Li, Chongyang},
  date = {2021-01-01},
  journaltitle = {Assembly Automation},
  volume = {41},
  number = {4},
  pages = {431--440},
  publisher = {{Emerald Publishing Limited}},
  issn = {0144-5154},
  doi = {10.1108/AA-08-2020-0110},
  url = {https://doi.org/10.1108/AA-08-2020-0110},
  urldate = {2022-04-28},
  abstract = {Purpose The purpose of this paper is to transfer the impedance regulation of manual belt grinding to robot belt grinding control. Design/methodology/approach This paper presents a novel methodology for transmitting human impedance regulation skills to robot control in robot belt grinding. First, according to the human grinding experimental data, the skilled worker’s arm impedance regulation is calculated. Next, the human skills are encapsulated as the statistical learning model where the kernel parameters are learned from the demonstration data by Gaussian process regression (GPR) algorithms. The desired profiles of robot are generated by the task planner based on the learned skill knowledge model. Lastly, the learned skill knowledge model is integrated with an adaptive hybrid position-force controller over the trajectory and force of end-effector in robot belt grinding task. Findings Manual grinding skills are represented and transferred to robot belt grinding for higher grinding quality of the workpiece. Originality/value The impedance of the manual grinding is estimated by k-means++ algorithm at different grinding phases. Manual grinding skills (e.g. trajectory, impedance regulation) are represented and modeled by GMM and GPR algorithms. The desired trajectory, force and impedance of robot are generated by the planner based on the learned skills knowledge model. An adaptive hybrid position-force controller is designed based on learned skill knowledge model. This paper proposes a torque-tracking controller to suppress the vibration in robot belt grinding process.},
  keywords = {Gaussian processes,GP,Impedance regulation,Learning from demonstration,READ,Robot belt grinding},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/zhang_et_al_2021_learning_impedance_regulation_skills_for_robot_belt_grinding_from_human.pdf}
}

@inproceedings{zhangLearningRiemannianStable2022,
  title = {Learning {{Riemannian Stable Dynamical Systems}} via {{Diffeomorphisms}}},
  author = {Zhang, Jiechao and Mohammadi, Hadi Beik and Rozo, Leonel},
  date = {2022-11-06},
  url = {https://openreview.net/forum?id=o8dLx8OVcNk},
  urldate = {2023-01-18},
  abstract = {Dexterous and autonomous robots should be capable of executing elaborated dynamical motions skillfully. Learning techniques may be leveraged to build models of such dynamic skills. To accomplish this, the learning model needs to encode a stable vector field that resembles the desired motion dynamics. This is challenging as the robot state does not evolve on a Euclidean space, and therefore the stability guarantees and vector field encoding need to account for the geometry arising from, for example, the orientation representation. To tackle this problem, we propose learning Riemannian stable dynamical systems (RSDS) from demonstrations, allowing us to account for different geometric constraints resulting from the dynamical system state representation. Our approach provides Lyapunov-stability guarantees on Riemannian manifolds that are enforced on the desired motion dynamics via diffeomorphisms built on neural manifold ODEs. We show that our Riemannian approach makes it possible to learn stable dynamical systems displaying complicated vector fields on both illustrative examples and real-world manipulation tasks, where Euclidean approximations fail.},
  eventtitle = {6th {{Annual Conference}} on {{Robot Learning}}},
  langid = {english},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/zhang_et_al_2022_learning_riemannian_stable_dynamical_systems_via_diffeomorphisms.pdf}
}

@article{zhangLearningVariableImpedance2021,
  title = {Learning {{Variable Impedance Control}} via {{Inverse Reinforcement Learning}} for {{Force-Related Tasks}}},
  author = {Zhang, Xiang and Sun, Liting and Kuang, Zhian and Tomizuka, Masayoshi},
  date = {2021-04},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {6},
  number = {2},
  pages = {2225--2232},
  issn = {2377-3766},
  doi = {10.1109/LRA.2021.3061374},
  abstract = {Many manipulation tasks require robots to interact with unknown environments. In such applications, the ability to adapt the impedance according to different task phases and environment constraints is crucial for safety and performance. Although many approaches based on deep reinforcement learning (RL) and learning from demonstration (LfD) have been proposed to obtain variable impedance skills on contact-rich manipulation tasks, these skills are typically task-specific and could be sensitive to changes in task settings. This letter proposes an inverse reinforcement learning (IRL) based approach to recover both the variable impedance policy and reward function from expert demonstrations. We explore different action space of the reward functions to achieve a more general representation of expert variable impedance skills. Experiments on two variable impedance tasks (Peg-in-Hole and Cup-on-Plate) were conducted in both simulations and on a real FANUC LR Mate 200iD/7 L industrial robot. The comparison results with behavior cloning and force-based IRL proved that the learned reward function in the gain action space has better transferability than in the force space. Experiment videos are available at https://msc.berkeley.edu/research/impedance-irl.html.},
  eventtitle = {{{IEEE Robotics}} and {{Automation Letters}}},
  keywords = {Aerospace electronics,Compliance and impedance control,Force,Impedance,learning from demonstration,machine learning for robot control,Reinforcement learning,Robots,Task analysis,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/zhang_et_al_2021_learning_variable_impedance_control_via_inverse_reinforcement_learning_for.pdf}
}

@article{zhangNeuralApproximationbasedAdaptive2020,
  title = {Neural Approximation-Based Adaptive Variable Impedance Control of Robots},
  author = {Zhang, Xuexin and Sun, Tairen and Deng, Dongning},
  date = {2020-09-01},
  journaltitle = {Transactions of the Institute of Measurement and Control},
  shortjournal = {Transactions of the Institute of Measurement and Control},
  volume = {42},
  number = {13},
  pages = {2589--2598},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {0142-3312},
  doi = {10.1177/0142331220932649},
  url = {https://doi.org/10.1177/0142331220932649},
  urldate = {2022-05-02},
  abstract = {Variable impedance control improves compliance and robustness in robot-environment interaction through variation of the desired stiffness and the desired damping. This paper proposes neural approximation-based variable impedance controllers for robots in robot-environment interaction. Constraints on variable impedance parameters are given to ensure the exponential stability of the desired first- and second-order variable impedance dynamics. Adaptive neural network controllers are proposed to ensure the achievement of the desired first- and second-order variable impedance dynamics through convergence of variable impedance errors. In the neural networks, deadzone modifications are utilized to enhance robustness by turning off adaptation when auxiliary tracking errors enter the constructed small neighbourhoods of zero. The proposed variable impedance control methods in this paper guarantee the stability and achievement of the desired variable impedance dynamics. Theoretical analysis and simulation results validate the effectiveness of the proposed variable impedance control methods.},
  langid = {english},
  keywords = {adaptive control,impedance control,IMPORTANT,neural network,READ,Robot,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/zhang_et_al_2020_neural_approximation-based_adaptive_variable_impedance_control_of_robots.pdf}
}

@article{zhangNeuralNetworkBased2023,
  title = {A Neural Network Based Framework for Variable Impedance Skills Learning from Demonstrations},
  author = {Zhang, Yu and Cheng, Long and Cao, Ran and Li, Houcheng and Yang, Chenguang},
  date = {2023-02-01},
  journaltitle = {Robotics and Autonomous Systems},
  shortjournal = {Robotics and Autonomous Systems},
  volume = {160},
  pages = {104312},
  issn = {0921-8890},
  doi = {10.1016/j.robot.2022.104312},
  url = {https://www.sciencedirect.com/science/article/pii/S0921889022002019},
  urldate = {2023-05-03},
  abstract = {Robots are becoming standard collaborators not only in factories, hospitals, and offices, but also in people’s homes, where they can play an important role in situations where a human cannot complete a task alone or needs the help of another person (i.e., collaborative tasks). Variable impedance control with contact forces is critical for robots to successfully perform such manipulation tasks, and robots should be equipped with adaptive capabilities because conditions vary significantly for different robotic tasks in dynamic environments. This can be achieved by learning human motion capabilities and variable impedance skills. In this paper, a neural-network-based framework for learning variable impedance skills is proposed. The proposed approach builds the full stiffness function with the acquired forces and position learned from demonstrations, and then is used together with the sensed data to achieve the variable impedance control. The proposed algorithm can adapt to unknown situations that change the learned motion skill as needed (e.g., adapt to intermediate via-points or avoid obstacles). The proposed framework consists of two parts: Learning motion features and learning impedance features. The motion features learning is validated by reproducing, generalizing, and adapting to transit points and avoiding obstacles in the LASA dataset. Impedance features learning is validated based on a virtual variable stiffness system that achieves higher accuracy (approximately 90\%) compared to traditional methods in a manual dataset, and the whole framework is validated through a co-manipulation task between a person and the Franka Emika robot.},
  langid = {english},
  keywords = {Human–robot interaction,Learning from demonstrations,Skills learning,Variable impedance skill},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/zhang_et_al_2023_a_neural_network_based_framework_for_variable_impedance_skills_learning_from2.pdf}
}

@article{zhaoHybridLearningOptimization2022,
  title = {A {{Hybrid Learning}} and {{Optimization Framework}} to {{Achieve Physically Interactive Tasks With Mobile Manipulators}}},
  author = {Zhao, Jianzhuang and Giammarino, Alberto and Lamon, Edoardo and Gandarias, Juan M. and Momi, Elena De and Ajoudani, Arash},
  date = {2022-07},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {3},
  pages = {8036--8043},
  issn = {2377-3766},
  doi = {10.1109/LRA.2022.3187258},
  abstract = {This letter proposes a hybrid learning and optimization framework for mobile manipulators for complex and physically interactive tasks. The framework exploits an admittance-type physical interface to obtain intuitive and simplified human demonstrations and Gaussian Mixture Model (GMM)/Gaussian Mixture Regression (GMR) to encode and generate the learned task requirements in terms of position, velocity, and force profiles. Next, using the desired trajectories and force profiles generated by GMM/GMR, the impedance parameters of a Cartesian impedance controller are optimized online through a Quadratic Program augmented with an energy tank to ensure the passivity of the controlled system. Two experiments are conducted to validate the framework, comparing our method with two approaches with constant stiffness (high and low). The results showed that the proposed method outperforms the other two cases in terms of trajectory tracking and generated interaction forces, even in the presence of disturbances such as unexpected end-effector collisions.},
  eventtitle = {{{IEEE Robotics}} and {{Automation Letters}}},
  keywords = {Compliance and impedance control,Force,imitation learning,Impedance,mobile mani- pulation,READ,Robot sensing systems,Sensors,Task analysis,Torque,Trajectory},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/zhao_et_al_2022_a_hybrid_learning_and_optimization_framework_to_achieve_physically_interactive.pdf}
}

@inproceedings{zhaoSimtoRealTransferDeep2020,
  title = {Sim-to-{{Real Transfer}} in {{Deep Reinforcement Learning}} for {{Robotics}}: A {{Survey}}},
  shorttitle = {Sim-to-{{Real Transfer}} in {{Deep Reinforcement Learning}} for {{Robotics}}},
  booktitle = {2020 {{IEEE Symposium Series}} on {{Computational Intelligence}} ({{SSCI}})},
  author = {Zhao, Wenshuai and Queralta, Jorge Peña and Westerlund, Tomi},
  date = {2020-12},
  pages = {737--744},
  doi = {10.1109/SSCI47803.2020.9308468},
  url = {https://ieeexplore.ieee.org/abstract/document/9308468},
  urldate = {2024-01-08},
  abstract = {Deep reinforcement learning has recently seen huge success across multiple areas in the robotics domain. Owing to the limitations of gathering real-world data, i.e., sample inefficiency and the cost of collecting it, simulation environments are utilized for training the different agents. This not only aids in providing a potentially infinite data source, but also alleviates safety concerns with real robots. Nonetheless, the gap between the simulated and real worlds degrades the performance of the policies once the models are transferred into real robots. Multiple research efforts are therefore now being directed towards closing this sim-toreal gap and accomplish more efficient policy transfer. Recent years have seen the emergence of multiple methods applicable to different domains, but there is a lack, to the best of our knowledge, of a comprehensive review summarizing and putting into context the different methods. In this survey paper, we cover the fundamental background behind sim-to-real transfer in deep reinforcement learning and overview the main methods being utilized at the moment: domain randomization, domain adaptation, imitation learning, meta-learning and knowledge distillation. We categorize some of the most relevant recent works, and outline the main application scenarios. Finally, we discuss the main opportunities and challenges of the different approaches and point to the most promising directions.},
  eventtitle = {2020 {{IEEE Symposium Series}} on {{Computational Intelligence}} ({{SSCI}})},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/Zhao et al_2020_Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics.pdf}
}

@article{zhengTimeVaryingImpedanceControl2018,
  title = {Time-{{Varying Impedance Control}} of {{Port Hamiltonian System}} with a {{New Energy-Storing Tank}}},
  author = {Zheng, Min and Yuan, Tangqing and Huang, Tao},
  date = {2018},
  journaltitle = {Complexity},
  shortjournal = {Complexity},
  pages = {8134230},
  publisher = {{Wiley-Hindawi}},
  location = {{London}},
  issn = {1076-2787},
  doi = {10.1155/2018/8134230},
  url = {https://www.hindawi.com/journals/complexity/2018/8134230/},
  urldate = {2022-05-02},
  abstract = {In order to guarantee the passivity of a kind of conservative system, the port Hamiltonian framework combined with a new energy tank is proposed in this paper. A time-varying impedance controller is designed based on this new framework. The time-varying impedance control method is an extension of conventional impedance control and overcomes the singularity problem that existed in the traditional form of energy tank. The validity of the controller designed in this paper is shown by numerical examples. The simulation results show that the proposed controller can not only eliminate the singularity problem but can also improve the control performance.},
  langid = {english},
  keywords = {bilateral teleoperation,READ,STABILITY},
  annotation = {WOS:000450146100001},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/zheng_et_al_2018_time-varying_impedance_control_of_port_hamiltonian_system_with_a_new.pdf}
}

@inproceedings{zhiDiffeomorphicTransformsGeneralised2022,
  title = {Diffeomorphic {{Transforms}} for {{Generalised Imitation Learning}}},
  booktitle = {Proceedings of {{The}} 4th {{Annual Learning}} for {{Dynamics}} and {{Control Conference}}},
  author = {Zhi, Weiming and Lai, Tin and Ott, Lionel and Ramos, Fabio},
  date = {2022-05-11},
  pages = {508--519},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v168/zhi22a.html},
  urldate = {2023-01-18},
  abstract = {We address the generalised imitation learning problem of producing robot motions to imitate expert demonstrations, while adapting to novel environments. Past studies have often focused on methods that closely mimic demonstrations. However, to operate reliably in novel environments, robots should be able to adapt their learned motions accordingly. Motivated by this, we devise a framework capable of learning a time-invariant dynamical system to imitate demonstrations, and generalise to account for changes to the surroundings. To ensure the system is robust to perturbations, we need to maintain its stability. Our framework enforces stability in a principled manner: we start with a known stable system and use differentiable bijections (diffeomorphisms) to morph the system into the desired target system. We modularise robot motion and develop diffeomorphic transforms to encode individual actions. A composition of transforms produces generalised behaviour that complies with multiple requirements, such as mimicking demonstrations while avoiding obstacles. We evaluate our framework in both simulation and on a real-world 6-DOF JACO manipulator. Results show our framework is capable of producing a stable system that is collision-free and incorporates user-specified biases, while closely resembling demonstrations.},
  eventtitle = {Learning for {{Dynamics}} and {{Control Conference}}},
  langid = {english},
  keywords = {READ},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/zhi_et_al_2022_diffeomorphic_transforms_for_generalised_imitation_learning.pdf}
}

@inproceedings{zhouFullyConvolutionalGrasp2018,
  title = {Fully Convolutional Grasp Detection Network with Oriented Anchor Box},
  booktitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Zhou, Xinwen and Lan, Xuguang and Zhang, Hanbo and Tian, Zhiqiang and Zhang, Yang and Zheng, Narming},
  date = {2018},
  pages = {7223--7230},
  publisher = {{IEEE}},
  keywords = {#nosource,grasp pose detection,neural network,object grasping,robotic grasping,tno internship},
  annotation = {00006}
}

@article{zhouHybridControlStrategy2021,
  title = {A Hybrid Control Strategy for Grinding and Polishing Robot Based on Adaptive Impedance Control},
  author = {Zhou, Haibo and Ma, Shitai and Wang, Guilian and Deng, Yuxin and Liu, Zhenzhong},
  date = {2021-03-01},
  journaltitle = {Advances in Mechanical Engineering},
  shortjournal = {Advances in Mechanical Engineering},
  volume = {13},
  number = {3},
  pages = {16878140211004034},
  publisher = {{SAGE Publications}},
  issn = {1687-8132},
  doi = {10.1177/16878140211004034},
  url = {https://doi.org/10.1177/16878140211004034},
  urldate = {2022-08-11},
  abstract = {In order to realize the active and compliant motion of the robot, it is necessary to eliminate the impact caused by processing contact. A hybrid control strategy for grinding and polishing robot is proposed based on adaptive impedance control. Firstly, an electrically driven linear end effector is designed for the robot system. The macro and micro motions control model of the robot is established, by using impedance control method, which based on the contact model of the robot system and the environment. Secondly, the active compliance method is adopted to establish adaptive force control and position tracking control strategies under impact conditions. Finally, the algorithm is verified by Simulink simulation and experiment. The simulation results are as follows: The position tracking error does not exceed 0.009\,m, and the steady-state error of the force is less than 1\,N. The experimental results show that the motion curve coincides with the surface morphology of the workpiece, and the contact force is stable at 10\,±\,3\,N. The algorithm can realize more accurate position tracking and force tracking, and provide a reference for the grinding and polishing robot to realize surface processing.},
  langid = {english},
  keywords = {adaptive impedance control,force and position hybrid control,Grinding and polishing robot},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/zhou_et_al_2021_a_hybrid_control_strategy_for_grinding_and_polishing_robot_based_on_adaptive.pdf}
}

@inproceedings{zhuPassivityAnalysisPassivation2014,
  title = {Passivity Analysis and Passivation of Feedback Systems Using Passivity Indices},
  booktitle = {2014 {{American Control Conference}}},
  author = {Zhu, Feng and Xia, Meng and Antsaklis, Panos J.},
  date = {2014-06},
  pages = {1833--1838},
  issn = {2378-5861},
  doi = {10.1109/ACC.2014.6858850},
  abstract = {Passivity indices are used to measure the excess or shortage of passivity. While most of the work in the literature focuses on stability conditions for interconnected systems using passivity indices, here we focus on passivity and passivation of the feedback interconnection of two input feed-forward output-feedback (IF-OF) passive systems. The conditions are given to determine passivity indices in feedback interconnected systems. The results can be viewed as the extension of the well-known compositional property of passivity. We also consider the passivation problem which can be used to render a non-passive plant passive using a feedback interconnected passive controller. The passivity indices of the passivated system are also determined. The results derived do not require linearity of the systems as it is commonly assumed in the literature.},
  eventtitle = {2014 {{American Control Conference}}},
  keywords = {Closed loop systems,Interconnected systems,Linear systems,Negative feedback,Nonlinear systems,Output feedback,Passivation,STABILITY,Stability criteria},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/zhu_et_al_2014_passivity_analysis_and_passivation_of_feedback_systems_using_passivity_indices.pdf}
}

@book{ziebartModelingPurposefulAdaptive2010,
  title = {Modeling {{Purposeful Adaptive Behavior}} with the {{Principle}} of {{Maximum Causal Entropy}}},
  author = {Ziebart, Brian D. and Hebert, Martial},
  date = {2010},
  abstract = {contract no. EEEC-0540865. The views and conclusions contained in this document are those of the author and should not be interpreted as representing the official policies, either expressed or implied, of any sponsoring institution, the U.S. government or any other entity. Keywords: Machine learning, decision making, probabilistic modeling, maximum entropy, inverse optimal control, influence diagrams, informational revelation, feedback, causality, goal Predicting human behavior from a small amount of training examples is a challenging machine learning problem. In this thesis, we introduce the principle of maximum causal entropy, a general technique for applying information theory to decision-theoretic, game-theoretic, and control settings where relevant information is sequentially revealed over time. This approach guarantees decision-theoretic performance by matching purposeful measures of behavior (Abbeel \& Ng, 2004), and/or enforces game-theoretic rationality constraints (Aumann, 1974), while otherwise being as uncertain as possible, which minimizes worst-case predictive log-loss (Grünwald \& Dawid, 2003). We derive probabilistic models for decision, control, and multi-player game settings using this approach. We then develop corresponding algorithms for efficient inference that include relaxations},
  keywords = {#nosource,imitation learning,inverse RL learning,reinforcement learning}
}

@article{zinageNeuralKoopmanLyapunov2023,
  title = {Neural {{Koopman Lyapunov Control}}},
  author = {Zinage, Vrushabh and Bakolas, Efstathios},
  date = {2023-01-13},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2023.01.029},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231223000413},
  urldate = {2023-01-19},
  abstract = {Learning and synthesizing stabilizing controllers for unknown nonlinear control systems is a challenging problem for real-world and industrial applications. Koopman operator theory allows one to analyze nonlinear systems through the lens of linear systems and nonlinear control systems through the lens of bilinear control systems. The key idea of these methods lies in the transformation of the coordinates of the nonlinear system into the Koopman observables, which are coordinates that allow the representation of the original system (control system) as a higher dimensional linear (bilinear control) system. However, for nonlinear control systems, the bilinear control model obtained by applying Koopman operator based learning methods is not necessarily stabilizable. Simultaneous identification of stabilizable lifted bilinear control systems as well as the associated Koopman observables is still an open problem. In this paper, we propose a framework to construct these stabilizable bilinear models and identify its associated observables from data by simultaneously learning a bilinear Koopman embedding for the underlying unknown control affine nonlinear system as well as a Control Lyapunov Function (CLF) for the Koopman based bilinear model using a learner and falsifier. Our proposed approach thereby provides provable guarantees of asymptotic stability for the Koopman based representation of the unknown control affine nonlinear control system as a bilinear system. Numerical simulations are provided to validate the efficacy of our proposed class of stabilizing feedback controllers for unknown control-affine nonlinear systems.},
  langid = {english},
  keywords = {Control Lyapunov Functions,Koopman operator theory,Neural networks,READ,STABILITY},
  file = {/home/ricks/Insync/OneDrive/Documents/Zotero/zinage_bakolas_2023_neural_koopman_lyapunov_control.pdf}
}
