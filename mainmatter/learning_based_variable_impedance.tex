\chapter{Stability methods in learning based variable impedance control}
\label{chapter:learning_based_variable_impedance}

% TODO: Seperation not correct see Abu-dakka

The previous chapter discussed methods for ensuring stable control in classical variable impedance controllers. Although impressive results were achieved using these classical controllers \cite{songTutorialSurveyComparison2019}, these controllers' variable impedance profiles are manually programmed for a given task. This considerably reduces the usability of these controllers since every time a robot is used for a new task, multiple hours of labour by skilled workers are required for re-programming the controller. In recent years, numerous authors have instead switched to learning-based variable impedance controllers \cite{abu-dakkaVariableImpedanceControl2020}. These controllers allow learning more complex behaviours without requiring the researcher to program this behaviour explicitly. A recent review of these learning-based variable impedance controllers was done by \cite{abu-dakkaVariableImpedanceControl2020}. In this review, the learning-based VIC methods were classified into two main groups: Variable Impedance Learning (VIL), in which the impedance profiles are learned from human demonstrations (through imitation learning) to be later executed with an existing impedance controller, and Variable Impedance Learning Control (VILC) in which the impedance control law is directly learned (through imitation learning, iterative learning or reinforcement learning). Where \cite{abu-dakkaVariableImpedanceControl2020} focuses on the learning-based VIC methods used in the current literature, the following sections concentrate on the stability and passivity techniques employed to ensure stability while using these methods.

\section{Variable impedance learning}

\subsection{Stability of the learned trajectories}

As explained above, in VIL methods, the desired trajectory and impedance profiles for a given task are learned from human demonstrations using a supervised learning procedure called Imitation Learning (IM), also called Learning from Demonstrations (LfD). Two steps generally make up this approach: A step in which kinematic (and dynamic) data is collected from sensors (i.e. data-collection step), followed by a step in which a model is fitted to the collected data using regression (i.e. data-fitting step). Depending on the task, any regression model (e.g. linear models, splines, gaussian mixture models, neural networks, gaussian processes) and accompanying regression technique can be used in this data-fitting step \cite{kroemerReviewRobotLearning2021,husseinImitationLearningSurvey2017}. Traditionally, LfD research has mainly focused on reproducing the demonstrations' kinematics (i.e. trajectories) \cite{siReviewManipulationSkill2021}. At first glance, this trajectory reproduction problem seems to be a simple regression task that can be solved by minimising the squared distance between the sampled trajectories and the regression model. A low-level controller like a PID controller can then be used to reproduce the recorded trajectories. However, in practice, such a naive regression approach has proven to be insufficient because it results in an over-fitted model that does not generalise outside of the demonstrated trajectories \cite{sindhwaniLearningContractingVector2018}. Therefore, this (nonlinear) model is only locally stable and does not converge to the desired trajectories if situations are encountered that were not present in the training data (e.g. different initial states, temporal disturbances and spatial disturbances). One possible solution would be to use linear regression models in which stability outside the demonstrated regions can be proven easily. Such models, however, cannot accurately reproduce desired trajectories that are nonlinear and non-smooth. Therefore, several methods have been proposed in the literature for learning (nonlinear) trajectories while guaranteeing stability outside the demonstrated region.

In LfD research, motions are often modelled as a Dynamic System (DS) \cite{khansari-zadehLearningStableNonlinear2011} to improve generalizability. Compared to classical approaches, which use time-indexed trajectories, in a DS, the trajectories are formulated as a differential equation. Because of this, the DS captures both the trajectories and the essential dynamics that underlie a given task, making it better able to adapt to environmental changes like temporal and spatial disturbances. The most widely used DS-based approach for learning stable nonlinear trajectories is the so-called "Dynamic Movement Primitives" (DMPs) \cite{ijspeertDynamicalMovementPrimitives2013,saverianoDynamicMovementPrimitives2021,wangLearningDemonstrationUsing2021,sidiropoulosReversibleDynamicMovement2021,ginesiOvercomingDrawbacksDynamic2021,rozanecNeuralDynamicMovement2022,liProDMPsUnifiedPerspective2022}. In DMPs, a globally stable linear DS is coupled with a nonlinear forcing term through a phase variable to create an autonomous, weakly nonlinear DS. This DS can learn complex high-dimensional motions from single \cite{ijspeertDynamicalMovementPrimitives2013,prakashDynamicTrajectoryGeneration2020} or multiple demonstrations \cite{matsubaraLearningParametricDynamic2011,pervezLearningTaskparameterizedDynamic2018} while guaranteeing global stability. This global stability is achieved by exponentially or linearly decaying the phase variable during the task, thereby decreasing the effect of the possibly unstable nonlinear forcing term. As these DMPs are time- and scale-invariant, the learnt motions' velocity and amplitude can be scaled without losing stability, making them well-suited for reacting to external perturbations in real-time. Unfortunately, although the phase variable ensures the stability of the motions, it introduces an implicit time dependency in the DMP formulation. Due to this time dependency, the motion of the DMP is very dependent on the phase variable, which is, again, task-dependent. Consequently, DMPs have poor generalisation capabilities outside the demonstrations and are not robust against temporal disturbances \cite{neumannLearningRobotMotions2015}.

Other authors use a constraint optimisation approach to overcome the abovementioned limitations and encode the demonstrated motions in a state-dependent nonlinear DS while enforcing global stability through Lyapunov constraints. Since these DSs are time-independent, the learned policies are now robust against temporal disturbances. This was first done by Khansari-Zadeh et al. \cite{khansari-zadehLearningStableNonlinear2011}, who introduced a stable estimator of a dynamical system (SEDS). By limiting the parameters of a Gaussian mixture regression model (GMM) using a Quadratic Lyapunov constraint, SEDS can learn accurate motions while guaranteeing global stability. Similar Lyapunov constraints were used in literature with other regression models to ensure the global stability of the learned motions \cite{lemmeNeurallyImprintedStable2013,huNeuralLearningStable2015,umlauftLearningStableStochastic2017,umlauftLearningStableGaussian2017,medinaLearningStableTask2017,duanFastStableLearning2019,xuRobotTrajectoryTracking2019,umlauftLearningStochasticallyStable2020,ledererGaussianProcessBasedRealTime2021,xuLearningBasedKinematicControl2022,salehiLearningDiscreteTimeUncertain2022,davoodiRuleBasedSafeProbabilistic2022}. Unfortunately, as these methods' stability criteria are derived based on a simple quadratic Lyapunov function, they can only model trajectories in which the distance to the target decreases monotonically in time, leading to poor accuracy if demonstrated trajectories are not contractive. As a result, several authors have used parametric and non-parametric Lyapunov candidates to learn less conservative Lyapunov constraints directly from data to improve this so-called accuracy-stability dilemma \cite{mohammadkhansari-zadehLearningControlLyapunov2014,neumannNeuralLearningStable2013,lemmeNeuralLearningVector2014,umlauftLearningStableGaussian2017,umlauftUncertaintybasedControlLyapunov2018,duttaLearningStableMovement2018,umlauftUncertaintybasedHumanMotion2019,duttaSkillLearningHuman2021,ravanbakhshLearningControlLyapunov2019,ravanbakhshFormalPolicyLearning2019,umlauftLearningStochasticallyStable2020,xiaoLearningStableNonparametric2020,tesfazgiInverseReinforcementLearning2021,coulombeGeneratingStableCollisionFree2022,jinLearningNeuralshapedQuadratic2023}. These less conservative Lyapunov constraints can then be enforced \textbf{offline} during learning or \textbf{online} through a stabilising control command to ensure the stability of the reproduced trajectories. If applied online \cite{mohammadkhansari-zadehLearningControlLyapunov2014,umlauftLearningStableGaussian2017,umlauftUncertaintybasedControlLyapunov2018,duttaLearningStableMovement2018,umlauftUncertaintybasedHumanMotion2019,ravanbakhshFormalPolicyLearning2019,ravanbakhshLearningControlLyapunov2019,xiaoLearningStableNonparametric2020,duttaSkillLearningHuman2021,umlauftLearningStochasticallyStable2020}, any regression method can be used. However, no guarantees can be given about the accuracy of reproduced trajectories since the control command potentially interferes with the dynamic system. If applied offline \cite{neumannNeuralLearningStable2013,lemmeNeuralLearningVector2014,tesfazgiInverseReinforcementLearning2021,coulombeGeneratingStableCollisionFree2022,jinLearningNeuralshapedQuadratic2023}, on the other hand, the shape and accuracy of the demonstrated trajectories are known beforehand, but the implementation is specific to the used regression model. This significantly restricts the applicability of these offline methods since it is unknown beforehand which regression technique would be more effective for a given task.

Although the learned Lyapunov constraints above yield more accurate motions than their fixed counterparts and DMPs, finding them is more computationally intensive and time-consuming. Furthermore, their reproduction accuracy still depends on the Lyapunov candidate function used during learning. While less conservative Lyapunov candidate functions could be used to improve the accuracy of the reproduced trajectories, finding these Lyapunov candidate functions is not easy, time-consuming, and quickly becomes computationally intractable \cite{ravanbakhshLearningControlLyapunov2019,havensImitationLearningLinear2021,tesfazgiInverseReinforcementLearning2021}. Additionally, except \cite{jinLearningNeuralshapedQuadratic2023}, all of the offline methods of the previous paragraph currently lack constructive mathematical guarantees causing the stability of these methods to be restricted to finite regions in the workspace. Therefore, these methods only apply to applications where the task region is bounded and known a priori. Because of this, recent papers have taken a diffeomorphic approach and transformed the demonstrations into a latent space in which the demonstrations are consistent with a (simpler) predefined or learned Lyapunov function \cite{neumannLearningRobotMotions2015,perrinFastDiffeomorphicMatching2016,jinLearningAccurateStable2019,ranaEuclideanizingFlowsDiffeomorphic2020,gaoLearningDynamicalSystem2021,urainImitationFlowLearningDeep2020,takeishiLearningDynamicsModels2021,ficheraLinearizationIdentificationMultipleAttractor2022,guptaLearningHighDimensional2022,zhiDiffeomorphicTransformsGeneralised2022,zhangLearningAccurateStable2022,saverianoLearningStableRobotic2022,urainLearningStableVector2022,wangLearningDeepRobotic2022,zhangLearningRiemannianStable2022}. By enforcing the Lyapunov constraints in this new latent space, the global stability of the learned motions can be guaranteed offline. Since this latent space, which can be of the same or a higher dimension, is topological equivalent to the original space, the stability achieved in this space is retained when the learned demonstrations are eventually transformed back to the original space \cite{leeIntroductionTopologicalManifolds2011,leeIntroductionSmoothManifolds2012,leeIntroductionRiemannianManifolds2018}. Although a transformation that preserves stability has to be found, finding this transformation is relatively easier than finding a suitable Lyapunov function in the untransformed space. As a result, compared to previous methods, these diffeomorphic methods can handle more complex high-dimensional motions with less computational effort. Research has learned that diffeomorphic methods exhibit better reproduction accuracy and generalizability since they can learn a more general group of demonstrations than the abovementioned methods. However, they introduce more tunable parameters, can currently only be used offline and are specific to the used regression technique. Moreover, the expressiveness of the function approximator used for the diffeomorphic transform determines the accuracy and generalizability of these methods.

The methods discussed above were able to achieve high accuracy while guaranteeing the stability of the equilibrium points. They, however, give no guarantee about the convergence of the trajectories. As a result, although learned policies will always reach the target, regardless of the region in the state space where they are perturbed, they will no longer follow the reference trajectories. These techniques are, therefore, inadequate for tasks where trajectory tracking is crucial. Because of this, several authors have used contraction theory to derive stability constraints that guarantee a "stronger" type of stability called incremental stability. Instead of looking at the stability of the equilibrium points, incremental stability studies the convergence between any two trajectories of a given system \cite{lohmillerContractionAnalysisNonlinear1998,tsukamotoContractionTheoryNonlinear2021}. These (incremental) stability constraints have been used \textbf{offline} \cite{ravichandarLearningContractingNonlinear2016,ravichandarLearningStableNonlinear2018,khadirTeleoperatorImitationContinuoustime2019,sindhwaniLearningContractingVector2018,ravichandarLearningPositionOrientation2019} and \textbf{online} \cite{blocherLearningStableDynamical2017} with different regression models to improve the accuracy of the reproduced trajectories. Contraction-based methods have shown better or similar results to the diffeomorphic methods above. However, since their contraction-based constraints lead to exponential stable DSs, they provide greater robustness against temporal and spatial perturbations \cite{ravichandarLearningPartiallyContracting2017}. Nonetheless, the robustness and accuracy of these methods are highly dependent on the contraction metrics used to derive these constraints. Furthermore, only a small class of contraction metrics (i.e. positive definite symmetric matrices and a sum of squares polynomials) are currently used in literature and deriving them is non-trivial and dependent on the used regression model. Additionally, due to their inherent contractive nature, these approaches, however, suffer from the risk of over-corrections, making them unsuited for tasks containing global or locally divergent trajectories \cite{figueroafernandezPhysicallyconsistentBayesianNonparametric2018,figueroaLocallyActiveGlobally2022}. Several papers have recently combined contraction theory and diffeomorphisms with global linearization methods inspired by Koopman theory to solve these issues \cite{bevandaKoopmanOperatorDynamical2021}. These Koopman-based approaches work by mapping the system states of the nonlinear system to high (possibly infinite) dimensional spaces of observables
in which the dynamics are linear. A globally linear representation of the original nonlinear system can be achieved by learning these Koopman observables and the accompanying mapping from data. Contraction theory can then be used on this representation to guarantee the global stability of the original nonlinear system. These Koopman-based methods, like the diffeomorphic methods above, lead to more challenging system identification but make it easier to guarantee global stability. Even though these methods have shown good accuracy and robustness in simple learning from demonstration datasets \cite{fanLearningStableKoopman2021,bevandaDiffeomorphicallyLearningStable2022}, they are relatively new and yet to be compared to the methods above on more complex LFD tasks.

Lastly, two other noteworthy approaches for guaranteeing stability in kinematic LfD research are found in the current literature. The first one, a paper by Savariano et al. \cite{saverianoEnergybasedApproachEnsure2020}, used the energy tank of the previous chapter to ensure the DS's stability \textbf{online}. By storing the energy of stable (conservative) actions in this tank, potentially unstable motions can be performed as long as there is enough energy in the tank, thereby preserving the system's overall stability. Their method can achieve comparable accuracy as the aforementioned constrained-based methods, can be used with any regression technique, is less computationally intensive and requires fewer parameters to be tuned. However, it has an implicit time dependency and, like the energy tank-based methods in the previous chapter, requires a tank initialisation procedure which can significantly affect the reproduction accuracy. The second one, a paper of Figueroa et al. \cite{figueroafernandezPhysicallyconsistentBayesianNonparametric2018}, used a non-parametric GMM to learn the movement policy and an elliptical Lyapunov function jointly. They introduce a novel similarity metric which leverages locality and directionally of the demonstrations and use this metric as a Bayesian prior in the constraint optimisation. By doing this, they end up with a globally asymptotically stable DS that also converges locally to the demonstrated trajectories. Their algorithm outperforms several methods described earlier in accuracy and generalizability without relying on diffeomorphisms or contraction theory.

\subsection{Stability of the learned impedance}

% TODO: Check Francesse papers.

% QUESTION: Are the papers of Jin et al. 2022, Duo et al. 2022, Duan et al. 2019 and Arduengo et al. 2020 also passive and thus IO stable? I think since Kronander et al. 2016 is used and they give no IO or passivity proof they are not. It could however be that my understanding of the problem is wrong.

The previous section depicted several solutions for learning stable kinematic motion from demonstrations. Since robotic manipulators seldom only perform movements in free space, researchers have augmented these methods for interaction tasks. Several papers, for example, have extended the original DMP model of the previous section with an additional forcing or coupling term that encodes the desired impedance behaviour \cite{saverianoDynamicMovementPrimitives2021,dongDMPbasedOnlineAdaptive2021,hanModifiedDynamicMovement2022,hanModifiedDynamicMovement2022,liaoDynamicSkillLearning2022,yangDMPsbasedFrameworkRobot2018,yuHumanRobotVariableImpedance2022,zengRobotLearningHuman2018,zhangLearningGeneralizingVariable2022}. Since both the kinematic and impedance terms are linked to the same exponentially decaying phase variable, the stability of the reproduced motions and impedance profiles is guaranteed. Other authors use the Lyapunov constraint-based approach from the previous section to ensure the stability of their learned variable impedance controllers. Some directly encode the impedance behaviour on a Riemannian manifold and use a diffeomorphic approach \cite{saverianoLearningStableRobotic2022,wangLearningDeepRobotic2022} while others use
stability constraints similar to those of Kronander et al. \cite{kronanderStabilityConsiderationsVariable2016} \cite{jinOptimalVariableImpedance2022,douRobotSkillLearning2022,duanSequentialLearningUnification2019,arduengoGaussianProcessbasedRobotLearning2020}. 

Although the stability of these impedance-based LfD methods in free motion is guaranteed, it does not imply passivity, and thus, the controlled system can become unstable when in contact. Therefore, several recent papers have focused on guaranteeing the passivity of these methods. In Khansari-Zadeh et al. \cite{khansari-zadehModelingRobotDiscrete2014}, both the motion and impedance control were encoded into a time-invariant DS, which was expressed as a nonlinear combination of a set of linear spring-damper systems. Stability constraints were derived by performing a passivity analysis on this DS to ensure free space stability and passivity in-contact \cite{khansariModelingRobotDiscrete2014}. This idea was further developed in \cite{khansari-zadehLearningPotentialFunctions2017}, where a non-parametric potential function and dissipative field were learned from demonstrations to encode the desired motion, stiffness and damping behaviour. The passivity and, thus, stability of this potential field-based variable impedance controller were demonstrated using a passivity analysis. Compared to \cite{khansariModelingRobotDiscrete2014}, this new method can model a more extensive range of motions, simplifies motion and impedance learning and allows explicit stiffness and damping adjustments without modifying the desired trajectory. Instead, others have modified the energy-tank method given in the previous chapter to be used with LfD-based impedance controllers \cite{kronanderPassiveInteractionControl2016,shahriariAdaptingContactsEnergy2017}. Kronander et al. \cite{kronanderPassiveInteractionControl2016} design a passive DS-based controller that can be used to learn motion and impedance control while ensuring stable interaction with a passive environment. The passivity and, thus, global stability of this controller is ensured through an energy-tank-like mechanism. This method was later extended by \cite{figueroaLocallyActiveGlobally2022,chenClosedLoopVariableStiffness2021} to encode locally contractive stiffness-like behaviours. Shahriari et al. \cite{shahriariAdaptingContactsEnergy2017}, on the other hand, augment the DMP approach from the previous section with a passivity observer, which modifies the phase variable when non-passive behaviour is detected. Ferraguti's energy tank is also added since this passivity observer merely reduces non-passive actions but does not guarantee passivity. Multiple authors have used these modified energy tank-based methods to ensure their learned controllers stay passive and thus stable during interaction tasks \cite{kastritsiPHRIFrameworkModifying2018,kastritsiProgressiveAutomationDMP2018,dimeasProgressiveAutomationPeriodic2020,papageorgiouKinestheticGuidanceUtilizing2020,krambergerPassivityBasedIterative2018,amanhoudDynamicalSystemApproach2019,enayatiVariableImpedanceForceControl2020,michelBilateralTeleoperationAdaptive2021,wuFrameworkAutonomousImpedance2021,wuLearningDemonstrationInteractive2022,zhaoHybridLearningOptimization2022}.

\section{Variable impedance learning control}

% TODO: See Jin  2023. Capturing stiffness ground truth hard to obtain. Therefore uesrs tried to use RL to learn the impedance behavoir. This however can be sample complex and is very dependent on the sim-to-real gap. Therefore, several authors combine LfD and RL. However only a handfull of papers have looked at stability or passivity of these methods.


% TODO: Check Khander review.
% TODO: Check chen 2021 and Friedrich 2021.
% TODO: Check Abu-Dakka review.
% TODO: Check abdul Khader 2021.
% TODO: Kim could be constant impedance?
% TODO: GAILS? INverse RL?
% TODO: Check Jin for downsides RL vs GGM. JIN 2022 good starting point for this chapter!.
% TODO: Liang 2021: Online RL with constraint on inertia.
% TODO: Check Franzese et al. 2021 RL.
% TODO: Zhang et al. 2022: Has sources about RL based impedance check them.

% TODO: See Jin et al. 2022. Downside RL is that there is a simulation to real gap. Several authors therefore combine LfD and RL.

\textbf{\textcolor{red}{Ik heb het stuk over RL er in gelaten zodat je kunt zien wat ik hier al aan gedaan had.}}

Stability guarantees in RL based methods are quite rare \cite{abdulkhaderDataDrivenMethodsContactRich2021}


Recently authors have also investigated the passivity problem in RL-based variable impedance methods, which can learn complex tasks without knowing anything about the system and environment. In these methods, the closed-loop system's passivity is ensured by guaranteeing that the actions coming for the policy adhere to certain passivity constraints. For example, Kim et al. 2008 \cite{kimLearningRobotStiffness2008} ensure that the policy's task-stiffness matrix is always symmetric and positive definite. Doing this guarantees the manipulator's stability when interacting with a passive environment. Liang et al. 2021 (Liang et al., 2021) keep the closed-loop system stable by deriving three Lyapunov stability constraints on the inertia matrix. As a result, the damping and stiffness matrices can vary.\cite{reyLearningMotionsDemonstrations2018} and \cite{khaderStabilityGuaranteedReinforcementLearning2020} take a similar approach by ensuring that the policy sample distribution can only generate passive parameter samples. Later \cite{khaderLearningDeepEnergy2021} ensure the passivity of their RL algorithm by using a deep neural policy in which the structure guarantees stability. Since both \cite{khaderStabilityGuaranteedReinforcementLearning2020,reyLearningMotionsDemonstrations2018} and \cite{khaderLearningDeepEnergy2021} can only use entire trajectories, they are not very sample efficient. \cite{khaderLearningStableNormalizingFlow2021} proposed a new method to utilize all state-action samples to solve this.
