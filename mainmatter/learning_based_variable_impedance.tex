% [ ]: Quickly check review papers.
%     [ ]: Sharifi et al. 2021.
%     [ ]: Al-Shuka 2018.
%     [ ]: Abu-Dakka 2020.
%     [ ]: Song 2019.
%     [ ]: Ravinchandar 2020 review.
%     [ ]: Figueroa et al. 2022.
% [ ]: Quickly check zotero other LfD papers.

\chapter{Stability methods in learning based variable impedance control}
\label{chapter:learning_based_variable_impedance}

The previous chapter discussed methods for ensuring stable control in classical variable impedance controllers. Although impressive results were achieved using these classical controllers \cite{songTutorialSurveyComparison2019}, the variable impedance profiles used by these controllers are manually programmed for a given task. This considerably reduces the usability of these controllers since every time a robot is used for a new task, multiple hours of labour by skilled workers are required for re-programming the controller. In recent years, numerous authors have instead switched to learning-based variable impedance controllers \cite{abu-dakkaVariableImpedanceControl2020}. These controllers allow for learning more complex behaviours without the need for the researcher to program this behaviour explicitly. A recent review of these learning-based variable impedance controllers was done by \cite{abu-dakkaVariableImpedanceControl2020}. In this review, the learning-based VIC methods were classified into two main groups: Variable Impedance Learning (VIL), in which the impedance profiles are learned from human demonstrations (through imitation learning) to be later executed with an existing impedance controller, and Variable Impedance Learning Control (VILC) in which the impedance control law is directly learned (through imitation learning, iterative learning or reinforcement learning). Where \cite{abu-dakkaVariableImpedanceControl2020} focuses on the learning-based VIC methods used in the current literature, the following sections concentrate on the stability and passivity techniques employed to ensure stability while using these methods.

\section{Variable impedance learning}

\subsection{Stability of the learned trajectories}

As explained above, in VIL methods, the desired trajectory and impedance profiles for a given task are learned from human demonstrations using a supervised learning procedure called Imitation Learning (IM), also called Learning from Demonstrations (LfD). Two steps generally make up this approach: A step in which kinematic (and dynamic) data is collected from sensors (i.e. data-collection step), followed by a step in which a model is fitted to the collected data using regression (i.e. data-fitting step). Depending on the task, any regression model (e.g. linear models, splines, gaussian mixture models, neural networks, gaussian processes) and accompanying regression technique can be used in this data-fitting step \cite{kroemerReviewRobotLearning2021,husseinImitationLearningSurvey2017}. Traditionally, LfD research has mainly focused on reproducing the demonstrations' kinematics (i.e. trajectories) \cite{siReviewManipulationSkill2021}. At first glance, this trajectory reproduction problem seems to be a simple regression task that can be solved by minimising the squared distance between the sampled trajectories and the regression model. A low-level controller like a PID controller can then be used to reproduce the recorded trajectories. However, in practice, such a naive regression approach has proven to be insufficient because it results in an over-fitted model that does not generalise outside of the demonstrated trajectories \cite{sindhwaniLearningContractingVector2018}. Therefore, this (non-linear) model is only locally stable and does not converge to the desired trajectories if situations are encountered that were not present in the training data (e.g. different initial states, temporal disturbances and spatial disturbances). One possible solution would be to use linear regression models in which stability outside the demonstrated regions can be proven easily. Such models, however, cannot accurately reproduce desired trajectories that are non-linear and non-smooth. Therefore, several methods have been proposed in the literature for learning (non-linear) trajectories while guaranteeing stability outside the demonstrated region.

In LfD research, motions are often modelled as a Dynamic System (DS) \cite{khansari-zadehLearningStableNonlinear2011} to improve generalizability. Compared to classical approaches, which use time-indexed trajectories, in a DS, the trajectories are formulated as a differential equation. Because of this, the DS captures both the trajectories and the essential dynamics that underlie a given task, making it better able to adapt to environmental changes like temporal and spatial disturbances. The most widely used DS-based approach for learning stable non-linear trajectories is the so-called "Dynamic Movement Primitives" (DMPs) \cite{ijspeertDynamicalMovementPrimitives2013,saverianoDynamicMovementPrimitives2021,wangLearningDemonstrationUsing2021,sidiropoulosReversibleDynamicMovement2021,ginesiOvercomingDrawbacksDynamic2021,rozanecNeuralDynamicMovement2022,liProDMPsUnifiedPerspective2022}. In DMPs, a globally stable linear DS is coupled with a non-linear forcing term through a phase variable to create an autonomous, weakly non-linear DS. This DS can learn complex high-dimensional motions from single \cite{ijspeertDynamicalMovementPrimitives2013,prakashDynamicTrajectoryGeneration2020} or multiple demonstrations \cite{matsubaraLearningParametricDynamic2011,pervezLearningTaskparameterizedDynamic2018} while guaranteeing global stability. This global stability is achieved by exponentially or linearly decaying the phase variable during the task, thereby decreasing the effect of the possibly unstable non-linear forcing term. As these DMPs are time- and scale-invariant, the learnt motions' velocity and amplitude can be scaled without losing stability, making them well-suited for reacting to external perturbations in real time. Unfortunately, although the phase variable ensures the stability of the motions, it introduces an implicit time dependency in the DMP formulation. Due to this time dependency, the motion of the DMP is very dependent on the phase variable, which is, again, task-dependent. Consequently, DMPs have poor generalisation capabilities outside the demonstrations and are not robust against temporal disturbances \cite{neumannLearningRobotMotions2015}.

Other authors use a constraint optimisation approach to overcome the abovementioned limitations and encode the demonstrated motions in a state-dependent non-linear DS while enforcing global stability through Lyapunov constraints. Since these DSs are time-independent, the learned policies are now robust against temporal disturbances. This was first done by Khansari et al. \cite{khansari-zadehLearningStableNonlinear2011}, who introduced a stable estimator of a dynamical system (SEDS). By limiting the parameters of a gaussian mixture regression model (GMM) using a Quadratic Lyapunov constraint, SEDS can learn accurate motions while guaranteeing global stability. Similar Lyapunov constraints were used in literature with other regression models to ensure the global stability of the learned motions \cite{lemmeNeurallyImprintedStable2013,huNeuralLearningStable2015,umlauftLearningStableStochastic2017,umlauftLearningStableGaussian2017,medinaLearningStableTask2017,duanFastStableLearning2019,xuRobotTrajectoryTracking2019,umlauftLearningStochasticallyStable2020,ledererGaussianProcessBasedRealTime2021,xuLearningBasedKinematicControl2022,salehiLearningDiscreteTimeUncertain2022,davoodiRuleBasedSafeProbabilistic2022}. Unfortunately, as these methods' stability criteria are derived based on a simple quadratic Lyapunov function, they can only model trajectories in which the distance to the target decreases monotonically in time, leading to poor accuracy if demonstrated trajectories are not contractive. As a result, several authors have used parametric and non-parametric Lyapunov candidates to learn less conservative Lyapunov constraints directly from data to improve this so-called accuracy-stability dilemma \cite{khansari-zadehLearningControlLyapunov2014,neumannNeuralLearningStable2013,lemmeNeuralLearningVector2014,umlauftLearningStableGaussian2017,umlauftUncertaintybasedControlLyapunov2018,duttaLearningStableMovement2018,umlauftUncertaintybasedHumanMotion2019,duttaSkillLearningHuman2021,ravanbakhshLearningControlLyapunov2019,ravanbakhshFormalPolicyLearning2019,umlauftLearningStochasticallyStable2020,xiaoLearningStableNonparametric2020,tesfazgiInverseReinforcementLearning2021,coulombeGeneratingStableCollisionFree2022}.

% QUESTION: `This significantly restricts the ...` added. Was `If applied offline \cite{neumannNeuralLearningStable2013,lemmeNeuralLearningVector2014,tesfazgiInverseReinforcementLearning2021,coulombeGeneratingStableCollisionFree2022}, on the other hand, the implementation is specific to the used regression model, but the shape and accuracy of the demonstrated trajectories are known beforehand.`

These less conservative Lyapunov constraints can then be enforced \textbf{offline} during learning or \textbf{online} through a stabilising control command to ensure the stability of the reproduced trajectories. If applied online \cite{khansari-zadehLearningControlLyapunov2014,umlauftLearningStableGaussian2017,umlauftUncertaintybasedControlLyapunov2018,duttaLearningStableMovement2018,umlauftUncertaintybasedHumanMotion2019,ravanbakhshFormalPolicyLearning2019,ravanbakhshLearningControlLyapunov2019,xiaoLearningStableNonparametric2020,duttaSkillLearningHuman2021,umlauftLearningStochasticallyStable2020}, any regression method can be used. However, no guarantees can be given about the accuracy of reproduced trajectories since the control command potentially interferes with the dynamic system. If applied offline \cite{neumannNeuralLearningStable2013,lemmeNeuralLearningVector2014,tesfazgiInverseReinforcementLearning2021,coulombeGeneratingStableCollisionFree2022}, on the other hand, the shape and accuracy of the demonstrated trajectories are known beforehand, but the implementation is specific to the used regression model. This significantly restricts the applicability of these offline methods since it is unknown beforehand which regression technique would be more effective for a given task.

% NOTE: -- TOT HIER NAGEKEKEN -- Alleen laatste zin toegevoegd.
% Question: Is het duidelijk dat diffeomorphic methods het probleem oplossen dat de offline methods niet kunnen generaliseren naar andere regressie technieken? Of moet dit nog duidelijker worden gemaakt?

Although the learned Lyapunov constraints above yield more accurate motions than their fixed counterparts and DMPs, finding them is more computationally intensive and time-consuming. Furthermore, their reproduction accuracy still depends on the Lyapunov candidate function used during learning. While less conservative Lyapunov candidate functions could be used to improve the accuracy of the reproduced trajectories, finding these Lyapunov candidate functions is not easy, time-consuming, and quickly becomes computationally intractable \cite{ravanbakhshLearningControlLyapunov2019,havensImitationLearningLinear2021,tesfazgiInverseReinforcementLearning2021}. Additionally, the offline methods of the previous paragraph currently lack constructive mathematical guarantees causing the stability of these methods to be restricted to finite regions in the workspace. These methods are, therefore, only applicable to applications where the task region is bounded and known a priori. Because of this, recent papers have taken a diffeomorphic approach and transformed the demonstrations into a latent space in which the demonstrations are consistent with a (simpler) predefined or learned Lyapunov function \cite{neumannLearningRobotMotions2015,perrinFastDiffeomorphicMatching2016,jinLearningAccurateStable2019,ranaEuclideanizingFlowsDiffeomorphic2020,gaoLearningDynamicalSystem2021,urainImitationFlowLearningDeep2020,takeishiLearningDynamicsModels2021,ficheraLinearizationIdentificationMultipleAttractor2022,guptaLearningHighDimensional2022,zhiDiffeomorphicTransformsGeneralised2022,zhangLearningAccurateStable2022,saverianoLearningStableRobotic2022,urainLearningStableVector2022,wangLearningDeepRobotic2022,zhangLearningRiemannianStable2022}. By enforcing the Lyapunov constraints in this new latent space, the global stability of the learned motions can be guaranteed offline. Since this latent space, which can be of the same or a higher dimension, is topological equivalent to the original space, the stability achieved in this space is retained when the learned demonstrations are eventually transformed back to the original space \cite{leeIntroductionTopologicalManifolds2011,leeIntroductionSmoothManifolds2012,leeIntroductionRiemannianManifolds2018}. Although a transformation that preserves stability has to be found, finding this transformation is relatively easier than finding a suitable Lyapunov function in the untransformed space. As a result, compared to previous methods, these diffeomorphic methods can handle more complex high-dimensional motions with less computational effort. 

% QUESTION: is the generalizability comment needed or is this already in the word?
% QUESTION: I first had `They also have been shown to exhibit better reproduction accuracy and generalizability since they can learn a more general group of demonstrations than the methods mentioned above` is the regression model thing needed?

They have been used with several regression models and exhibit better reproduction accuracy and generalizability since they can learn a more general group of demonstrations than the methods mentioned above. However, they introduce more tunable parameters, can currently only be used offline and are specific to the used regression technique. Moreover, the expressiveness of the function approximator used for the diffeomorphic transform determines the accuracy and generalizability of these methods.

The methods above were able to achieve high accuracy while guaranteeing the stability of the equilibrium points. They, however, give no guarantee about the convergence of the trajectories. As a result, although learned policies will always reach the target, regardless of the region in the state space where they are perturbed, they will no longer follow the reference trajectories. These techniques are, therefore, inadequate for tasks where trajectory tracking is crucial. Because of this, several authors have used contraction theory to derive stability constraints that guarantee a "stronger" type of stability called incremental stability. Instead of looking at the stability of the equilibrium points, incremental stability studies the convergence between any two trajectories of a given system \cite{lohmillerContractionAnalysisNonlinear1998,tsukamotoContractionTheoryNonlinear2021}. These (incremental) stability constraints have been used \textbf{offline} \cite{ravichandarLearningContractingNonlinear2016,ravichandarLearningStableNonlinear2018,khadirTeleoperatorImitationContinuoustime2019,sindhwaniLearningContractingVector2018,ravichandarLearningPositionOrientation2019} and \textbf{online} \cite{blocherLearningStableDynamical2017} with several different regression models to improve the accuracy of the reproduced trajectories. Contraction-based methods have shown better or similar results to the diffeomorphic methods above. However, since their contraction-based constraints lead to exponential stable DSs, they are more robust against temporal and spatial perturbations \cite{ravichandarLearningPartiallyContracting2017}. Nonetheless, the robustness and accuracy of these methods are highly dependent on the contraction metrics used to derive these constraints. Furthermore, only a small class of contraction metrics (i.e. positive definite symmetric matrices and a sum of squares polynomials) are currently used in literature and deriving them is non-trivial and dependent on the used regression model. Additionally, due to their inherent contractive nature, these approaches, however, suffer from the risk of over-corrections, making them unsuited for tasks containing global or locally divergent trajectories \cite{figueroafernandezPhysicallyconsistentBayesianNonparametric2018,figueroaLocallyActiveGlobally2022}. 

% QUESTION: This is a totally new part is it clear?

Several papers have recently combined contraction theory and diffeomorphisms with global linearization methods inspired by Koopman theory to solve these issues \cite{bevandaKoopmanOperatorDynamical2021}. These Koopman-based approaches work by mapping the system states of the nonlinear system to high (possibly infinite) dimensional spaces of observables
in which the dynamics are linear. By learning these Koopman observables and the accompanying mapping from data, a globally linear representation of the original nonlinear system can be achieved. Contraction theory can then be used on this representation to guarantee the global stability of the original nonlinear system. These Koopman-based methods, like the diffeomorphic methods above, lead to more challenging system identification but make it easier to guarantee global stability. Even though these methods have shown good accuracy and robustness in simple learning from demonstration datasets \cite{fanLearningStableKoopman2021,bevandaDiffeomorphicallyLearningStable2022}, they are relatively new and yet to be compared to the methods above on more complex LFD tasks.

% QUESTION: Two times stability needed first part?

Lastly, two other noteworthy approaches for guaranteeing stability in kinematic LfD research are found in the current literature. The first one, a paper by Savariano et al. \cite{saverianoEnergybasedApproachEnsure2020}, used the energy tank of the previous chapter to ensure the DS's stability \textbf{online}. By storing the energy of stable (conservative) actions in this tank, potentially unstable motions can be performed as long as there is enough energy in the tank, thereby preserving the system's overall stability. Their method can achieve comparable accuracy as the constrained-based methods above, can be used with any regression technique, is less computationally intensive and requires fewer parameters to be tuned. However, it has an implicit time dependency and, like the energy tank-based methods in the previous chapter, requires a tank initialisation procedure which can significantly affect the reproduction accuracy. The second paper, Figueroa et al. \cite{figueroafernandezPhysicallyconsistentBayesianNonparametric2018}, used a non-parametric GMM to learn the movement policy and an elliptical Lyapunov function jointly. They introduce a novel similarity metric which leverages locality and directionally of the demonstrations and use this metric as a Bayesian prior in the constraint optimisation. By doing this, they end up with a globally asymptotically stable DS that also converges locally to the demonstrated tractories. Their algorithm outperforms several methods mentioned above in accuracy and generalizability without relying on diffeomorphisms or contraction theory.

\subsection{Stability of the learned impedance}

% TODO: Measurement of physical interaction might not be possible when learning from demonstrations LFD mostly at kinematic level(balatti 2020).

% TODO: Check bensi 2022 for earlier papers non manipulator CBF ect.

% TODO: Spaandonk thesis for general model based impedance methods.

% TODO: See si for force articles.

% TODO: Arduengo uses kronander, others use tank based.

% TODO: Old methods -> Stability constraint on parameters of DS (see Saveriano et al. 2020) -> Potential fields ()

% TODO: Optimize with constraint but this might lead to decreased performance therefore energy tank (Savariano)!

% TODO: See arduengo for overview of LFD methods.

% TODO: Give an extension to impedance research.
% - [ ] SEDS of DMP: Trajectory + GMM for impedance paramters
% - [ ] GMM voor alles
% - [ ] DMP voor alles CMP.
% - [ ] But these methods are rather conservative and do not show passivity when using in interaction. Therefore several authors have used the passivity methods in the previous chapters like energy-tanks and lyapunov constraints for ensure passivity.
% - [ ]List tank-based and Lyapunov constraints.

% == IMPEDANCE BASED METHODS ==
% [ ] Impedance based LfD methods.
% [ ] Combined methods.
% [ ] Separate methods.
% [ ] Energy tanks.
% [ ] Potential fields.
% [ ] Lyapunov constraints.

% [ ] Explain how kronander et al. 2016 uses energy tank to keep conservative DS passive.
%     [ ] Kronander creates a new passive DS that also can encode non-passive actions.
%     [ ] Give papers that use this techique.
%         [ ] Extended by amoud for contact tasks with force tracking.
% [ ] Show other methods.

% NOTE: Cheng 2021 mentions tank but does not yet investigate it.

Several authors have extended these trajectory-based LfD methods to interaction tasks. Duo et al. 2022, for example, \cite{douRobotSkillLearning2022} extended a DMP with a variable stiffness term to create a Compliant Movement Principle (CMP) method that can be used to learn interaction tasks. Using this method, they learned the trajectory and stiffness profile from sensors. Arduingo et al. 2020, on the other hand, resorted to a GP-based learning approach in which the stiffness is related to the uncertainty of the movement. The more uncertain a movement is, the more the stiffness is adjusted. Both papers used Kronander stability constraints to stabilise the variable impedance profile.

First \cite{shahriariAdaptingContactsEnergy2017}, achieve this by adding valves to the tank design. By adjusting the weights of these valves, the power released during task execution can be controlled. Tank in demonstrations!

VIL methods only variable and damping matrices (see abu daku 2020).

All of the VIL methods that are currently used only vary the stiffness and damping matrices.

On top of that several authors have used the traditional stability methods discussed in the previous chapter for keeping the control passive.

But these methods are rather conservative and do not show passivity when using in interaction. Therefore several authors have used the passivity methods in the previous chapters like energy-tanks and lyapunov constraints for ensure passivity.

Several authors have for example used energy tanks to keep the variable impedance passive \cite{amanhoudForceAdaptationContact2020,enayatiVariableImpedanceForceControl2020,kastritsiProgressiveAutomationDMP2018,michelBilateralTeleoperationAdaptive2021,saverianoEnergybasedApproachEnsure2020,shahriariAdaptingContactsEnergy2017,wuFrameworkAutonomousImpedance2021,amanhoudDynamicalSystemApproach2019,kronanderPassiveInteractionControl2016} while others have used the stability constraints of and demonstration-learned \cite{arduengoGaussianProcessbasedRobotLearning2020,douRobotSkillLearning2022}.

Several authors have extended these trajectory based LfD methods to interaction tasks. Duo et al. 2022 for example \cite{douRobotSkillLearning2022} extended a DMP with a variable stiffness term to create a Compliant Movement Principle (CMP) method that can be used to learn interaction tasks. Using this method they were able to learn the trajectory and stiffness profile from sensors. Arduingo et al. 2020 on the other hand resorted to a GP based learning approach in which the stiffness is related to the uncertainty of the movement. The more uncertain a movement is the more the stiffness is adjusted. Both these papers used Kronander stability constraint to keep the variable impedance profile stable.

\section{Variable impedance learning control}

% TODO: Kim could be constant impedance?
% TODO: GAILS? INverse RL?
% TODO: Check Jin for downsides RL vs GGM. JIN 2022 good starting point for this chapter!.
% TODO: Liang 2021: Online RL with constraint on inertia.

One of the earliest papers that use stiffness like behavoirs in DS is calinonLearningReproductionGestures2010.

Recently authors have also investigated the passivity problem in RL-based variable impedance methods, which can learn complex tasks without knowing anything about the system and environment. In these methods, the closed-loop system's passivity is ensured by guaranteeing that the actions coming for the policy adhere to certain passivity constraints. For example, Kim et al. 2008 \cite{kimLearningRobotStiffness2008} ensure that the policy's task-stiffness matrix is always symmetric and positive definite. Doing this guarantees the manipulator's stability when interacting with a passive environment. Liang et al. 2021 (Liang et al., 2021) keep the closed-loop system stable by deriving three Lyapunov stability constraints on the inertia matrix. As a result, the damping and stiffness matrices can vary.

\cite{reyLearningMotionsDemonstrations2018} and \cite{khaderStabilityGuaranteedReinforcementLearning2020} take a similar approach by ensuring that the policy sample distribution can only generate passive parameter samples. Later \cite{khaderLearningDeepEnergy2021} ensure the passivity of their RL algorithm by using a deep neural policy in which the structure guarantees stability. Since both \cite{khaderStabilityGuaranteedReinforcementLearning2020,reyLearningMotionsDemonstrations2018} and \cite{khaderLearningDeepEnergy2021}can only use entire trajectories, they are not very sample efficient. \cite{khaderLearningStableNormalizingFlow2021} proposed a new method to utilize all state-action samples to solve this.

As explained above, in VIL methods, the desired trajectory and impedance profiles for a given task are learned from human demonstrations using a supervised learning procedure called Imitation Learning (IM), also called Learning from Demonstrations (LfD). Two steps generally make up this approach: A step in which [kinetic/kinematic] (and dynamic) data is collected from sensors (i.e. data-collection step), followed by a step in which a model is fitted to the collected data using regression (i.e. data-fitting step). Any regression model and technique can be used in this data-fitting step.

Traditionally, LfD research has mainly focused on reproducing the demonstrations' kinematics (i.e. trajectories). A good overview of these trajectory-based methods can be found in \cite{siReviewManipulationSkill2021}. Within these methods, two main approaches are found to ensure the reproduced trajectories' stability.
Give the stability methods used in trajectory-based LfD research
SEDS optimi lyapunov conraind
DMP intrinsicly stable.

Several authors have extended these trajectory-based LfD methods to interaction tasks. Within these methods two groups can be found. Methods that encode both the desired trajectory and variable impedance in one model and methods that use a seperate model for each component.
Duo et al. 2022, for example \cite{douRobotSkillLearning2022} extended a DMP with a variable stiffness term to create a Compliant Movement Principle (CMP) method that can be used to learn interaction tasks. Using this method they were able to learn the trajectory and stiffness profile from sensors. Arduingo et al. 2020 on the other hand resorted to a GP based learning approach in which the stiffness is related to the uncertainty of the movement. The more uncertain a movement is the more the stiffness is adjusted. Both these papers used Kronander stability constraint to keep the variable impedance profile stable.

Several authors have for example used energy tanks to keep the variable impedance passive \cite{amanhoudForceAdaptationContact2020,enayatiVariableImpedanceForceControl2020,kastritsiProgressiveAutomationDMP2018,michelBilateralTeleoperationAdaptive2021,saverianoEnergybasedApproachEnsure2020,shahriariAdaptingContactsEnergy2017,wuFrameworkAutonomousImpedance2021,amanhoudDynamicalSystemApproach2019,kronanderPassiveInteractionControl2016} while others have used the stability constraints of and demonstration-learned \cite{arduengoGaussianProcessbasedRobotLearning2020,douRobotSkillLearning2022}.

Several models were used to represent these trajectory

Two main approaches are found to ensure the reproduced trajectory's stability within these methods.
The second approach is

A good overview of these trajectory-based methods can be found in \cite{siReviewManipulationSkill2021}. Within these LfD methods, two general
This method can be broadly divided into two main groups, both coming with their strengths and shortcomings: (\textbf{probabilistic or statistical})-based methods and \textbf{dynamical system}-based methods. The main difference between these groups is the type of model that represents the collected data. Probabilistic-based methods, like Gaussian Mixture Models (GMM) and Gaussian Processes (GP),
use probability functions to quantify the learned trajectory, while Dynamic system-based methods, like Stable Estimator of Dynamical Systems (SEDS) and Dynamic Movement Primitives (DMPs), represent the trajectory as a set of (non)-linear (autonomous) dynamical systems. Each of these representations has its strengths and shortcomings.
These methods were later extended for also including force ino
