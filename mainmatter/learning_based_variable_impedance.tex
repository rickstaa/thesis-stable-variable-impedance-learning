\chapter{Stability methods in learning based variable impedance control}
\label{chapter:learning_based_variable_impedance}

The previous chapter discussed methods for ensuring stable control in classical variable impedance controllers. Although impressive results were achieved using these classical controllers \cite{songTutorialSurveyComparison2019}, the variable impedance profiles used by these controllers are manually programmed for a given task. This considerably reduces the usability of these controllers since every time a robot is used for a new task, multiple hours of labour by skilled workers are required for re-programming the controller. In recent years, numerous authors have instead switched to learning-based variable impedance controllers \cite{abu-dakkaVariableImpedanceControl2020}. These controllers allow for learning more complex behaviours without the need for the researcher to program this behaviour explicitly. A recent review of these learning-based variable impedance controllers was done by \cite{abu-dakkaVariableImpedanceControl2020}. In this review, the learning-based VIC methods were classified into two main groups: Variable Impedance Learning (VIL), in which the impedance profiles are learned from human demonstrations (through imitation learning) to be later executed with an existing impedance controller, and Variable Impedance Learning Control (VILC) in which the impedance control law is directly learned (through imitation learning, iterative learning or reinforcement learning). Where \cite{abu-dakkaVariableImpedanceControl2020} focuses on the learning-based VIC methods used in the current literature, the following sections concentrate on the stability and passivity techniques employed to ensure stability while using these methods.

\section{Variable impedance learning}

\subsection{Stability of the learned trajectories}
% - [x] DMP
% - [x] Constraint optimzation
% - [x] Learned optimization
% - [x] energy tank.
% - [ ] diffeomorphism
% - [ ] convergence
% - [ ] Local stability + uncertainty estimation.

% [ ] Check anand review.

% [ ] Check Ravanbaksch.
% [ ] Check Xiao resources.
% [ ] Check articles related to Ravanbaksch.
% [ ] Check articles diffeomorphic articles.
% [ ] Check convergence articles.

% [x]: Check existing SEDS quadratic articles.
% [ ]: Check existing LEANRING BASED ARTICLES.
% [ ]: Check references khansari et al. 2011.
% [ ]: Check references khansari et al. 2014.


% QUESTION: Two times LfD in first sentence and two times called. Is there a better way to say this?
As explained above, in VIL methods, the desired trajectory and impedance profiles for a given task are learned from human demonstrations using a supervised learning procedure called Imitation Learning (IM), also called Learning from Demonstrations (LfD). Two steps generally make up this approach: A step in which kinematic (and dynamic) data is collected from sensors (i.e. data-collection step), followed by a step in which a model is fitted to the collected data using regression (i.e. data-fitting step). Depending on the task, any regression model (e.g. linear models, splines, gaussian mixture models, neural networks, gaussian processes etc.) and accompanying regression technique can be used in this data-fitting step \cite{kroemerReviewRobotLearning2021,husseinImitationLearningSurvey2017}. Traditionally, LfD research has mainly focused on reproducing the demonstrations' kinematics (i.e. trajectories) \cite{siReviewManipulationSkill2021}. At first glance, this trajectory reproduction problem seems to be a simple regression task that can be solved by minimising the squared distance between the sampled trajectories and the regression model. A low-level controller like a PID controller can then be used to reproduce the recorded trajectories. However, in practice, such a naive regression approach has proven to be insufficient because it results in an over-fitted model that does not generalise outside of the demonstrated trajectories \cite{sindhwaniLearningContractingVector2018}. Therefore, this (non-linear) model is only locally stable and does not converge to the desired trajectories if situations are encountered that were not present in the training data (e.g. different initial states, temporal disturbances and spatial disturbances). One possible solution would be to use linear regression models in which stability outside the demonstrated regions can be proven easily. Such models, however, cannot accurately reproduce desired trajectories that are non-linear and non-smooth. Therefore, several methods have been proposed in the literature for learning (non-linear) trajectories while guaranteeing stability outside the demonstrated region.

%QUESTION: "This is achieved by exponentially decaying the phase variable during the task, thereby decreasing the effect of the "possibly unstable non-linear" forcing term". Is this correct?
%QUESTION: (-invariant). Is it written correctly?
%TODO: Add complexity note?
%TODO: Type of stability AS, ES, UUB.
In LfD research, motions are often modelled as a Dynamic System (DS) \cite{khansari-zadehLearningStableNonlinear2011} to improve generalizability. Compared to classical approaches, which use time-indexed trajectories, in a DS, the trajectories are formulated as a differential equation. Because of this, the DS captures both the trajectories and the essential dynamics that underlie a given task, making it better able to adapt to environmental changes like temporal and spatial disturbances. The most widely used DS-based approach for learning stable non-linear trajectories is the so-called "Dynamic Movement Primitives" (DMPs) \cite{ijspeertDynamicalMovementPrimitives2013,saverianoDynamicMovementPrimitives2021,wangLearningDemonstrationUsing2021,sidiropoulosReversibleDynamicMovement2021,ginesiOvercomingDrawbacksDynamic2021,rozanecNeuralDynamicMovement2022,liProDMPsUnifiedPerspective2022}. In DMPs, a globally stable linear DS is coupled with a non-linear forcing term through a phase variable to create an autonomous, weakly non-linear DS. 

% TODO: Add computational efficient way?
This DS can learn complex high-dimensional motions from single \cite{ijspeertDynamicalMovementPrimitives2013,prakashDynamicTrajectoryGeneration2020} or multiple demonstrations \cite{matsubaraLearningParametricDynamic2011,pervezLearningTaskparameterizedDynamic2018} while guaranteeing global stability. This is achieved by exponentially decaying the phase variable during the task, thereby decreasing the effect of the possibly unstable non-linear forcing term. As these DMPs are both time and scale (-invariant), the learnt motions' velocity and amplitude can be scaled without losing stability, making them well-suited for reacting to external perturbations in real time. Unfortunately, although the phase variable ensures the stability of the motions, it introduces an implicit time dependency in the DMP formulation. Due to this time dependency, the motion of the DMP is very dependent on the phase variable, which is, again, task-dependent. Consequently, DMPs have poor generalisation capabilities outside the demonstrations and are not robust against temporal disturbances \cite{neumannLearningRobotMotions2015}.

% TODO: Cheack Figueroa et al. 2022!
% TODO: Check Zhang et al. 2022
%TODO: Local stability Dutta 2021 add comment to text?

%QUESTION: Is it clear which trade-off is meant?
%QUESTION: The online offline part is it clear?

Other authors use a constraint optimisation approach to overcome the abovementioned limitations and encode the demonstrated motions in a state-dependent non-linear DS while enforcing stability through Lyapunov constraints. Since these DSs are time-independent, the learned policies are now robust against temporal disturbances. This was first done by Khansari et al. 2011 \cite{khansari-zadehLearningStableNonlinear2011}, who introduced a stable estimator of a dynamical system (SEDS). By limiting the parameters of a gaussian mixture regression model (GMM) using a Quadratic Lyapunov constraint, SEDS can learn accurate motions while guaranteeing global stability. Similar Lyapunov constraints were used in literature with other regression models to ensure the global stability of the learned motions \cite{lemmeNeurallyImprintedStable2013,huNeuralLearningStable2015,umlauftLearningStableStochastic2017,umlauftLearningStableGaussian2017,medinaLearningStableTask2017,duanFastStableLearning2019,xuRobotTrajectoryTracking2019,umlauftLearningStochasticallyStable2020,ledererGaussianProcessBasedRealTime2021,xuLearningBasedKinematicControl2022,salehiLearningDiscreteTimeUncertain2022,davoodiRuleBasedSafeProbabilistic2022}. Unfortunately, as these methods' stability criteria are derived based on a simple quadratic Lyapunov function, they can only model trajectories in which the distance to the target decreases monotonically in time, leading to poor accuracy if demonstrated trajectories are not contractive. As a result, several authors have used parametric and non-parametric Lyapunov candidates to learn less conservative Lyapunov constraints directly from data to improve this so-called accuracy-stability dilemma \cite{khansari-zadehLearningControlLyapunov2014,neumannNeuralLearningStable2013,lemmeNeuralLearningVector2014,umlauftLearningStableGaussian2017,umlauftUncertaintybasedControlLyapunov2018,duttaLearningStableMovement2018,umlauftUncertaintybasedHumanMotion2019,duttaSkillLearningHuman2021,ravanbakhshLearningControlLyapunov2019,ravanbakhshFormalPolicyLearning2019,umlauftLearningStochasticallyStable2020,xiaoLearningStableNonparametric2020,tesfazgiInverseReinforcementLearning2021,coulombeGeneratingStableCollisionFree2022}. These less conservative Lyapunov constraints can then be enforced \textbf{offline} during learning or \text{online} through a stabilising control command to ensure the stability of the reproduced trajectories. If applied online \cite{khansari-zadehLearningControlLyapunov2014,umlauftLearningStableGaussian2017,umlauftUncertaintybasedControlLyapunov2018,duttaLearningStableMovement2018,umlauftUncertaintybasedHumanMotion2019,ravanbakhshFormalPolicyLearning2019,ravanbakhshLearningControlLyapunov2019,xiaoLearningStableNonparametric2020,duttaSkillLearningHuman2021,umlauftLearningStochasticallyStable2020}, any regression method can be used, but no guarantees can be given about the accuracy of reproduced trajectories since the control command potentially interferes with the dynamic system. If applied offline \cite{neumannNeuralLearningStable2013,lemmeNeuralLearningVector2014,tesfazgiInverseReinforcementLearning2021,coulombeGeneratingStableCollisionFree2022}, on the other hand, the implementation is specific to the used regression model, but the shape and accuracy of the demonstrated trajectories are known beforehand. Unfortunately, most offline methods currently lack constructive mathematical guarantees causing the stability of these methods to be restricted to finite regions in the workspace. 




% TODO: Dummy text

Although these learned Lyapunov constraints result in better accuracy than their fixed counterparts, the reproduction accuracy still depends on the Lyapunov candidate function used during learning. Although less conservative Lyapunov Functions could be used to improve the accuracy, finding them is not easy, time-consuming, and quickly becomes computationally intensive or intractable \cite {}.

Although less conservative parameterisations of Lyapunov candidates could be used to improve the accuracy, these quickly become computationally intensive, time-consuming or intractable \cite{}. 

Furthermore, as there is no general analytic method for finding proper Lyapunov candidate functions and the accuracy still depends on the energy function (i.e. Lyapunov candidate) used as a basis for the stabilisation mechanism.

To overcome these issues, recent papers have taken a diffeomorphic approach and train a DS on the demonstrated motions directly while applying the Lyapunov constraint to a topological equivalent latent space \cite{neumannLearningRobotMotions2015,perrinFastDiffeomorphicMatching2016,jinImprovedLearningAccuracy2019}. 

Since a diffeomorphism is used to translate the demonstrated and reproduced motions between these spaces, the stability guarantee in the latent space is retained in the original space \cite{leeIntroductionTopologicalManifolds2011,leeIntroductionSmoothManifolds2012,leeIntroductionRiemannianManifolds2018}. 


This method offers an elegant solution to the accuracy-stability trade-off and dramatically improves generalizability.


Since now more complex Lyapunov functions like elliptical can be learned. 


Due to the diffeomorphic 

By transforming the original demonstrated data into a new space (of the same dimension or a higher dimension), where the transformed demonstrations are
consistent with the Lyapunov function candidate




to diffeomorphic methods for increasing the stability of the reproduced motions 


Unfortunately, the accuracy of these learning-based methods is still dependent on the choice of the Lyapunov candidate function. 

Unfortunately, most offline methods currently lack constructive mathematical guarantees causing the stability of these methods to be restricted to finite regions in the workspace.

 Several recent papers have, therefore 


By transforming the original demonstrated data into a new space (of the same dimension or a higher dimension), where the transformed demonstrations are
consistent with the Lyapunov function candidate


Altough this adds some model complexity it results in better accuracy.

In a attempt to overcome the shortcomings of the learning based methods several authors have used diffeomorphic translations to learn complex Lyapunov functions while

Lastly several papers have convergence

More complex lyapunov functions like elliptical while keeping the optimization computational unfeasible.

% TODO: DUMMY TEXT

% TODO: Check duan 2019 remarks on neuman.
% TODO: Check jin et al. 2019 dimension ascent.
% TODO: Check billart 2022.
% TODO: Check if diffeomorphisms can be used with any regression technique

Recent papers have used a diffeomorphism to train a non-autonomous DS on the motions directly while applying the Lyapunov constraint to a topological equivalent latent space \cite{neumannLearningRobotMotions2015,perrinFastDiffeomorphicMatching2016,jinImprovedLearningAccuracy2019,ranaEuclideanizingFlowsDiffeomorphic2020,urainImitationFlowLearningDeep2020,saverianoLearningStableRobotic2022,urainLearningStableVector2022,wangLearningDeepRobotic2022}. Since a diffeomorphism is used to translate the demonstrated and reproduced motions between these spaces, the stability guarantee in the latent space is retained in the original space \cite{leeIntroductionTopologicalManifolds2011,leeIntroductionSmoothManifolds2012,leeIntroductionRiemannianManifolds2018}. This method offers an elegant solution to the accuracy-stability trade-off and dramatically improves the generalizability since now more complex lyapunov functions like elliptical can be learned. 


Another noteworthy strategy was proposed by Savariano et al. 2020\cite{saver}, who used the previous chapter's energy tank-based technique to ensure the DS's stability \textbf{online}. Storing the energy of stable, conservative actions in this tank allows potentially unstable motions to be performed as long as there is enough energy, preserving the system's overall stability. Their method can achieve comparable accuracy as the constrained-based methods above, can be used with any regression technique, is less computationally intensive and requires fewer parameters to be tuned. However, their method has an implicit time dependency and, like the energy tank-based methods in the previous chapter, requires a tank initialisation procedure which can significantly affect the reproduction accuracy.

% TODO: Check Rana and Urain 2020 remarks on convergence.
Although the methods above achieve high accuracy while guaranteeing the stability of the equilibrium points, they give no guarantee about the convergence of the trajectories. As a result, although learned policies will always reach the target, regardless of the region in the state space where they are perturbed, they will no longer follow the reference trajectories. These techniques are, therefore, inadequate for tasks where trajectory tracking is crucial. Several authors have used contraction theory to derive stability conditions that guarantee incremental stability to improve the accuracy of the reproduced trajectories \cite{blocherLearningStableDynamical2017}. Incremental stability can be viewed as a "strong" type of stability that studies the convergence between any two trajectories of a given system \cite{lohmillerContractionAnalysisNonlinear1998}.

Unfortunately, these contraction based methods like the aforementioned based methods can suffer from the risk of over-corrections which might deviate the DS from the desired non-linear motion. \cite{figueroafernandezPhysicallyconsistentBayesianNonparametric2018}. They further are limited to local stability guarantees, making the approximated vector field unsuitable for sparse or no data regions.

More recently, Figueroa et al. 2018 \cite{figueroafernandezPhysicallyconsistentBayesianNonparametric2018} were able to learn globally asymptotically stable DS from demonstrations by jointly learning the policy and an elliptical Lyapunov function using a non-parametric GMM. They designed a novel similarity metric that leverages locality and directionally of the demonstrations and used these as a Bayesian prior in their constraint optimization. By doing this, their method outperforms previous methods' accuracy and generalizability without relying on diffeomorphisms or contraction theory.

\subsection{Stability of the learned impedance}

% TODO: See methods with diffemorphic rieman manifolds.

Several authors have extended these trajectory-based LfD methods to interaction tasks. Duo et al. 2022, for example, \cite{douRobotSkillLearning2022} extended a DMP with a variable stiffness term to create a Compliant Movement Principle (CMP) method that can be used to learn interaction tasks. Using this method, they learned the trajectory and stiffness profile from sensors. Arduingo et al. 2020, on the other hand, resorted to a GP-based learning approach in which the stiffness is related to the uncertainty of the movement. The more uncertain a movement is, the more the stiffness is adjusted. Both papers used Kronander stability constraints to stabilise the variable impedance profile.

First \cite{shahriariAdaptingContactsEnergy2017}, achieve this by adding valves to the tank design. By adjusting the weights of these valves, the power released during task execution can be controlled. Tank in demonstrations!

VIL methods only variable and damping matrixes (see abu daku 2020).

All of the VIL methods that are currently used only vary the stiffness and damping matrixes.

On top of that several authors have used the traditional stability methods discussed in the previous chapter for keeping the control passive.

But these methods are rather conservative and do not show passivity when using in interaction. Therefore several authors have used the passivity methods in the previous chapters like energy-tanks and lyapunov constraints for ensure passivity.

Several authors have for example used energy tanks to keep the variable impedance passive \cite{amanhoudForceAdaptationContact2020,enayatiVariableImpedanceForceControl2020,kastritsiProgressiveAutomationDMP2018,michelBilateralTeleoperationAdaptive2021,saverianoEnergybasedApproachEnsure2020,shahriariAdaptingContactsEnergy2017,wuFrameworkAutonomousImpedance2021,amanhoudDynamicalSystemApproach2019,kronanderPassiveInteractionControl2016} while others have used the stability constraints of and demonstration-learned \cite{arduengoGaussianProcessbasedRobotLearning2020,douRobotSkillLearning2022}.

Several authors have extended these trajectory based LfD methods to interaction tasks. Duo et al. 2022 for example \cite{douRobotSkillLearning2022} extended a DMP with a variable stiffness term to create a Compliant Movement Principle (CMP) method that can be used to learn interaction tasks. Using this method they were able to learn the trajectory and stiffness profile from sensors. Arduingo et al. 2020 on the other hand resorted to a GP based learning approach in which the stiffness is related to the uncertainty of the movement. The more uncertain a movement is the more the stiffness is adjusted. Both these papers used Kronander stability constraint to keep the variable impedance profile stable.

% TODO: Check sharifi et al. 2022 for impedance learning.

\section{Variable impedance learning control}


One of the earliest papers that use stiffness like behavoirs in DS is calinonLearningReproductionGestures2010.


Recently authors have also investigated the passivity problem in RL-based variable impedance methods, which can learn complex tasks without knowing anything about the system and environment. In these methods, the closed-loop system's passivity is ensured by guaranteeing that the actions coming for the policy adhere to certain passivity constraints. For example, Kim et al. 2008 \cite{kimLearningRobotStiffness2008} ensure that the policy's task-stiffness matrix is always symmetric and positive definite. Doing this guarantees the manipulator's stability when interacting with a passive environment. Liang et al. 2021 (Liang et al., 2021) keep the closed-loop system stable by deriving three Lyapunov stability constraints on the inertia matrix. As a result, the damping and stiffness matrices can vary.

\cite{reyLearningMotionsDemonstrations2018} and \cite{khaderStabilityGuaranteedReinforcementLearning2020} take a similar approach by ensuring that the policy sample distribution can only generate passive parameter samples. Later \cite{khaderLearningDeepEnergy2021} ensure the passivity of their RL algorithm by using a deep neural policy in which the structure guarantees stability. Since both \cite{khaderStabilityGuaranteedReinforcementLearning2020,reyLearningMotionsDemonstrations2018} and \cite{khaderLearningDeepEnergy2021}can only use entire trajectories, they are not very sample efficient. \cite{khaderLearningStableNormalizingFlow2021} proposed a new method to utilize all state-action samples to solve this.

As explained above, in VIL methods, the desired trajectory and impedance profiles for a given task are learned from human demonstrations using a supervised learning procedure called Imitation Learning (IM), also called Learning from Demonstrations (LfD). Two steps generally make up this approach: A step in which [kinetic/kinematic] (and dynamic) data is collected from sensors (i.e. data-collection step), followed by a step in which a model is fitted to the collected data using regression (i.e. data-fitting step). Any regression model and technique can be used in this data-fitting step.

Traditionally, LfD research has mainly focused on reproducing the demonstrations' kinematics (i.e. trajectories). A good overview of these trajectory-based methods can be found in \cite{siReviewManipulationSkill2021}. Within these methods, two main approaches are found to ensure the reproduced trajectories' stability.
Give the stability methods used in trajectory-based LfD research
SEDS optimi lyapunov conraind
DMP intrinsicly stable.

Several authors have extended these trajectory-based LfD methods to interaction tasks. Within these methods two groups can be found. Methods that encode both the desired trajectory and variable impedance in one model and methods that use a seperate model for each component.
Duo et al. 2022, for example \cite{douRobotSkillLearning2022} extended a DMP with a variable stiffness term to create a Compliant Movement Principle (CMP) method that can be used to learn interaction tasks. Using this method they were able to learn the trajectory and stiffness profile from sensors. Arduingo et al. 2020 on the other hand resorted to a GP based learning approach in which the stiffness is related to the uncertainty of the movement. The more uncertain a movement is the more the stiffness is adjusted. Both these papers used Kronander stability constraint to keep the variable impedance profile stable.

Several authors have for example used energy tanks to keep the variable impedance passive \cite{amanhoudForceAdaptationContact2020,enayatiVariableImpedanceForceControl2020,kastritsiProgressiveAutomationDMP2018,michelBilateralTeleoperationAdaptive2021,saverianoEnergybasedApproachEnsure2020,shahriariAdaptingContactsEnergy2017,wuFrameworkAutonomousImpedance2021,amanhoudDynamicalSystemApproach2019,kronanderPassiveInteractionControl2016} while others have used the stability constraints of and demonstration-learned \cite{arduengoGaussianProcessbasedRobotLearning2020,douRobotSkillLearning2022}.

Several models were used to represent these trajectory

Two main approaches are found to ensure the reproduced trajectory's stability within these methods.
The second approach is

A good overview of these trajectory-based methods can be found in \cite{siReviewManipulationSkill2021}. Within these LfD methods, two general
This method can be broadly divided into two main groups, both coming with their strengths and shortcomings: (\textbf{probabilistic or statistical})-based methods and \textbf{dynamical system}-based methods. The main difference between these groups is the type of model that represents the collected data. Probabilistic-based methods, like Gaussian Mixture Models (GMM) and Gaussian Processes (GP),
use probability functions to quantify the learned trajectory, while Dynamic system-based methods, like Stable Estimator of Dynamical Systems (SEDS) and Dynamic Movement Primitives (DMPs), represent the trajectory as a set of (non)-linear (autonomous) dynamical systems. Each of these representations has its strengths and shortcomings.
These methods were later extended for also including force ino
